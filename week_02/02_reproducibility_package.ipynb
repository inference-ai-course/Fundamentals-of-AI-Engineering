{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Part 02: Reproducibility Package Lab\n",
    "\n",
    "**Estimated time:** 90–120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on environments, dependencies, and reproducibility:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Chapter 2: Python and Environment Management](../self_learn/Chapters/2/Chapter2.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 02)\n",
    "\n",
    "- You save a reproducibility package under `output/` that includes:\n",
    "  - `config.json` — the exact settings used\n",
    "  - `metrics.json` — the results\n",
    "  - `requirements.txt` — the dependencies\n",
    "- Another person can recreate your environment and get similar results.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook:\n",
    "- You can point to `output/reproducibility_package/config.json`\n",
    "- You can point to `output/reproducibility_package/requirements.txt`\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Create a complete reproducibility package\n",
    "- Capture configuration, metrics, and dependencies\n",
    "- Understand what makes an experiment reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65520fd5",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook focuses on **reproducibility** — the ability to re-run an experiment and get the same (or explainably similar) results.\n",
    "\n",
    "A reproducibility package bundles together:\n",
    "- **`config.json`** — exact parameters used (seed, model, hyperparameters)\n",
    "- **`metrics.json`** — what happened (accuracy, F1, latency)\n",
    "- **`requirements.txt`** — exact library versions\n",
    "\n",
    "**Why this matters for LLM work:** When you later compare prompt strategies or model versions, reproducibility lets you isolate what changed. Was it the data? The prompt? The model version? Without saved configs and artifacts, you're guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfd155",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook explores **reproducibility packages** using our unified `ml_package`.\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- explore the existing `ml_package` structure \n",
    "- use the package to create reproducible runs\n",
    "- capture run inputs and metadata\n",
    "- generate complete reproducibility packages\n",
    "\n",
    "> **Note**: We now have a complete, production-ready `ml_package` that handles training, comparison, reporting, and reproducibility. This notebook will reference and use this package instead of creating duplicate code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a4dfb",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Imports the `ml_package` modules and shows the package structure. The package is split into four focused modules:\n",
    "\n",
    "- **`trainer.py`** — the 6-stage training pipeline (load → split → scale → train → evaluate → save)\n",
    "- **`reproducibility.py`** — captures dependencies, validates the environment, creates run metadata\n",
    "- **`comparison.py`** — loads multiple run folders, selects the best, summarizes across runs\n",
    "- **`reporting.py`** — generates markdown reports and dashboards\n",
    "\n",
    "**Why a package instead of one big script?** Each module has a single responsibility. You can test, reuse, and update each part independently. This is the same principle as pipeline stages in Week 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1debf1",
   "metadata": {},
   "source": [
    "## Exercise 1: Explore the ML Package Structure\n",
    "\n",
    "Let's examine our unified `ml_package` that handles all ML training needs:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "# Look at our package structure\n",
    "package_root = Path(\"ml_package\")\n",
    "print(\"Package structure:\")\n",
    "for path in sorted(package_root.rglob(\"*\")):\n",
    "    if path.is_file():\n",
    "        print(f\"  {path.relative_to(package_root)}\")\n",
    "```\n",
    "\n",
    "This package provides:\n",
    "- **trainer.py** - Core training pipeline with 6 modular stages\n",
    "- **comparison.py** - Run comparison and analysis utilities  \n",
    "- **reporting.py** - Report generation and dashboards\n",
    "- **reproducibility.py** - Dependency capture and environment validation\n",
    "\n",
    "Let's import and explore the key components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a485d",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Creates a sample dataset, configures a training run with explicit parameters, and runs `trainer.train()`.\n",
    "\n",
    "**Key reproducibility habits shown here:**\n",
    "- `random_state=42` — fixed seed so train/val splits are identical across runs\n",
    "- `max_iter=500` — explicit hyperparameter, not a hidden default\n",
    "- All parameters go into `TrainConfig` so they're automatically saved to `config.json`\n",
    "\n",
    "**What to check:** After running, find the new timestamped folder under `reproducibility_artifacts/`. Open `config.json` — it should contain every parameter you set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078108a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the package structure\n",
    "from pathlib import Path\n",
    "\n",
    "package_root = Path(\"ml_package\")\n",
    "print(\"Package structure:\")\n",
    "for path in sorted(package_root.rglob(\"*\")):\n",
    "    if path.is_file():\n",
    "        print(f\"  {path.relative_to(package_root)}\")\n",
    "\n",
    "# Import key components\n",
    "from ml_package import trainer, reproducibility, comparison, reporting\n",
    "\n",
    "print(\"\\n✅ Successfully imported all package modules!\")\n",
    "print(\"Available functions:\")\n",
    "print(\"- trainer: train(), create_sample_dataset(), TrainConfig\")\n",
    "print(\"- reproducibility: capture_dependencies(), validate_environment()\")\n",
    "print(\"- comparison: load_runs(), select_best_run(), summarize_runs()\")\n",
    "print(\"- reporting: generate_experiment_summary(), write_comparison_report()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0904289",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Captures the full dependency environment (`requirements.txt`), validates that key packages are importable, and saves run metadata (config + environment info) to a JSON file.\n",
    "\n",
    "**Why capture dependencies at run time?** Library versions change. If you re-run an experiment 3 months later with a newer `scikit-learn`, results may differ. Saving `requirements.txt` alongside your metrics means you can always recreate the exact environment that produced a result.\n",
    "\n",
    "**What `validate_environment()` checks:** That each required package is importable and returns its version. This catches silent failures where a package is listed in `requirements.txt` but not actually installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85800ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_csv = \"reproducibility_sample.csv\"\n",
    "trainer.create_sample_dataset(sample_csv, \"iris\")\n",
    "\n",
    "# Configure training\n",
    "config = trainer.TrainConfig(\n",
    "    input_csv=sample_csv,\n",
    "    label_col=\"label\", \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "# Run training\n",
    "result = trainer.train(config, \"reproducibility_artifacts\")\n",
    "print(f\"Training completed! Accuracy: {result.metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete reproducibility package\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the run directory\n",
    "run_id = time.strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "run_dir = Path(\"reproducibility_artifacts\") / run_id\n",
    "\n",
    "# Capture dependencies\n",
    "reproducibility.capture_dependencies(run_dir / \"requirements.txt\")\n",
    "\n",
    "# Validate environment\n",
    "env_info = reproducibility.validate_environment([\"pandas\", \"scikit-learn\", \"joblib\", \"numpy\"])\n",
    "\n",
    "# Create comprehensive metadata\n",
    "metadata = reproducibility.create_run_metadata(\n",
    "    config=config.__dict__,\n",
    "    environment_info=env_info\n",
    ")\n",
    "reproducibility.save_run_metadata(metadata, run_dir / \"run_metadata.json\")\n",
    "\n",
    "print(f\"Reproducibility package created in: {run_dir}\")\n",
    "print(\"Files created:\")\n",
    "for file in run_dir.iterdir():\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 3: Validate Reproducibility\n",
    "\n",
    "Let's check if our run can be reproduced:\n",
    "\n",
    "```python\n",
    "# Check reproducibility score\n",
    "repro_score = reproducibility.check_reproducibility(run_dir)\n",
    "print(f\"Reproducibility score: {repro_score['overall_score']}/100\")\n",
    "\n",
    "# Create a standalone reproducibility package\n",
    "package_dir = Path(\"reproducibility_package\")\n",
    "reproducibility.create_reproducibility_package(run_dir, package_dir)\n",
    "\n",
    "print(f\"\\nStandalone package created: {package_dir}\")\n",
    "print(\"This package can be shared and reproduced by others!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd7e29",
   "metadata": {},
   "source": [
    "# Check reproducibility score\n",
    "repro_score = reproducibility.check_reproducibility(run_dir)\n",
    "print(f\"Reproducibility score: {repro_score['overall_score']}/100\")\n",
    "\n",
    "# Create a standalone reproducibility package\n",
    "package_dir = Path(\"reproducibility_package\")\n",
    "reproducibility.create_reproducibility_package(run_dir, package_dir)\n",
    "\n",
    "print(f\"\\nStandalone package created: {package_dir}\")\n",
    "print(\"This package can be shared and reproduced by others!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a65868",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 4: Compare Multiple Runs\n",
    "\n",
    "Let's run multiple experiments and compare them:\n",
    "\n",
    "```python\n",
    "# Run multiple experiments with different configurations\n",
    "configs = [\n",
    "    trainer.TrainConfig(\"reproducibility_sample.csv\", \"label\", 0.2, 42, 200),\n",
    "    trainer.TrainConfig(\"reproducibility_sample.csv\", \"label\", 0.2, 42, 500),\n",
    "    trainer.TrainConfig(\"reproducibility_sample.csv\", \"label\", 0.3, 42, 500),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, cfg in enumerate(configs):\n",
    "    result = trainer.train(cfg, \"comparison_artifacts\")\n",
    "    results.append(result)\n",
    "    print(f\"Run {i+1}: accuracy={result.metrics['accuracy']:.4f}, max_iter={cfg.max_iter}\")\n",
    "\n",
    "# Load and compare runs\n",
    "runs = comparison.load_runs(\"comparison_artifacts\")\n",
    "summary = comparison.summarize_runs(runs)\n",
    "best = comparison.select_best_run(runs, \"accuracy\")\n",
    "\n",
    "print(f\"\\nBest run: {best.run_id} with accuracy {best.metrics['accuracy']:.4f}\")\n",
    "print(f\"Average accuracy across {len(runs)} runs: {summary['avg_accuracy']:.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924fb05",
   "metadata": {},
   "source": [
    "# Run multiple experiments with different configurations\n",
    "configs = [\n",
    "    trainer.TrainConfig(\"reproducibility_sample.csv\", \"label\", 0.2, 42, 200),\n",
    "    trainer.TrainConfig(\"reproducibility_sample.csv\", \"label\", 0.2, 42, 500),\n",
    "    trainer.TrainConfig(\"reproducibility_sample.csv\", \"label\", 0.3, 42, 500),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, cfg in enumerate(configs):\n",
    "    result = trainer.train(cfg, \"comparison_artifacts\")\n",
    "    results.append(result)\n",
    "    print(f\"Run {i+1}: accuracy={result.metrics['accuracy']:.4f}, max_iter={cfg.max_iter}\")\n",
    "\n",
    "# Load and compare runs\n",
    "runs = comparison.load_runs(\"comparison_artifacts\")\n",
    "summary = comparison.summarize_runs(runs)\n",
    "best = comparison.select_best_run(runs, \"accuracy\")\n",
    "\n",
    "print(f\"\\nBest run: {best.run_id} with accuracy {best.metrics['accuracy']:.4f}\")\n",
    "print(f\"Average accuracy across {len(runs)} runs: {summary['avg_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd7786",
   "metadata": {},
   "source": [
    "## Exercise 5: Generate Reports\n",
    "\n",
    "Let's create comprehensive reports for our experiments:\n",
    "\n",
    "```python\n",
    "# Generate comparison report\n",
    "report_dir = Path(\"experiment_reports\")\n",
    "reports = reporting.generate_experiment_summary(runs, report_dir)\n",
    "\n",
    "print(\"Generated reports:\")\n",
    "for name, path in reports.items():\n",
    "    print(f\"  {name}: {path}\")\n",
    "\n",
    "# Create a dashboard summary\n",
    "dashboard_path = report_dir / \"dashboard.md\"\n",
    "reporting.write_quick_summary(dashboard_path, runs)\n",
    "print(f\"  dashboard: {dashboard_path}\")\n",
    "\n",
    "# Show a snippet of the comparison report\n",
    "comparison_content = (report_dir / \"comparison_report.md\").read_text()\n",
    "print(\"\\nComparison report preview:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_content[:500] + \"...\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ff904",
   "metadata": {},
   "source": [
    "## Self-check\n",
    "\n",
    "✅ **Reproducibility Achieved:**\n",
    "- Complete package structure with modular components\n",
    "- Deterministic seeds stored with artifacts  \n",
    "- Dependency capture and environment validation\n",
    "- Run comparison and reporting capabilities\n",
    "\n",
    "✅ **Package Features:**\n",
    "- **trainer.py**: 6-stage modular training pipeline\n",
    "- **comparison.py**: Load, compare, and analyze multiple runs\n",
    "- **reporting.py**: Generate markdown reports and dashboards\n",
    "- **reproducibility.py**: Capture dependencies and validate environments\n",
    "\n",
    "✅ **CLI Tools Available:**\n",
    "- `python train.py` - Run training with reproducibility\n",
    "- `python compare_runs.py` - Compare and analyze experiments\n",
    "\n",
    "The package eliminates code duplication while providing enhanced functionality for reproducible ML experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison report\n",
    "report_dir = Path(\"experiment_reports\")\n",
    "reports = reporting.generate_experiment_summary(runs, report_dir)\n",
    "\n",
    "print(\"Generated reports:\")\n",
    "for name, path in reports.items():\n",
    "    print(f\"  {name}: {path}\")\n",
    "\n",
    "# Create a dashboard summary\n",
    "dashboard_path = report_dir / \"dashboard.md\"\n",
    "reporting.write_quick_summary(dashboard_path, runs)\n",
    "print(f\"  dashboard: {dashboard_path}\")\n",
    "\n",
    "# Show a snippet of the comparison report\n",
    "comparison_content = (report_dir / \"comparison_report.md\").read_text()\n",
    "print(\"\\nComparison report preview:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_content[:500] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
