{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Part 03: Compare Runs + Report Lab\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on evaluation metrics:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Evaluation metrics (accuracy/precision/recall/F1)](../self_learn/Chapters/4/02_core_concepts.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You compare at least 2 runs with a change in exactly one variable.\n",
    "- You write a short `report.md` that explains:\n",
    "  - what changed\n",
    "  - what happened\n",
    "  - what you think caused it\n",
    "  - what you'd try next\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook:\n",
    "- `output/report.md` exists and compares two runs\n",
    "- The comparison explains why the change mattered\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Compare ML experiment runs systematically\n",
    "- Write evidence-based reports\n",
    "- Identify the key variable that changed between runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0ec8c",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook teaches you to **compare experiment runs systematically** and write an evidence-based report.\n",
    "\n",
    "The core discipline: change **one variable at a time**, measure the effect, and write down what you found and why you think it happened.\n",
    "\n",
    "**Why this matters for LLM work:** The same discipline applies when comparing prompts, models, or temperatures. You need a consistent comparison framework — same inputs, same metrics, same artifact structure — to know whether a change actually helped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b45f3b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Comparing runs requires consistent fields and consistent artifacts.\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- load or create a small list of runs\n",
    "- select the best run using a clear rule\n",
    "- compute a summary\n",
    "- write report artifacts under `output/compare_runs/`\n",
    "\n",
    "If you need a refresher on evaluation metrics, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9d586",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Creates a list of 3 run records (simulating saved experiment results) and writes them to `output/compare_runs/runs.json`.\n",
    "\n",
    "**Key design:** Each run has a `run_id`, `model`, `accuracy`, `f1`, and `notes`. The `notes` field records what changed — this is how you track \"what was different about this run?\" without digging through code history.\n",
    "\n",
    "**Notice:** `run_002` changed only `max_iter` (more iterations), `run_003` switched to a different model (`rf` = Random Forest). A good experiment changes **one thing at a time** so you know what caused the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "runs = [\n",
    "    {\"run_id\": \"run_001\", \"model\": \"logreg\", \"accuracy\": 0.84, \"f1\": 0.82, \"notes\": \"baseline\"},\n",
    "    {\"run_id\": \"run_002\", \"model\": \"logreg\", \"accuracy\": 0.87, \"f1\": 0.86, \"notes\": \"more iterations\"},\n",
    "    {\"run_id\": \"run_003\", \"model\": \"rf\", \"accuracy\": 0.89, \"f1\": 0.88, \"notes\": \"higher depth\"},\n",
    "]\n",
    "\n",
    "out_dir = Path(\"output/compare_runs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "(out_dir / \"runs.json\").write_text(json.dumps(runs, indent=2), encoding=\"utf-8\")\n",
    "print(\"wrote\", out_dir / \"runs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1bc34",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `select_best_run_todo()` and `summarize_runs_todo()` — two functions you need to implement.\n",
    "\n",
    "**`select_best_run_todo()`** should return the run with the highest accuracy (tie-break on F1). This is a deliberate design choice: you pick *one* primary metric to rank by, then use a secondary metric to break ties.\n",
    "\n",
    "**`summarize_runs_todo()`** should return a summary dict with the best run, average accuracy, and total run count. This summary is what goes into your report.\n",
    "\n",
    "**Your task:** The current implementations are placeholders. Implement them properly — the solution is in the Appendix if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_run_todo(runs):\n",
    "    \"\"\"TODO: return the best run.\n",
    "\n",
    "    Criteria (suggested):\n",
    "\n",
    "    - highest accuracy\n",
    "    - tie-break: highest f1\n",
    "    \"\"\"\n",
    "    return runs[0]\n",
    "\n",
    "\n",
    "def summarize_runs_todo(runs):\n",
    "    \"\"\"TODO: return a small summary dict used for reporting.\"\"\"\n",
    "    best = select_best_run_todo(runs)\n",
    "    avg_acc = sum(r[\"accuracy\"] for r in runs) / len(runs)\n",
    "    return {\"best\": best, \"avg_accuracy\": round(avg_acc, 3), \"n\": len(runs)}\n",
    "\n",
    "\n",
    "summary = summarize_runs_todo(runs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf0c48e",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `write_report_todo()` — a function that writes a Markdown report summarizing the comparison.\n",
    "\n",
    "**What a good report includes:**\n",
    "- Total number of runs (so readers know the scope)\n",
    "- Average accuracy across all runs (baseline context)\n",
    "- The best run with its exact `run_id`, model, metrics, and notes\n",
    "\n",
    "**Why write to a file?** A report in a file is an artifact — it can be committed to git, shared with teammates, and referenced in future retrospectives. A print statement disappears when the notebook closes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report_todo(path: Path, summary: dict) -> None:\n",
    "    \"\"\"TODO: write a markdown report.\n",
    "\n",
    "    Suggested sections:\n",
    "\n",
    "    - Total runs\n",
    "    - Average accuracy\n",
    "    - Best run (with run_id, model, metrics, notes)\n",
    "    \"\"\"\n",
    "    lines = [\"# Run Comparison Report\", \"\", f\"Total runs: {summary['n']}\"]\n",
    "    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "write_report_todo(out_dir / \"report.md\", summary)\n",
    "print(\"wrote\", out_dir / \"report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0f6eb",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for the TODO functions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_run_todo(runs):\n",
    "    return max(runs, key=lambda r: (r[\"accuracy\"], r[\"f1\"]))\n",
    "\n",
    "\n",
    "def summarize_runs_todo(runs):\n",
    "    best = select_best_run_todo(runs)\n",
    "    avg_acc = sum(r[\"accuracy\"] for r in runs) / len(runs)\n",
    "    avg_f1 = sum(r[\"f1\"] for r in runs) / len(runs)\n",
    "    return {\n",
    "        \"best\": best,\n",
    "        \"avg_accuracy\": round(avg_acc, 3),\n",
    "        \"avg_f1\": round(avg_f1, 3),\n",
    "        \"n\": len(runs),\n",
    "    }\n",
    "\n",
    "\n",
    "def write_report_todo(path: Path, summary: dict) -> None:\n",
    "    lines = [\"# Run Comparison Report\", \"\"]\n",
    "    lines.append(f\"Total runs: {summary['n']}\")\n",
    "    lines.append(f\"Average accuracy: {summary['avg_accuracy']}\")\n",
    "    if \"avg_f1\" in summary:\n",
    "        lines.append(f\"Average f1: {summary['avg_f1']}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    best = summary[\"best\"]\n",
    "    lines.append(\"## Best run\")\n",
    "    lines.append(f\"- run_id: {best['run_id']}\")\n",
    "    lines.append(f\"- model: {best['model']}\")\n",
    "    lines.append(f\"- accuracy: {best['accuracy']}\")\n",
    "    lines.append(f\"- f1: {best['f1']}\")\n",
    "    lines.append(f\"- notes: {best['notes']}\")\n",
    "\n",
    "    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "summary_solution = summarize_runs_todo(runs)\n",
    "write_report_todo(out_dir / \"report_solution.md\", summary_solution)\n",
    "print(\"wrote\", out_dir / \"report_solution.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dde561",
   "metadata": {},
   "source": [
    "## Exercise: Live experiment comparison\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Run **two** experiments that differ by exactly one change.\n",
    "- Write a short `output/compare_runs/report.md` explaining:\n",
    "  - what changed\n",
    "  - what happened (metrics)\n",
    "  - what you think caused it\n",
    "  - what you'd try next\n",
    "\n",
    "Checkpoint:\n",
    "\n",
    "- `output/compare_runs/report.md` exists and mentions both experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 42\n",
    "    test_size: float = 0.2\n",
    "    max_iter: int = 200\n",
    "\n",
    "\n",
    "data = load_iris(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "def run_experiment(cfg: Config):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=cfg.test_size, random_state=cfg.seed, stratify=y\n",
    "    )\n",
    "    m = LogisticRegression(max_iter=cfg.max_iter)\n",
    "    m.fit(X_train, y_train)\n",
    "    pred = m.predict(X_val)\n",
    "\n",
    "    return {\n",
    "        \"config\": cfg.__dict__.copy(),\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": float(accuracy_score(y_val, pred)),\n",
    "            \"f1_macro\": float(f1_score(y_val, pred, average=\"macro\")),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "cfg_a = Config()\n",
    "cfg_b = Config(seed=42, test_size=0.2, max_iter=cfg_a.max_iter * 2)\n",
    "\n",
    "run_a = run_experiment(cfg_a)\n",
    "run_b = run_experiment(cfg_b)\n",
    "\n",
    "report_md = \"\\n\".join(\n",
    "    [\n",
    "        \"# Experiment Comparison Report\",\n",
    "        \"\",\n",
    "        \"## What changed\",\n",
    "        \"TODO: Describe the one change you made (max_iter / solver / model type).\",\n",
    "        \"\",\n",
    "        \"## Results\",\n",
    "        f\"- Experiment A config: {run_a['config']}\",\n",
    "        f\"- Experiment A metrics: {run_a['metrics']}\",\n",
    "        f\"- Experiment B config: {run_b['config']}\",\n",
    "        f\"- Experiment B metrics: {run_b['metrics']}\",\n",
    "        \"\",\n",
    "        \"## Why you think it happened\",\n",
    "        \"TODO: Write 2-5 sentences.\",\n",
    "        \"\",\n",
    "        \"## Next experiment\",\n",
    "        \"TODO: What will you try next?\",\n",
    "        \"\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "(out_dir / \"report.md\").write_text(report_md, encoding=\"utf-8\")\n",
    "print(\"wrote\", out_dir / \"report.md\")\n",
    "run_a, run_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7e32d",
   "metadata": {},
   "source": [
    "### Solution: Live experiment comparison\n",
    "\n",
    "Reference approach for comparing experiments and writing a short report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7742dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_run(run: dict) -> str:\n",
    "    cfg = run[\"config\"]\n",
    "    metrics = run[\"metrics\"]\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            f\"- config: {cfg}\",\n",
    "            f\"- accuracy: {metrics['accuracy']}\",\n",
    "            f\"- f1_macro: {metrics['f1_macro']}\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "report_solution = \"\\n\".join(\n",
    "    [\n",
    "        \"# Experiment Comparison Report\",\n",
    "        \"\",\n",
    "        \"## What changed\",\n",
    "        \"In Experiment B, I increased `max_iter` while holding `seed` and `test_size` constant.\",\n",
    "        \"\",\n",
    "        \"## Results\",\n",
    "        \"### Experiment A\",\n",
    "        format_run(run_a),\n",
    "        \"\",\n",
    "        \"### Experiment B\",\n",
    "        format_run(run_b),\n",
    "        \"\",\n",
    "        \"## Why you think it happened\",\n",
    "        \"Logistic regression sometimes needs more optimization steps to converge; increasing `max_iter` can improve metrics if the model was under-trained.\",\n",
    "        \"\",\n",
    "        \"## Next experiment\",\n",
    "        \"Try a different solver (e.g. `lbfgs` vs `liblinear`) or add feature scaling and compare again.\",\n",
    "        \"\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "(out_dir / \"report_solution.md\").write_text(report_solution, encoding=\"utf-8\")\n",
    "print(\"wrote\", out_dir / \"report_solution.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
