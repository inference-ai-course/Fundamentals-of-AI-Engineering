{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352160b1",
   "metadata": {},
   "source": [
    "# Week 2 — Part 01: ML Training Loop Lab\n",
    "\n",
    "**Estimated time:** 120–150 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on evaluation mindset and metrics:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Evaluation metrics (accuracy/precision/recall/F1)](../self_learn/Chapters/4/02_core_concepts.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 01)\n",
    "\n",
    "- You can run a full loop:\n",
    "  - split → train → evaluate\n",
    "- You save artifacts under `output/`:\n",
    "  - one run file under a timestamped folder (e.g. `output/run_.../result.json`)\n",
    "  - one summary file (e.g. `output/training_loop_summary.json`)\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should be able to point to:\n",
    "\n",
    "- the exact `result.json` that produced one metric\n",
    "- the `training_loop_summary.json` that ranks multiple configs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement a complete ML training loop (split → train → evaluate → save)\n",
    "- Understand train/validation splits\n",
    "- Practice model evaluation metrics\n",
    "- Save and reload artifacts for reproducibility\n",
    "- Compare model configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f1861",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook implements the **ML training loop** — the core engineering pattern for running, evaluating, and saving machine learning experiments.\n",
    "\n",
    "The loop has 5 steps: **Load → Split → Train → Evaluate → Save artifacts**\n",
    "\n",
    "Each step produces something concrete you can inspect. By the end you will have a timestamped folder under `output/` containing the config, metrics, and model for every run — so you can always trace back \"what produced this result?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b1951",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lab is a minimal end-to-end baseline:\n",
    "\n",
    "1. Load data\n",
    "2. Split train/validation (fixed seed)\n",
    "3. Train a baseline model\n",
    "4. Evaluate on validation\n",
    "5. Save artifacts to `output/`\n",
    "\n",
    "If you need a refresher on why we split data and how to interpret metrics, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8c6f0",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `TrainConfig` — a typed dataclass holding all hyperparameters — and `run_once()` which executes one full training loop iteration.\n",
    "\n",
    "**Key design decisions:**\n",
    "- **`TrainConfig` dataclass:** All parameters in one place. When you save `config.json`, you save the exact settings that produced the result. No guessing later.\n",
    "- **`stratify=y` in `train_test_split`:** Ensures each class appears proportionally in both train and validation sets. Without this, a small dataset might have all examples of one class in train and none in validation.\n",
    "- **`StandardScaler`:** Fit on train data only, then applied to validation. Fitting on the full dataset would \"leak\" validation statistics into training — a subtle but common mistake.\n",
    "- **Artifacts saved per run:** `result.json` (single run) and `training_loop_summary.json` (all candidates ranked). This is your audit trail.\n",
    "\n",
    "**What to check:** After running, open `output/run_.../result.json` and verify the metrics match what's printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dca3a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T01:04:17.475181Z",
     "iopub.status.busy": "2026-02-20T01:04:17.474939Z",
     "iopub.status.idle": "2026-02-20T01:04:18.604105Z",
     "shell.execute_reply": "2026-02-20T01:04:18.603036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainConfig(seed=7, test_size=0.25, max_iter=250)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seed: int = 7\n",
    "    test_size: float = 0.25\n",
    "    max_iter: int = 250\n",
    "\n",
    "\n",
    "cfg = TrainConfig()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f19e4",
   "metadata": {},
   "source": [
    "### Task 1.1: Load Data\n",
    "\n",
    "Load a dataset and inspect basic shapes/labels. We'll use Iris for a small reproducible example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a57edc32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T01:04:18.606308Z",
     "iopub.status.busy": "2026-02-20T01:04:18.605966Z",
     "iopub.status.idle": "2026-02-20T01:04:18.618625Z",
     "shell.execute_reply": "2026-02-20T01:04:18.617934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (150, 4)\n",
      "Labels shape: (150,)\n",
      "\n",
      "First 3 rows of features:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "\n",
      "Class distribution:\n",
      "target\n",
      "0    50\n",
      "1    50\n",
      "2    50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "data = load_iris(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(\"\\nFirst 3 rows of features:\")\n",
    "print(X.head(3))\n",
    "print(\"\\nClass distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bc3a6",
   "metadata": {},
   "source": [
    "### Task 1.2: Train/Validation Split & Preprocessing\n",
    "\n",
    "We need to hold out some data to honestly evaluate the model. We also scale the features. \n",
    "**Crucial**: Fit the scaler only on the training data, then transform the validation data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fe8039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T01:04:18.620470Z",
     "iopub.status.busy": "2026-02-20T01:04:18.620189Z",
     "iopub.status.idle": "2026-02-20T01:04:18.629933Z",
     "shell.execute_reply": "2026-02-20T01:04:18.629209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 112\n",
      "Validation set size: 38\n",
      "\n",
      "Feature means after scaling (Train): [ 5.70971841e-16  1.01506105e-15 -1.26882631e-16  2.29974769e-16]\n",
      "Feature means after scaling (Val): [-0.16690571 -0.10741899 -0.07138625 -0.07201851]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=cfg.test_size, random_state=cfg.seed, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "\n",
    "print(\"\\nFeature means after scaling (Train):\", X_train_s.mean(axis=0))\n",
    "print(\"Feature means after scaling (Val):\", X_val_s.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd79fff",
   "metadata": {},
   "source": [
    "### Task 1.3: Train the Model\n",
    "\n",
    "We initialize and train a `LogisticRegression` baseline model using parameters from `TrainConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e2b703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T01:04:18.631798Z",
     "iopub.status.busy": "2026-02-20T01:04:18.631637Z",
     "iopub.status.idle": "2026-02-20T01:04:18.641058Z",
     "shell.execute_reply": "2026-02-20T01:04:18.640311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained in 0.0050 seconds\n",
      "Model classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=cfg.max_iter, random_state=cfg.seed)\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train_s, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"Model trained in {train_time:.4f} seconds\")\n",
    "print(f\"Model classes: {model.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e5c65",
   "metadata": {},
   "source": [
    "### Task 1.4: Evaluate the Model\n",
    "\n",
    "We compute metrics on the validation set, including a full classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fe5959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T01:04:18.642945Z",
     "iopub.status.busy": "2026-02-20T01:04:18.642639Z",
     "iopub.status.idle": "2026-02-20T01:04:18.658708Z",
     "shell.execute_reply": "2026-02-20T01:04:18.657980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9737\n",
      "F1 (macro): 0.9733\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.92      0.96        13\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_val_s)\n",
    "\n",
    "acc = float(accuracy_score(y_val, pred))\n",
    "f1 = float(f1_score(y_val, pred, average=\"macro\"))\n",
    "report = classification_report(y_val, pred)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": acc,\n",
    "    \"f1_macro\": f1,\n",
    "    \"train_time_seconds\": train_time\n",
    "}\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 (macro): {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284c9e7",
   "metadata": {},
   "source": [
    "### Task 1.5: Save Artifacts\n",
    "\n",
    "This is the core of ML discipline. Every run produces a traceable folder containing the configuration, metrics, reports, and the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18043fed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T01:04:18.660467Z",
     "iopub.status.busy": "2026-02-20T01:04:18.660249Z",
     "iopub.status.idle": "2026-02-20T01:04:18.668253Z",
     "shell.execute_reply": "2026-02-20T01:04:18.667425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifacts to output/run_20260220_010418:\n",
      " - model.joblib\n",
      " - val_report.txt\n",
      " - config.json\n",
      " - metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54011/4216108549.py:2: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  run_id = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S\")\n"
     ]
    }
   ],
   "source": [
    "# Create a unique run ID using the current timestamp\n",
    "run_id = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "run_dir = OUTPUT_DIR / run_id\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Save config\n",
    "config_dict = {\"seed\": cfg.seed, \"test_size\": cfg.test_size, \"max_iter\": cfg.max_iter}\n",
    "(run_dir / \"config.json\").write_text(json.dumps(config_dict, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# 2. Save metrics\n",
    "(run_dir / \"metrics.json\").write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# 3. Save validation report\n",
    "(run_dir / \"val_report.txt\").write_text(report, encoding=\"utf-8\")\n",
    "\n",
    "# 4. Save model\n",
    "joblib.dump(model, run_dir / \"model.joblib\")\n",
    "\n",
    "print(f\"Saved artifacts to {run_dir}:\")\n",
    "for f in run_dir.iterdir():\n",
    "    print(f\" - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527755f",
   "metadata": {},
   "source": [
    "### Task 1.6: The Full Loop (Putting it together)\n",
    "\n",
    "Let's wrap these steps into a function, run multiple configurations, and save a summary. This proves we can automate experimentation and compare runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3f7d5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T01:04:18.670467Z",
     "iopub.status.busy": "2026-02-20T01:04:18.670242Z",
     "iopub.status.idle": "2026-02-20T01:04:18.725989Z",
     "shell.execute_reply": "2026-02-20T01:04:18.725181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running candidates...\n",
      "\n",
      "Wrote summary to: output/training_loop_summary.json\n",
      "Best config: {'seed': 13, 'test_size': 0.2, 'max_iter': 250}\n",
      "Best metrics: {'accuracy': 1.0, 'f1_macro': 1.0, 'train_time_seconds': 0.002576589584350586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54011/4015287313.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rid = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S_%f\") # add microsecond to avoid rapid run clashes\n",
      "/tmp/ipykernel_54011/4015287313.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rid = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S_%f\") # add microsecond to avoid rapid run clashes\n",
      "/tmp/ipykernel_54011/4015287313.py:29: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  rid = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S_%f\") # add microsecond to avoid rapid run clashes\n"
     ]
    }
   ],
   "source": [
    "def run_once(seed: int, test_size: float, max_iter: int) -> dict:\n",
    "    \"\"\"Executes a full training loop and saves artifacts.\"\"\"\n",
    "    # Split\n",
    "    X_tr, X_v, y_tr, y_v = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale\n",
    "    scl = StandardScaler()\n",
    "    X_tr_s = scl.fit_transform(X_tr)\n",
    "    X_v_s = scl.transform(X_v)\n",
    "    \n",
    "    # Train\n",
    "    t0 = time.time()\n",
    "    clf = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "    clf.fit(X_tr_s, y_tr)\n",
    "    t_diff = time.time() - t0\n",
    "    \n",
    "    # Evaluate\n",
    "    p = clf.predict(X_v_s)\n",
    "    m = {\n",
    "        \"accuracy\": float(accuracy_score(y_v, p)),\n",
    "        \"f1_macro\": float(f1_score(y_v, p, average=\"macro\")),\n",
    "        \"train_time_seconds\": t_diff\n",
    "    }\n",
    "    rep = classification_report(y_v, p)\n",
    "    \n",
    "    # Save Artifacts\n",
    "    rid = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S_%f\") # add microsecond to avoid rapid run clashes\n",
    "    rdir = OUTPUT_DIR / rid\n",
    "    rdir.mkdir(exist_ok=True)\n",
    "    \n",
    "    cfg_dict = {\"seed\": seed, \"test_size\": test_size, \"max_iter\": max_iter}\n",
    "    (rdir / \"config.json\").write_text(json.dumps(cfg_dict, indent=2), encoding=\"utf-8\")\n",
    "    (rdir / \"metrics.json\").write_text(json.dumps(m, indent=2), encoding=\"utf-8\")\n",
    "    (rdir / \"val_report.txt\").write_text(rep, encoding=\"utf-8\")\n",
    "    joblib.dump(clf, rdir / \"model.joblib\")\n",
    "    \n",
    "    return {\n",
    "        \"run_id\": rid,\n",
    "        \"config\": cfg_dict,\n",
    "        \"metrics\": m\n",
    "    }\n",
    "\n",
    "# Run multiple candidates\n",
    "candidates = [\n",
    "    {\"seed\": 7, \"test_size\": 0.25, \"max_iter\": 100},\n",
    "    {\"seed\": 7, \"test_size\": 0.25, \"max_iter\": 400},\n",
    "    {\"seed\": 13, \"test_size\": 0.20, \"max_iter\": 250},\n",
    "]\n",
    "\n",
    "print(\"Running candidates...\")\n",
    "results = [run_once(**c) for c in candidates]\n",
    "results_sorted = sorted(results, key=lambda r: r[\"metrics\"][\"accuracy\"], reverse=True)\n",
    "\n",
    "summary = {\n",
    "    \"best\": results_sorted[0],\n",
    "    \"all\": results_sorted,\n",
    "}\n",
    "\n",
    "summary_path = OUTPUT_DIR / \"training_loop_summary.json\"\n",
    "summary_path.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\nWrote summary to: {summary_path}\")\n",
    "print(\"Best config:\", summary[\"best\"][\"config\"])\n",
    "print(\"Best metrics:\", summary[\"best\"][\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
