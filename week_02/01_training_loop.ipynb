{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Part 01: ML Training Loop Lab\n",
    "\n",
    "**Estimated time:** 120–150 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on evaluation mindset and metrics:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Evaluation metrics (accuracy/precision/recall/F1)](../self_learn/Chapters/4/02_core_concepts.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 01)\n",
    "\n",
    "- You can run a full loop:\n",
    "  - split → train → evaluate\n",
    "- You save artifacts under `output/`:\n",
    "  - one run file under a timestamped folder (e.g. `output/run_.../result.json`)\n",
    "  - one summary file (e.g. `output/training_loop_summary.json`)\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should be able to point to:\n",
    "\n",
    "- the exact `result.json` that produced one metric\n",
    "- the `training_loop_summary.json` that ranks multiple configs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement a complete ML training loop (split → train → evaluate → save)\n",
    "- Understand train/validation splits\n",
    "- Practice model evaluation metrics\n",
    "- Save and reload artifacts for reproducibility\n",
    "- Compare model configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f1861",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook implements the **ML training loop** — the core engineering pattern for running, evaluating, and saving machine learning experiments.\n",
    "\n",
    "The loop has 5 steps: **Load → Split → Train → Evaluate → Save artifacts**\n",
    "\n",
    "Each step produces something concrete you can inspect. By the end you will have a timestamped folder under `output/` containing the config, metrics, and model for every run — so you can always trace back \"what produced this result?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b1951",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lab is a minimal end-to-end baseline:\n",
    "\n",
    "1. Load data\n",
    "2. Split train/validation (fixed seed)\n",
    "3. Train a baseline model\n",
    "4. Evaluate on validation\n",
    "5. Save artifacts to `output/`\n",
    "\n",
    "If you need a refresher on why we split data and how to interpret metrics, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8c6f0",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `TrainConfig` — a typed dataclass holding all hyperparameters — and `run_once()` which executes one full training loop iteration.\n",
    "\n",
    "**Key design decisions:**\n",
    "- **`TrainConfig` dataclass:** All parameters in one place. When you save `config.json`, you save the exact settings that produced the result. No guessing later.\n",
    "- **`stratify=y` in `train_test_split`:** Ensures each class appears proportionally in both train and validation sets. Without this, a small dataset might have all examples of one class in train and none in validation.\n",
    "- **`StandardScaler`:** Fit on train data only, then applied to validation. Fitting on the full dataset would \"leak\" validation statistics into training — a subtle but common mistake.\n",
    "- **Artifacts saved per run:** `result.json` (single run) and `training_loop_summary.json` (all candidates ranked). This is your audit trail.\n",
    "\n",
    "**What to check:** After running, open `output/run_.../result.json` and verify the metrics match what's printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dca3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seed: int = 7\n",
    "    test_size: float = 0.25\n",
    "    max_iter: int = 250\n",
    "\n",
    "\n",
    "cfg = TrainConfig()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f19e4",
   "metadata": {},
   "source": [
    "### Task 1.1: Load Data\n",
    "\n",
    "Load a dataset and inspect basic shapes/labels. We'll use Iris for a small reproducible example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_once(seed: int, test_size: float, max_iter: int) -> dict:\n",
    "    data = load_iris(as_frame=True)\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "\n",
    "    model = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    pred = model.predict(X_val_s)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_val, pred)),\n",
    "        \"f1_macro\": float(f1_score(y_val, pred, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"config\": {\"seed\": seed, \"test_size\": test_size, \"max_iter\": max_iter},\n",
    "        \"metrics\": metrics,\n",
    "        \"class_counts_train\": [int(x) for x in np.bincount(y_train)],\n",
    "        \"class_counts_val\": [int(x) for x in np.bincount(y_val)],\n",
    "    }\n",
    "\n",
    "\n",
    "single = run_once(seed=cfg.seed, test_size=cfg.test_size, max_iter=cfg.max_iter)\n",
    "print(\"single run:\", single[\"config\"], single[\"metrics\"])\n",
    "\n",
    "run_id = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "run_dir = OUTPUT_DIR / run_id\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "(run_dir / \"result.json\").write_text(json.dumps(single, indent=2), encoding=\"utf-8\")\n",
    "print(\"wrote:\", run_dir / \"result.json\")\n",
    "\n",
    "candidates = [\n",
    "    {\"seed\": 7, \"test_size\": 0.25, \"max_iter\": 100},\n",
    "    {\"seed\": 7, \"test_size\": 0.25, \"max_iter\": 400},\n",
    "    {\"seed\": 13, \"test_size\": 0.20, \"max_iter\": 250},\n",
    "]\n",
    "\n",
    "results = [run_once(**c) for c in candidates]\n",
    "results_sorted = sorted(results, key=lambda r: r[\"metrics\"][\"accuracy\"], reverse=True)\n",
    "\n",
    "summary = {\n",
    "    \"best\": results_sorted[0],\n",
    "    \"all\": results_sorted,\n",
    "}\n",
    "\n",
    "(OUTPUT_DIR / \"training_loop_summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"wrote:\", OUTPUT_DIR / \"training_loop_summary.json\")\n",
    "print(\"best config:\", summary[\"best\"][\"config\"], \"metrics:\", summary[\"best\"][\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
