{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundamental Course — Week 5 Capstone Practice: Local inference benchmarking (Ollama)\n",
        "\n",
        "This notebook is a guided mini-capstone. Your goal is to produce **benchmark artifacts + written conclusions** comparing local models.\n",
        "\n",
        "## What success looks like\n",
        "\n",
        "- You benchmark **2+ models** on a **consistent prompt set** (suggested: 10–20 prompts).\n",
        "- You save results to disk under:\n",
        "  - `output/week_05_bench/results.json`\n",
        "- You can answer:\n",
        "  - best model for speed\n",
        "  - best model for quality\n",
        "  - major failure modes (timeouts, low-quality outputs, format failures)\n",
        "\n",
        "## References (docs)\n",
        "- Ollama (official): https://ollama.com/\n",
        "- Ollama GitHub (docs): https://github.com/ollama/ollama\n",
        "- Python `time` (official): https://docs.python.org/3/library/time.html\n",
        "- `requests` (official docs): https://requests.readthedocs.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "This assumes you have Ollama running locally.\n",
        "\n",
        "Checklist:\n",
        "\n",
        "- `ollama --version` works\n",
        "- `ollama serve` is running\n",
        "- `ollama list` shows at least 2 models (pull more if needed)\n",
        "\n",
        "Default endpoint:\n",
        "\n",
        "- `http://localhost:11434`\n",
        "\n",
        "If the endpoint is different, update `OLLAMA_BASE_URL`.\n",
        "\n",
        "### Checkpoint\n",
        "\n",
        "After you run the health check cell below, you should see a list of available model names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
        "GENERATE_URL = f'{OLLAMA_BASE_URL}/api/generate'\n",
        "GENERATE_URL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Define a benchmark set (run after Step 0 health check)\n",
        "\n",
        "Keep your benchmark **small and consistent** so model comparisons are fair.\n",
        "\n",
        "Suggested prompt categories (mix at least 2–3 per category):\n",
        "\n",
        "- JSON extraction (format adherence)\n",
        "- summarization (faithfulness)\n",
        "- short instruction following\n",
        "\n",
        "Start with 3 prompts to verify the loop works. Then scale to 10–20 prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPTS = [\n",
        "    # Start small (3 prompts) to validate the benchmark loop works.\n",
        "    \"Summarize: Local inference trades API cost for hardware constraints.\",\n",
        "    \"Extract as JSON with keys {name, email} from: John Doe (john@example.com).\",\n",
        "    \"Write 3 bullets: pros/cons of local inference.\",\n",
        "]\n",
        "\n",
        "# TODO: choose 2–3 models you have pulled (see the health check output / `ollama list`).\n",
        "# Tip: include at least one smaller/faster model and one larger/slower model.\n",
        "MODELS = []\n",
        "\n",
        "PROMPTS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd03000e",
      "metadata": {},
      "source": [
        "## Step 0: Health check (run this first)\n",
        "\n",
        "If this fails, fix Ollama first before debugging your benchmark code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8da4606",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "TAGS_URL = f\"{OLLAMA_BASE_URL}/api/tags\"\n",
        "\n",
        "try:\n",
        "    resp = requests.get(TAGS_URL, timeout=2.0)\n",
        "    resp.raise_for_status()\n",
        "    tags = resp.json()\n",
        "    AVAILABLE_MODELS = [m.get(\"name\") for m in tags.get(\"models\", []) if m.get(\"name\")]\n",
        "    print(\"ollama_ok=True\")\n",
        "    print(\"available_models=\", AVAILABLE_MODELS)\n",
        "except Exception as e:\n",
        "    AVAILABLE_MODELS = []\n",
        "    print(\"ollama_ok=False\")\n",
        "    print(\"error=\", type(e).__name__, str(e))\n",
        "    print(\"Next steps:\")\n",
        "    print(\"- Start server: ollama serve\")\n",
        "    print(\"- Pull a model: ollama pull <model>\")\n",
        "    print(\"- Confirm: ollama list\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollama generate call\n",
        "\n",
        "We call the HTTP API and measure latency.\n",
        "If you get connection errors, confirm Ollama is running and your model name is correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ollama_generate(model: str, prompt: str, *, timeout_s: float = 60.0) -> Dict[str, Any]:\n",
        "    payload = {\n",
        "        'model': model,\n",
        "        'prompt': prompt,\n",
        "        'stream': False,\n",
        "    }\n",
        "    resp = requests.post(GENERATE_URL, json=payload, timeout=timeout_s)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark loop (starter)\n",
        "\n",
        "Stores latency and a small output sample for later inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-fill MODELS (so the notebook can run end-to-end)\n",
        "# For the assignment, you should still choose MODELS yourself.\n",
        "if not MODELS:\n",
        "    try:\n",
        "        MODELS = AVAILABLE_MODELS[:2]\n",
        "    except Exception:\n",
        "        MODELS = []\n",
        "\n",
        "if not MODELS:\n",
        "    print(\"No MODELS selected.\")\n",
        "    print(\"Next steps:\")\n",
        "    print(\"- Ensure you pulled at least one model: ollama pull <model>\")\n",
        "    print(\"- Set MODELS = [...] to model names shown by the health check / ollama list\")\n",
        "\n",
        "results = []\n",
        "for model in MODELS:\n",
        "    for prompt in PROMPTS:\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            out = ollama_generate(model, prompt)\n",
        "            ok = True\n",
        "            text = out.get('response', '')\n",
        "        except Exception as e:\n",
        "            ok = False\n",
        "            text = f'ERROR: {type(e).__name__}: {e}'\n",
        "        latency_ms = int((time.time() - t0) * 1000)\n",
        "        results.append({\n",
        "            'model': model,\n",
        "            'prompt': prompt,\n",
        "            'ok': ok,\n",
        "            'latency_ms': latency_ms,\n",
        "            'output_preview': text[:400],\n",
        "        })\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776ce89e",
      "metadata": {},
      "source": [
        "## Step 3: Save artifacts\n",
        "\n",
        "Write a single `results.json` file so you can review quality later and share evidence.\n",
        "\n",
        "### Checkpoint\n",
        "\n",
        "After running the next cell, you should have:\n",
        "\n",
        "- `output/week_05_bench/results.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a2f2618",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DIR = Path(\"output/week_05_bench\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "out_path = OUT_DIR / \"results.json\"\n",
        "out_path.write_text(json.dumps(results, indent=2), encoding=\"utf-8\")\n",
        "print(\"wrote=\", out_path.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0ea4a6",
      "metadata": {},
      "source": [
        "## Step 4: Conclusions template\n",
        "\n",
        "Fill this in after you run at least 2 models.\n",
        "\n",
        "- Best for speed:\n",
        "- Best for quality:\n",
        "- Biggest failure modes:\n",
        "- When you would choose each model:\n",
        "\n",
        "Optional: paste 1–2 example outputs that demonstrate the differences."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c403d4d",
      "metadata": {},
      "source": [
        "## Optional: Choose your models (do this before running the benchmark loop)\n",
        "\n",
        "You already have a `MODELS = []` TODO near the top of the notebook.\n",
        "\n",
        "If you want to override what the notebook auto-selects, set `MODELS` manually (2–3 models) and then re-run the benchmark loop cell.\n",
        "\n",
        "Checklist:\n",
        "\n",
        "- At least one smaller/faster model\n",
        "- At least one larger/slower model\n",
        "\n",
        "You can copy names from the health check output (`AVAILABLE_MODELS`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e97320a",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"AVAILABLE_MODELS=\", AVAILABLE_MODELS)\n",
        "print(\"MODELS=\", MODELS)\n",
        "\n",
        "print(\"If you change MODELS, re-run the benchmark loop cell to regenerate results.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Solutions (peek only after trying)\n",
        "\n",
        "Reference defaults that make the notebook runnable.\n",
        "\n",
        "Use these only if you got stuck; for a meaningful comparison you should still choose your own prompt set and models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9bf181",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reference: pick up to 2 models automatically if you left MODELS empty.\n",
        "# For the assignment, explicitly set MODELS yourself.\n",
        "if not MODELS and AVAILABLE_MODELS:\n",
        "    MODELS = AVAILABLE_MODELS[:2]\n",
        "    print(\"MODELS auto-filled:\", MODELS)\n",
        "\n",
        "# Reference: expand prompts (copy this pattern)\n",
        "# PROMPTS = PROMPTS + [\"Add more prompts here...\"]\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
