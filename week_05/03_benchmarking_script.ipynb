{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Part 03: Benchmarking script (latency + quality artifacts)\n",
    "\n",
    "**Estimated time:** 90–150 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Explain why benchmarks must control variables\n",
    "- Measure latency as a distribution (not a single number)\n",
    "- Save benchmark artifacts to disk for manual quality review\n",
    "- Build a small benchmark harness for local models (Ollama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8353e",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook builds a **benchmarking harness** — a script that runs the same prompts across multiple models and records latency + outputs for comparison.\n",
    "\n",
    "**Why benchmark systematically?**\n",
    "- Latency is a distribution, not a single number. One fast run doesn't tell you the typical or worst-case performance.\n",
    "- Quality varies by task. A model that's fast for simple extraction may be slow or inaccurate for complex reasoning.\n",
    "- Without consistent inputs (same prompts, same settings), you're comparing apples to oranges.\n",
    "\n",
    "**Benchmark hygiene rules:**\n",
    "1. Same prompt set for all models\n",
    "2. Warmup run per model (don't count model-loading time)\n",
    "3. Save all outputs to disk for manual quality review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d6df6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A benchmark must be consistent:\n",
    "\n",
    "- same prompt set\n",
    "- same measurement method\n",
    "- same saved outputs\n",
    "\n",
    "We will write a small benchmark harness that:\n",
    "\n",
    "- loops over prompts\n",
    "- loops over models\n",
    "- records latency\n",
    "- saves outputs to disk for later quality review\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You can run a benchmark over **at least 2 models** and **at least 5 prompts**.\n",
    "- You write JSON artifacts to `output/benchmarks/`.\n",
    "- You produce a `summary.json` with basic latency distribution statistics.\n",
    "\n",
    "If you cannot reach Ollama, fix the local runtime first (Week 5 Part 01).\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: benchmarking is measurement under controlled conditions\n",
    "\n",
    "You are trying to estimate:\n",
    "\n",
    "- **speed** (latency / throughput)\n",
    "- **quality** (correctness, format adherence, completeness)\n",
    "\n",
    "The key rule is controlling variables:\n",
    "\n",
    "- same prompts\n",
    "- same settings\n",
    "- same machine state as much as possible\n",
    "\n",
    "Latency is a distribution, not a single number. Two useful summaries:\n",
    "\n",
    "- average latency (typical case)\n",
    "- slowest case / tail latency (worst case)\n",
    "\n",
    "Practical implication: a model that is “fast on average” but has very slow worst cases may still feel bad in a demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc658c",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines the benchmark infrastructure:\n",
    "\n",
    "- **`BenchmarkItem`** — one (model, prompt, prompt_id) combination to run\n",
    "- **`call_ollama_local()`** — makes one HTTP call and returns model, prompt_id, response, and latency\n",
    "- **`summarize_latencies()`** — computes n, average, P95, and max latency from a list of results\n",
    "\n",
    "**Why P95 latency?** Average latency tells you the typical case. P95 (the 95th percentile) tells you the worst case most users will experience. A model with avg=1s but P95=10s will feel slow to 1 in 20 users — important to know before choosing it for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607048ca",
   "metadata": {},
   "source": [
    "## Benchmark harness (example)\n",
    "\n",
    "Benchmark hygiene notes:\n",
    "\n",
    "- consider a warmup run per model (do not record it) to avoid counting model load time\n",
    "- keep prompts short enough that you are comparing models, not just tokenization overhead\n",
    "- avoid changing the prompt set while comparing models (version your prompt list)\n",
    "\n",
    "Next you’ll implement a simple benchmark harness that saves JSON artifacts to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c2cee",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements `run_benchmark()` — the main harness that loops over all (model, prompt) combinations, runs each one, saves a JSON artifact per result, and writes a `summary.json`.\n",
    "\n",
    "**Key design decisions:**\n",
    "- **Warmup per model:** The first call to a model is slow (loading weights into memory). The warmup call is discarded so it doesn't skew your latency measurements.\n",
    "- **One JSON file per result:** Naming pattern `{model}_{prompt_id}.json` makes it easy to find specific results later. You can inspect any individual output without parsing a large combined file.\n",
    "- **`summary.json`:** Aggregated statistics (avg, P95, max latency) across all runs. This is what you report in your comparison.\n",
    "\n",
    "**What to check after running:** Open `output/benchmarks/summary.json` and verify the latency numbers look reasonable for your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception as e:  # pragma: no cover\n",
    "    requests = None\n",
    "    _requests_import_error = e\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BenchmarkItem:\n",
    "    model: str\n",
    "    prompt: str\n",
    "    prompt_id: str\n",
    "\n",
    "\n",
    "def call_ollama_local(host: str, item: BenchmarkItem, *, timeout_s: float = 120.0) -> Dict[str, Any]:\n",
    "    if requests is None:\n",
    "        raise RuntimeError(\"requests is required: %s\" % _requests_import_error)\n",
    "\n",
    "    url = \"%s/api/generate\" % host\n",
    "    payload = {\"model\": item.model, \"prompt\": item.prompt, \"stream\": False}\n",
    "    t0 = time.time()\n",
    "    resp = requests.post(url, json=payload, timeout=timeout_s)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return {\n",
    "        \"model\": item.model,\n",
    "        \"prompt_id\": item.prompt_id,\n",
    "        \"response\": data.get(\"response\", \"\"),\n",
    "        \"latency_s\": time.time() - t0,\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_latencies(results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    lats = [float(r[\"latency_s\"]) for r in results]\n",
    "    if not lats:\n",
    "        return {\"n\": 0}\n",
    "    return {\n",
    "        \"n\": len(lats),\n",
    "        \"avg_latency_s\": sum(lats) / len(lats),\n",
    "        \"p95_latency_s\": sorted(lats)[max(0, int(len(lats) * 0.95) - 1)],\n",
    "        \"max_latency_s\": max(lats),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"benchmark helpers ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bd1e8",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Runs a tiny benchmark (1 model × 2 prompts) to validate the full harness end-to-end.\n",
    "\n",
    "**What to notice:**\n",
    "- `list_models_via_tags()` auto-discovers available models — no hardcoding needed\n",
    "- Only 1 model is used for the first run (`models[:1]`) to keep it fast\n",
    "- If Ollama is not running, the error message tells you exactly what to do\n",
    "\n",
    "**Your task:** Implement `list_models_via_tags_todo()` — a reusable version of the model discovery function. Once implemented, you can use it in any script to avoid hardcoding model names. The solution is in the Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "\n",
    "def run_benchmark(items: List[BenchmarkItem], *, host: str, out_dir: Path) -> List[Dict[str, Any]]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    seen_models: Set[str] = set()\n",
    "    for item in items:\n",
    "        if item.model not in seen_models:\n",
    "            seen_models.add(item.model)\n",
    "            _ = call_ollama_local(host, BenchmarkItem(item.model, \"Warmup\", \"warmup\"), timeout_s=120.0)\n",
    "\n",
    "        r = call_ollama_local(host, item, timeout_s=120.0)\n",
    "        results.append(r)\n",
    "        out_path = out_dir / (\"%s_%s.json\" % (item.model, item.prompt_id))\n",
    "        out_path.write_text(json.dumps(r, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    summary = summarize_latencies(results)\n",
    "    (out_dir / \"summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (requires Ollama running locally):\n",
    "# host = \"http://localhost:11434\"\n",
    "# prompts = [\n",
    "#     (\"p01\", \"Summarize: Large language models are useful but require careful evaluation.\"),\n",
    "#     (\"p02\", \"Extract JSON with keys {name, email} from: 'Name: Sam, Email: sam@example.com'\"),\n",
    "#     (\"p03\", \"Write 3 bullet points about caching.\"),\n",
    "# ]\n",
    "# items = [BenchmarkItem(model=m, prompt=p, prompt_id=pid) for m in [\"llama3.1\", \"qwen2.5\"] for pid, p in prompts]\n",
    "# results = run_benchmark(items, host=host, out_dir=Path(\"output/benchmarks\"))\n",
    "# print(\"wrote\", len(results), \"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2872bc",
   "metadata": {},
   "source": [
    "## Guided walkthrough: run a tiny benchmark once\n",
    "\n",
    "Goal: verify the full flow **models × prompts → artifacts on disk → summary.json**.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "- You should see JSON files created under `output/benchmarks/`.\n",
    "- If Ollama is not running, you should see a clear error and next steps.\n",
    "\n",
    "Tip: start with 1 model and 2–3 prompts, then scale up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c6c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "DEFAULT_HOST = \"http://localhost:11434\"\n",
    "\n",
    "# Tiny benchmark inputs (start small, then scale up)\n",
    "PROMPTS = [\n",
    "    (\"p01\", \"Summarize: Local inference trades cost for hardware constraints.\"),\n",
    "    (\"p02\", \"Write 3 bullet points about benchmarking.\"),\n",
    "]\n",
    "\n",
    "\n",
    "def list_models_via_tags(host: str) -> List[str]:\n",
    "    # Minimal helper used by the walkthrough.\n",
    "    if requests is None:\n",
    "        raise RuntimeError(\"requests is required: %s\" % _requests_import_error)\n",
    "    resp = requests.get(\"%s/api/tags\" % host, timeout=2.0)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return [m.get(\"name\") for m in data.get(\"models\", []) if m.get(\"name\")]\n",
    "\n",
    "\n",
    "try:\n",
    "    models = list_models_via_tags(DEFAULT_HOST)\n",
    "    if not models:\n",
    "        print(\"No local models found.\")\n",
    "        print(\"Next steps:\")\n",
    "        print(\"- Pull a model: ollama pull <model>\")\n",
    "        print(\"- Confirm: ollama list\")\n",
    "    else:\n",
    "        # Use 1 model for the first run to validate the harness.\n",
    "        MODELS = models[:1]\n",
    "        items = [BenchmarkItem(model=m, prompt=p, prompt_id=pid) for m in MODELS for pid, p in PROMPTS]\n",
    "        results = run_benchmark(items, host=DEFAULT_HOST, out_dir=Path(\"output/benchmarks\"))\n",
    "        print(\"wrote_results=\", len(results))\n",
    "        print(\"summary_path=\", Path(\"output/benchmarks/summary.json\").resolve())\n",
    "except Exception as e:\n",
    "    print(\"benchmark_ok=False\")\n",
    "    print(\"error=\", type(e).__name__, str(e))\n",
    "    print(\"Next steps:\")\n",
    "    print(\"- Ensure Ollama is running: ollama serve\")\n",
    "    print(\"- Ensure /api/tags is reachable: http://localhost:11434/api/tags\")\n",
    "    print(\"- Ensure you pulled a model: ollama pull <model>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8dda6",
   "metadata": {},
   "source": [
    "## Exercise: make model discovery reusable\n",
    "\n",
    "Implement the TODO below.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Return a list of model names available locally (via `GET /api/tags`).\n",
    "- Fail fast with a clear error if Ollama is not reachable.\n",
    "\n",
    "You’ll use this to avoid hardcoding `MODELS = [...]` in future scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc50265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def list_models_via_tags_todo(host: str = \"http://localhost:11434\") -> List[str]:\n",
    "    # TODO: implement.\n",
    "    # Suggested approach:\n",
    "    # - GET {host}/api/tags with a short timeout\n",
    "    # - parse JSON: data['models'][*]['name']\n",
    "    # - return List[str]\n",
    "    return []\n",
    "\n",
    "\n",
    "print(\"Implement list_models_via_tags_todo().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2845208",
   "metadata": {},
   "source": [
    "## How to compare models\n",
    "\n",
    "Compare:\n",
    "\n",
    "- **Speed**: average latency + slowest case\n",
    "- **Quality**: read saved outputs for:\n",
    "  - correctness\n",
    "  - adherence to format\n",
    "  - completeness\n",
    "\n",
    "Simple “quality heuristics” without heavy math:\n",
    "\n",
    "- for JSON prompts: count parse failures\n",
    "- for extraction: check if required keys exist\n",
    "- for summaries: check length caps and whether key facts are present\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Python `time`: https://docs.python.org/3/library/time.html\n",
    "- Python `timeit`: https://docs.python.org/3/library/timeit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4536626f",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `list_models_via_tags_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def list_models_via_tags_todo(host: str = \"http://localhost:11434\") -> List[str]:\n",
    "    if requests is None:\n",
    "        raise RuntimeError(\"requests is required: %s\" % _requests_import_error)\n",
    "    resp = requests.get(\"%s/api/tags\" % host, timeout=2.0)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return [m.get(\"name\") for m in data.get(\"models\", []) if m.get(\"name\")]\n",
    "\n",
    "\n",
    "try:\n",
    "    print(list_models_via_tags_todo()[:5])\n",
    "except Exception as e:\n",
    "    print(\"error=\", type(e).__name__, str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
