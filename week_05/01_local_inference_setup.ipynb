{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 \u2014 Part 01: Local inference concepts + setup checklist\n",
    "\n",
    "**Estimated time:** 45\u201375 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Define inference and local inference\n",
    "- Explain how moving from hosted APIs to local inference changes constraints\n",
    "- Understand how model size, context length, and quantization affect latency and memory\n",
    "- Follow a practical setup checklist for Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31a08c",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook covers **local inference** \u2014 running LLM models on your own machine instead of calling a cloud API.\n",
    "\n",
    "**Key trade-offs vs cloud APIs:**\n",
    "| | Cloud API | Local (Ollama) |\n",
    "|---|---|---|\n",
    "| Privacy | Data leaves your machine | Data stays local |\n",
    "| Cost | Pay per token | Free (your hardware) |\n",
    "| Model quality | Best available (GPT-4, Claude) | Smaller models (1B\u201313B) |\n",
    "| Setup | API key only | Install Ollama + pull model |\n",
    "\n",
    "**Hardware is now part of your system:** If a model doesn't fit in RAM/VRAM, it won't run. Start with the smallest model that meets your quality bar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357111d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Inference** = using a trained model to generate outputs.\n",
    "\n",
    "**Local inference** = you run the model on your own machine.\n",
    "\n",
    "Local inference is useful for:\n",
    "\n",
    "- privacy (data stays local)\n",
    "- cost control (no per-request billing)\n",
    "- offline capability\n",
    "\n",
    "Trade-offs:\n",
    "\n",
    "- quality may be lower than top hosted models\n",
    "- performance depends on your CPU/GPU/RAM/VRAM\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 01)\n",
    "\n",
    "- You can run:\n",
    "  - `ollama --version`\n",
    "  - `ollama list`\n",
    "- You can confirm the local server is reachable either by:\n",
    "  - CLI (`ollama list` succeeds), or\n",
    "  - HTTP (`GET http://localhost:11434/api/tags` succeeds)\n",
    "\n",
    "If you cannot complete the checks above, fix runtime/environment issues before writing client/benchmark code.\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: moving the boundary changes your constraints\n",
    "\n",
    "When you use a hosted API, the provider owns the compute and you mostly worry about:\n",
    "\n",
    "- request formatting\n",
    "- rate limits\n",
    "- latency and cost\n",
    "\n",
    "When you run locally, you become the provider. That means **hardware is now part of your system design**.\n",
    "\n",
    "You can think of local inference performance as a function:\n",
    "\n",
    "$$\n",
    "\\text{latency} = f(\\text{model size},\\ \\text{context length},\\ \\text{hardware},\\ \\text{quantization})\n",
    "$$\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- if a model does not fit in RAM/VRAM, it won\u2019t run (or will thrash)\n",
    "- even if it fits, throughput/latency can vary dramatically across machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61715da",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Checks your system environment: Python version, platform, whether `ollama` is in your PATH, and runs `ollama --version` and `ollama list` via subprocess.\n",
    "\n",
    "**What to look for:**\n",
    "- `ollama_in_path=True` \u2014 Ollama CLI is installed\n",
    "- `ollama --version` returns a version string \u2014 CLI works\n",
    "- `ollama list` shows your pulled models \u2014 at least one model is ready\n",
    "\n",
    "**If `ollama_in_path=False`:** Install Ollama from https://ollama.com/ and restart your terminal/kernel so the PATH is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b937e",
   "metadata": {},
   "source": [
    "## Setup checklist (practical)\n",
    "\n",
    "1. Install Ollama\n",
    "2. Start the Ollama service\n",
    "3. Pull a model\n",
    "4. Run a test prompt\n",
    "\n",
    "What to do and what \u201csuccess\u201d looks like:\n",
    "\n",
    "1. **Install Ollama**\n",
    "    - Goal: have the `ollama` CLI available.\n",
    "    - Verify: `ollama --version` prints a version.\n",
    "\n",
    "2. **Start the Ollama service**\n",
    "    - Goal: local server process ready to accept requests.\n",
    "    - Verify: `ollama serve` starts without immediately exiting.\n",
    "    - Common failure: port conflicts or permission issues.\n",
    "\n",
    "3. **Pull a model**\n",
    "    - Goal: download at least one model.\n",
    "    - Verify: `ollama list` shows the model.\n",
    "    - Practical note: start small to avoid memory failures.\n",
    "\n",
    "4. **Run a test prompt**\n",
    "    - Goal: confirm request \u2192 generation works locally.\n",
    "    - Verify: `ollama run <model_name>` produces output and doesn\u2019t crash.\n",
    "    - Note: first run can be slow due to model loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729f781",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Calls `GET http://localhost:11434/api/tags` \u2014 the Ollama health check endpoint \u2014 and prints the list of locally available models.\n",
    "\n",
    "**Why check via HTTP (not just CLI)?** Your Python client will use HTTP to call Ollama. If the HTTP endpoint is reachable, you know the full stack works: Ollama is running, the port is open, and `requests` can connect. A successful CLI check doesn't guarantee the HTTP API is accessible from Python.\n",
    "\n",
    "**What to do if this fails:**\n",
    "1. Run `ollama serve` in a terminal (starts the local server)\n",
    "2. Pull at least one model: `ollama pull llama3.2:1b`\n",
    "3. Re-run this cell"
   ]
  },
  {
   "cell_type": "code",
   "id": "e632a9d7",
   "metadata": {},
   "source": [
    "import platform\n",
    "import shutil\n",
    "import subprocess\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def try_run(cmd: List[str]) -> None:\n",
    "    print(\"$\", \" \".join(cmd))\n",
    "    try:\n",
    "        out = subprocess.run(cmd, capture_output=True, text=True, check=False)\n",
    "        print(\"returncode=\", out.returncode)\n",
    "        if out.stdout:\n",
    "            print(out.stdout.strip())\n",
    "        if out.stderr:\n",
    "            print(out.stderr.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(\"command not found\")\n",
    "\n",
    "\n",
    "print(\"python=\", platform.python_version())\n",
    "print(\"platform=\", platform.platform())\n",
    "print(\"ollama_in_path=\", shutil.which(\"ollama\") is not None)\n",
    "\n",
    "try_run([\"ollama\", \"--version\"])\n",
    "try_run([\"ollama\", \"list\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4e458ab5",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `estimate_memory_gb()` \u2014 a formula to estimate how much RAM/VRAM a model needs based on parameter count and quantization bit-width.\n",
    "\n",
    "**Formula:** `params \u00d7 (bits/8) / 1GB` \u2014 this gives the weight-only memory. Actual runtime usage is higher (roughly 1.5\u20132\u00d7 due to KV cache and activations).\n",
    "\n",
    "**Practical rule of thumb:**\n",
    "- 7B model at 4-bit \u2248 3.5 GB weights \u2192 ~5\u20137 GB total at runtime\n",
    "- 13B model at 4-bit \u2248 6.5 GB weights \u2192 ~9\u201313 GB total at runtime\n",
    "- If your machine has 8 GB RAM, stick to 3B or smaller models\n",
    "\n",
    "**Your task:** Implement `choose_model_for_hardware(vram_gb)` \u2014 a simple rule-of-thumb function that recommends a model size given available VRAM. The solution is in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0956d",
   "metadata": {},
   "source": [
    "## Guided check (recommended): verify the Ollama HTTP endpoint\n",
    "\n",
    "Even though Ollama runs on your machine, it exposes a local HTTP API (default `http://localhost:11434`).\n",
    "\n",
    "This is useful because:\n",
    "\n",
    "- it confirms the server is actually running\n",
    "- it matches how your later Python client and benchmark will call Ollama\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "- If the server is running, you should see a JSON response containing a `models` list.\n",
    "- If it is not running, you will typically see a connection/refused error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception as e:  # pragma: no cover\n",
    "    requests = None\n",
    "    _requests_import_error = e\n",
    "\n",
    "\n",
    "def fetch_ollama_tags(host: str = \"http://localhost:11434\", *, timeout_s: float = 2.0) -> dict:\n",
    "    if requests is None:\n",
    "        raise RuntimeError(f\"requests is required for HTTP health checks: {_requests_import_error}\")\n",
    "    url = f\"{host}/api/tags\"\n",
    "    resp = requests.get(url, timeout=timeout_s)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "try:\n",
    "    tags = fetch_ollama_tags()\n",
    "    model_names = [m.get(\"name\") for m in tags.get(\"models\", [])]\n",
    "    print(\"ollama_http_ok=True\")\n",
    "    print(\"models=\", model_names)\n",
    "except Exception as e:\n",
    "    print(\"ollama_http_ok=False\")\n",
    "    print(\"error=\", type(e).__name__, str(e))\n",
    "    print(\"Next steps:\")\n",
    "    print(\"- Ensure Ollama is installed: ollama --version\")\n",
    "    print(\"- Ensure the server is running: ollama serve\")\n",
    "    print(\"- Ensure at least one model is pulled: ollama pull <model>\")\n",
    "    print(\"- Then re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06690bb",
   "metadata": {},
   "source": [
    "## What \u201cmodel size / context window / quantization\u201d mean\n",
    "\n",
    "- **Size (e.g. 7B, 13B)**: larger often means better quality but slower and more memory.\n",
    "- **Context window**: how much text you can include per request.\n",
    "- **Quantization**: smaller memory footprint (quality may change slightly).\n",
    "\n",
    "More concrete intuition:\n",
    "\n",
    "- model size is roughly the number of parameters\n",
    "- more parameters usually means more compute per generated token\n",
    "- quantization stores weights with fewer bits, reducing memory and often increasing speed on constrained hardware\n",
    "\n",
    "Practical rule of thumb: local inference is often bottlenecked by memory bandwidth and/or VRAM capacity, not just CPU speed.\n",
    "\n",
    "For Foundations Course, focus on the practical effect:\n",
    "\n",
    "- if it doesn\u2019t fit, you can\u2019t run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_gb(params_billion: float, bits_per_weight: int) -> float:\n",
    "    # Weight-only estimate: params * bits per weight -> bytes -> GB\n",
    "    # Note: actual runtime memory is higher (KV cache + activations add ~1.2-2x overhead)\n",
    "    params = params_billion * 1_000_000_000\n",
    "    bytes_used = params * (bits_per_weight / 8)\n",
    "    return bytes_used / (1024 ** 3)\n",
    "\n",
    "\n",
    "print(\"Weight-only memory estimates (actual runtime usage will be higher due to KV cache + activations):\")\n",
    "sizes = [7, 13, 70]\n",
    "for s in sizes:\n",
    "    for bits in [4, 8, 16]:\n",
    "        print(f\"{s}B @ {bits}-bit: {estimate_memory_gb(s, bits):.2f} GB (weights only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_for_hardware(vram_gb: float) -> str:\n",
    "    # TODO: implement a rule-of-thumb mapping.\n",
    "    # Example:\n",
    "    # - vram < 8 -> prefer 3B or smaller\n",
    "    # - vram < 16 -> prefer 7B\n",
    "    # - otherwise -> 13B+ (if latency acceptable)\n",
    "    #\n",
    "    # Placeholder behavior (so the notebook can run end-to-end):\n",
    "    return \"TODO: implement choose_model_for_hardware(vram_gb)\"\n",
    "\n",
    "\n",
    "print(\"Implement choose_model_for_hardware().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d0df2",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Ollama: https://ollama.com/\n",
    "- Ollama GitHub: https://github.com/ollama/ollama\n",
    "- Hugging Face model cards: https://huggingface.co/docs/hub/model-cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831ab86",
   "metadata": {},
   "source": [
    "## Exercise: implement a health check function\n",
    "\n",
    "In later notebooks you\u2019ll call Ollama via HTTP. Before you do that, you need a quick local health check.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Return `True` only when Ollama is reachable.\n",
    "- If you use HTTP, use a short timeout (fast failure).\n",
    "- If you use CLI, check return codes.\n",
    "\n",
    "When done, you should be able to run:\n",
    "\n",
    "- `check_ollama_status()` \u2192 `True` when Ollama is running, otherwise `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2372d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_status() -> bool:\n",
    "    # TODO: implement a quick local health check.\n",
    "    # Option 1: run `ollama list` and check return code.\n",
    "    # Option 2: attempt a small HTTP request to localhost:11434.\n",
    "    #\n",
    "    # Placeholder behavior (so the notebook can run end-to-end):\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"Implement check_ollama_status().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1a6b6",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for the TODO exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0978803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_for_hardware(vram_gb: float) -> str:\n",
    "    if vram_gb < 8:\n",
    "        return \"Prefer a 3B (or smaller) model / stronger quantization (e.g., 4-bit)\"\n",
    "    if vram_gb < 16:\n",
    "        return \"Prefer a 7B model (quantized if needed)\"\n",
    "    if vram_gb < 24:\n",
    "        return \"Try a 13B model (quantized if needed)\"\n",
    "    return \"Try 13B+ (and compare speed/quality); consider context length and latency\"\n",
    "\n",
    "\n",
    "def check_ollama_status() -> bool:\n",
    "    # Fast path: HTTP tags endpoint\n",
    "    try:\n",
    "        _ = fetch_ollama_tags(timeout_s=1.5)\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: CLI `ollama list`\n",
    "    try:\n",
    "        out = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, check=False)\n",
    "        return out.returncode == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(choose_model_for_hardware(6))\n",
    "print(choose_model_for_hardware(12))\n",
    "print(choose_model_for_hardware(20))\n",
    "print(\"ollama_ok=\", check_ollama_status())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}