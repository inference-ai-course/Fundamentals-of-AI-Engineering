{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Part 01: Local inference concepts + setup checklist\n",
    "\n",
    "**Estimated time:** 45–75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Define inference and local inference\n",
    "- Explain how moving from hosted APIs to local inference changes constraints\n",
    "- Understand how model size, context length, and quantization affect latency and memory\n",
    "- Follow a practical setup checklist for Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357111d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Inference** = using a trained model to generate outputs.\n",
    "\n",
    "**Local inference** = you run the model on your own machine.\n",
    "\n",
    "Local inference is useful for:\n",
    "\n",
    "- privacy (data stays local)\n",
    "- cost control (no per-request billing)\n",
    "- offline capability\n",
    "\n",
    "Trade-offs:\n",
    "\n",
    "- quality may be lower than top hosted models\n",
    "- performance depends on your CPU/GPU/RAM/VRAM\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 01)\n",
    "\n",
    "- You can run:\n",
    "  - `ollama --version`\n",
    "  - `ollama list`\n",
    "- You can confirm the local server is reachable either by:\n",
    "  - CLI (`ollama list` succeeds), or\n",
    "  - HTTP (`GET http://localhost:11434/api/tags` succeeds)\n",
    "\n",
    "If you cannot complete the checks above, fix runtime/environment issues before writing client/benchmark code.\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: moving the boundary changes your constraints\n",
    "\n",
    "When you use a hosted API, the provider owns the compute and you mostly worry about:\n",
    "\n",
    "- request formatting\n",
    "- rate limits\n",
    "- latency and cost\n",
    "\n",
    "When you run locally, you become the provider. That means **hardware is now part of your system design**.\n",
    "\n",
    "You can think of local inference performance as a function:\n",
    "\n",
    "$$\n",
    "\\text{latency} = f(\\text{model size},\\ \\text{context length},\\ \\text{hardware},\\ \\text{quantization})\n",
    "$$\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- if a model does not fit in RAM/VRAM, it won’t run (or will thrash)\n",
    "- even if it fits, throughput/latency can vary dramatically across machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b937e",
   "metadata": {},
   "source": [
    "## Setup checklist (practical)\n",
    "\n",
    "1. Install Ollama\n",
    "2. Start the Ollama service\n",
    "3. Pull a model\n",
    "4. Run a test prompt\n",
    "\n",
    "What to do and what “success” looks like:\n",
    "\n",
    "1. **Install Ollama**\n",
    "    - Goal: have the `ollama` CLI available.\n",
    "    - Verify: `ollama --version` prints a version.\n",
    "\n",
    "2. **Start the Ollama service**\n",
    "    - Goal: local server process ready to accept requests.\n",
    "    - Verify: `ollama serve` starts without immediately exiting.\n",
    "    - Common failure: port conflicts or permission issues.\n",
    "\n",
    "3. **Pull a model**\n",
    "    - Goal: download at least one model.\n",
    "    - Verify: `ollama list` shows the model.\n",
    "    - Practical note: start small to avoid memory failures.\n",
    "\n",
    "4. **Run a test prompt**\n",
    "    - Goal: confirm request → generation works locally.\n",
    "    - Verify: `ollama run <model_name>` produces output and doesn’t crash.\n",
    "    - Note: first run can be slow due to model loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632a9d7",
   "metadata": {},
   "source": [
    "import platform\n",
    "import shutil\n",
    "import subprocess\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def try_run(cmd: List[str]) -> None:\n",
    "    print(\"$\", \" \".join(cmd))\n",
    "    try:\n",
    "        out = subprocess.run(cmd, capture_output=True, text=True, check=False)\n",
    "        print(\"returncode=\", out.returncode)\n",
    "        if out.stdout:\n",
    "            print(out.stdout.strip())\n",
    "        if out.stderr:\n",
    "            print(out.stderr.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(\"command not found\")\n",
    "\n",
    "\n",
    "print(\"python=\", platform.python_version())\n",
    "print(\"platform=\", platform.platform())\n",
    "print(\"ollama_in_path=\", shutil.which(\"ollama\") is not None)\n",
    "\n",
    "try_run([\"ollama\", \"--version\"])\n",
    "try_run([\"ollama\", \"list\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0956d",
   "metadata": {},
   "source": [
    "## Guided check (recommended): verify the Ollama HTTP endpoint\n",
    "\n",
    "Even though Ollama runs on your machine, it exposes a local HTTP API (default `http://localhost:11434`).\n",
    "\n",
    "This is useful because:\n",
    "\n",
    "- it confirms the server is actually running\n",
    "- it matches how your later Python client and benchmark will call Ollama\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "- If the server is running, you should see a JSON response containing a `models` list.\n",
    "- If it is not running, you will typically see a connection/refused error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception as e:  # pragma: no cover\n",
    "    requests = None\n",
    "    _requests_import_error = e\n",
    "\n",
    "\n",
    "def fetch_ollama_tags(host: str = \"http://localhost:11434\", *, timeout_s: float = 2.0) -> dict:\n",
    "    if requests is None:\n",
    "        raise RuntimeError(f\"requests is required for HTTP health checks: {_requests_import_error}\")\n",
    "    url = f\"{host}/api/tags\"\n",
    "    resp = requests.get(url, timeout=timeout_s)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "try:\n",
    "    tags = fetch_ollama_tags()\n",
    "    model_names = [m.get(\"name\") for m in tags.get(\"models\", [])]\n",
    "    print(\"ollama_http_ok=True\")\n",
    "    print(\"models=\", model_names)\n",
    "except Exception as e:\n",
    "    print(\"ollama_http_ok=False\")\n",
    "    print(\"error=\", type(e).__name__, str(e))\n",
    "    print(\"Next steps:\")\n",
    "    print(\"- Ensure Ollama is installed: ollama --version\")\n",
    "    print(\"- Ensure the server is running: ollama serve\")\n",
    "    print(\"- Ensure at least one model is pulled: ollama pull <model>\")\n",
    "    print(\"- Then re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06690bb",
   "metadata": {},
   "source": [
    "## What “model size / context window / quantization” mean\n",
    "\n",
    "- **Size (e.g. 7B, 13B)**: larger often means better quality but slower and more memory.\n",
    "- **Context window**: how much text you can include per request.\n",
    "- **Quantization**: smaller memory footprint (quality may change slightly).\n",
    "\n",
    "More concrete intuition:\n",
    "\n",
    "- model size is roughly the number of parameters\n",
    "- more parameters usually means more compute per generated token\n",
    "- quantization stores weights with fewer bits, reducing memory and often increasing speed on constrained hardware\n",
    "\n",
    "Practical rule of thumb: local inference is often bottlenecked by memory bandwidth and/or VRAM capacity, not just CPU speed.\n",
    "\n",
    "For Foundamental Course, focus on the practical effect:\n",
    "\n",
    "- if it doesn’t fit, you can’t run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_gb(params_billion: float, bits_per_weight: int) -> float:\n",
    "    # Rough estimate: params * bits per weight -> bytes -> GB\n",
    "    params = params_billion * 1_000_000_000\n",
    "    bytes_used = params * (bits_per_weight / 8)\n",
    "    return bytes_used / (1024 ** 3)\n",
    "\n",
    "\n",
    "sizes = [7, 13, 70]\n",
    "for s in sizes:\n",
    "    for bits in [4, 8, 16]:\n",
    "        print(f\"{s}B @ {bits}-bit: {estimate_memory_gb(s, bits):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_for_hardware(vram_gb: float) -> str:\n",
    "    # TODO: implement a rule-of-thumb mapping.\n",
    "    # Example:\n",
    "    # - vram < 8 -> prefer 3B or smaller\n",
    "    # - vram < 16 -> prefer 7B\n",
    "    # - otherwise -> 13B+ (if latency acceptable)\n",
    "    #\n",
    "    # Placeholder behavior (so the notebook can run end-to-end):\n",
    "    return \"TODO: implement choose_model_for_hardware(vram_gb)\"\n",
    "\n",
    "\n",
    "print(\"Implement choose_model_for_hardware().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d0df2",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Ollama: https://ollama.com/\n",
    "- Ollama GitHub: https://github.com/ollama/ollama\n",
    "- Hugging Face model cards: https://huggingface.co/docs/hub/model-cards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831ab86",
   "metadata": {},
   "source": [
    "## Exercise: implement a health check function\n",
    "\n",
    "In later notebooks you’ll call Ollama via HTTP. Before you do that, you need a quick local health check.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- Return `True` only when Ollama is reachable.\n",
    "- If you use HTTP, use a short timeout (fast failure).\n",
    "- If you use CLI, check return codes.\n",
    "\n",
    "When done, you should be able to run:\n",
    "\n",
    "- `check_ollama_status()` → `True` when Ollama is running, otherwise `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2372d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_status() -> bool:\n",
    "    # TODO: implement a quick local health check.\n",
    "    # Option 1: run `ollama list` and check return code.\n",
    "    # Option 2: attempt a small HTTP request to localhost:11434.\n",
    "    #\n",
    "    # Placeholder behavior (so the notebook can run end-to-end):\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"Implement check_ollama_status().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1a6b6",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for the TODO exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0978803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_for_hardware(vram_gb: float) -> str:\n",
    "    if vram_gb < 8:\n",
    "        return \"Prefer a 3B (or smaller) model / stronger quantization (e.g., 4-bit)\"\n",
    "    if vram_gb < 16:\n",
    "        return \"Prefer a 7B model (quantized if needed)\"\n",
    "    if vram_gb < 24:\n",
    "        return \"Try a 13B model (quantized if needed)\"\n",
    "    return \"Try 13B+ (and compare speed/quality); consider context length and latency\"\n",
    "\n",
    "\n",
    "def check_ollama_status() -> bool:\n",
    "    # Fast path: HTTP tags endpoint\n",
    "    try:\n",
    "        _ = fetch_ollama_tags(timeout_s=1.5)\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: CLI `ollama list`\n",
    "    try:\n",
    "        out = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, check=False)\n",
    "        return out.returncode == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "print(choose_model_for_hardware(6))\n",
    "print(choose_model_for_hardware(12))\n",
    "print(choose_model_for_hardware(20))\n",
    "print(\"ollama_ok=\", check_ollama_status())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
