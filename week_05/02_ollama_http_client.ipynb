{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Part 02: Calling Ollama via HTTP (minimal client)\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Treat local inference as a service call with the same reliability concerns\n",
    "- Implement a minimal HTTP client for Ollama `/api/generate`\n",
    "- Apply timeouts and basic parsing/validation\n",
    "- Save outputs + latency as artifacts for later comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77dd89",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook builds a **minimal HTTP client for Ollama** — treating local inference as a service call with the same reliability concerns as a cloud API.\n",
    "\n",
    "Even though Ollama runs on your machine, your Python script is still making a network-style call:\n",
    "- your client process sends a request to `localhost:11434`\n",
    "- the Ollama server process does the work\n",
    "- you receive a response (or a failure)\n",
    "\n",
    "So the same engineering principles apply: **always set timeouts, log failures, validate responses.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e5088",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Ollama exposes a local HTTP API. This lets you treat local inference like a normal service call.\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 02)\n",
    "\n",
    "- You can make one request to `POST http://localhost:11434/api/generate` and receive a response.\n",
    "- You record latency in seconds.\n",
    "- You can save a JSON artifact to disk under `output/ollama_runs/`.\n",
    "\n",
    "If you cannot reach the endpoint, confirm Ollama is running before debugging your client code.\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: local inference is still a distributed system (just smaller)\n",
    "\n",
    "Even though the model is on your machine, your Python script is still making a network-style call:\n",
    "\n",
    "- your client process sends a request\n",
    "- the Ollama server process does work\n",
    "- you receive a response (or a failure)\n",
    "\n",
    "So the same engineering principles apply:\n",
    "\n",
    "- always set timeouts\n",
    "- log failures with enough context to debug\n",
    "- treat response parsing as untrusted input (validate what you need)\n",
    "\n",
    "We’ll implement a minimal client that:\n",
    "\n",
    "- sends a prompt\n",
    "- receives text output\n",
    "- prints it\n",
    "- records latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3e84d",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `OllamaGenerateRequest` (typed parameters for one call) and `call_ollama()` — the core HTTP client function.\n",
    "\n",
    "**Walk through `call_ollama()` step by step:**\n",
    "1. Build the URL: `{host}/api/generate`\n",
    "2. Build the payload: `model`, `prompt`, `stream=False` (wait for full response), `temperature=0.0` (deterministic)\n",
    "3. Record `t0 = time.time()` before the call\n",
    "4. `requests.post(..., timeout=req.timeout_s)` — the timeout is a real network-level timeout\n",
    "5. `resp.raise_for_status()` — raises an exception for any 4xx/5xx response\n",
    "6. Parse the response JSON and extract `response` text\n",
    "7. Return a dict with model, response text, and latency\n",
    "\n",
    "**`stream=False` vs `stream=True`:** With `stream=False`, Ollama waits until the full response is generated before sending it back. With `stream=True`, you get tokens as they're generated (better for interactive UX, more complex to parse). Use `stream=False` for batch/pipeline work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884aa85",
   "metadata": {},
   "source": [
    "## Minimal client (Python)\n",
    "\n",
    "Dependency:\n",
    "\n",
    "- `requests` (commonly used for HTTP)\n",
    "\n",
    "This notebook calls Ollama’s local endpoint:\n",
    "\n",
    "- `POST http://localhost:11434/api/generate`\n",
    "\n",
    "Two practical notes:\n",
    "\n",
    "- The timeout is a policy choice. Slower hardware or larger models may need longer.\n",
    "- When you later build a benchmark, consider warmup: the first call can be slower due to model loading.\n",
    "\n",
    "---\n",
    "\n",
    "## Guided walkthrough: one request end-to-end\n",
    "\n",
    "Goal: prove the contract **request → response → latency → save artifact**.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "- If Ollama is running and you have at least one model pulled, you should see:\n",
    "  - `ok=True`\n",
    "  - a non-empty `response_preview`\n",
    "  - an artifact path under `output/ollama_runs/`\n",
    "\n",
    "If not, you should see a clear error and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c7a35",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Makes one real request to Ollama and saves the result as a JSON artifact under `output/ollama_runs/`.\n",
    "\n",
    "**What to check before running:**\n",
    "- `DEFAULT_MODEL` must match a model name shown by `ollama list`\n",
    "- Ollama must be running (`ollama serve`)\n",
    "\n",
    "**What to check after running:**\n",
    "- `ok=True` and a non-empty `response_preview`\n",
    "- A JSON file created under `output/ollama_runs/`\n",
    "- `latency_s` — note the first call is slower (model loading). Subsequent calls to the same model are faster.\n",
    "\n",
    "**If it fails:** The error message tells you exactly what to fix (Ollama not running, wrong model name, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception as e:  # pragma: no cover\n",
    "    requests = None\n",
    "    _requests_import_error = e\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OllamaGenerateRequest:\n",
    "    model: str\n",
    "    prompt: str\n",
    "    host: str = \"http://localhost:11434\"\n",
    "    timeout_s: float = 60.0\n",
    "\n",
    "\n",
    "def call_ollama(req: OllamaGenerateRequest) -> dict:\n",
    "    if requests is None:\n",
    "        raise RuntimeError(f\"requests is required: {_requests_import_error}\")\n",
    "\n",
    "    url = f\"{req.host}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": req.model,\n",
    "        \"prompt\": req.prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.0},\n",
    "    }\n",
    "\n",
    "    t0 = time.time()\n",
    "    resp = requests.post(url, json=payload, timeout=req.timeout_s, headers={\"X-Client\": \"level-1\"})\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    response_text = data.get(\"response\", \"\")\n",
    "    return {\n",
    "        \"model\": req.model,\n",
    "        \"response\": response_text,\n",
    "        \"latency_s\": time.time() - t0,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"call_ollama() ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fee4dd",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `call_ollama_safe()` — a wrapper that **never raises**. Instead of propagating exceptions, it catches them and returns a structured dict with an `ok` flag and `error` string.\n",
    "\n",
    "**Why a \"safe\" wrapper?** When benchmarking multiple models, you want to collect all results — including failures — without the loop crashing on the first error. A safe wrapper lets you store failures as data: `{\"ok\": false, \"error\": \"ConnectionError: ...\"}` is a valid benchmark result.\n",
    "\n",
    "**Your task:** Implement `call_ollama_safe()` using `call_ollama()`. Requirements:\n",
    "- Always return a dict with keys: `ok`, `model`, `latency_s`, `response`, `error`\n",
    "- On success: `ok=True`, `error=None`\n",
    "- On failure: `ok=False`, `error=\"{ExceptionType}: {message}\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9eb58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_HOST = \"http://localhost:11434\"\n",
    "DEFAULT_MODEL = \"llama3.1\"  # TODO: change to a model you have pulled (see `ollama list`)\n",
    "DEFAULT_PROMPT = \"Say hello in one sentence.\"\n",
    "\n",
    "req = OllamaGenerateRequest(model=DEFAULT_MODEL, prompt=DEFAULT_PROMPT, host=DEFAULT_HOST, timeout_s=60.0)\n",
    "\n",
    "try:\n",
    "    out = call_ollama(req)\n",
    "    ok = True\n",
    "    err = None\n",
    "except Exception as e:\n",
    "    out = {\"model\": req.model, \"response\": \"\", \"latency_s\": None}\n",
    "    ok = False\n",
    "    err = f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "print(\"ok=\", ok)\n",
    "if err:\n",
    "    print(\"error=\", err)\n",
    "    print(\"Next steps:\")\n",
    "    print(\"- Ensure Ollama is running: ollama serve\")\n",
    "    print(\"- Ensure you pulled a model: ollama pull <model>\")\n",
    "    print(\"- Set DEFAULT_MODEL to a model shown by: ollama list\")\n",
    "else:\n",
    "    print(\"latency_s=\", out.get(\"latency_s\"))\n",
    "    print(\"response_preview=\", (out.get(\"response\") or \"\")[:200])\n",
    "    saved = save_result(out, out_dir=Path(\"output/ollama_runs\"))\n",
    "    print(\"saved_to=\", saved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34434c76",
   "metadata": {},
   "source": [
    "## Exercise: make the client more robust\n",
    "\n",
    "Implement the TODO below.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Return a dict that *always* has keys: `ok`, `model`, `latency_s`, `response`, `error`.\n",
    "- Never raise in this helper (errors should be captured as strings).\n",
    "\n",
    "This pattern makes benchmarking easier because you can store failures as data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def call_ollama_safe(req: OllamaGenerateRequest) -> Dict[str, Any]:\n",
    "    # TODO: implement using call_ollama(req).\n",
    "    # Requirements:\n",
    "    # - include ok flag\n",
    "    # - include error string on failure\n",
    "    # - include latency_s when available\n",
    "    return {\n",
    "        \"ok\": False,\n",
    "        \"model\": req.model,\n",
    "        \"latency_s\": None,\n",
    "        \"response\": \"\",\n",
    "        \"error\": \"TODO: implement call_ollama_safe(req)\",\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Implement call_ollama_safe().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bad034",
   "metadata": {},
   "source": [
    "## Next: saving artifacts\n",
    "\n",
    "After you can make one successful request, the next step is to save results to disk so you can:\n",
    "\n",
    "- inspect outputs later for quality\n",
    "- compare runs across models\n",
    "- share evidence with others\n",
    "\n",
    "The `save_result()` helper below writes one JSON file per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(out: dict, *, out_dir: Path) -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"ollama_{int(time.time())}.json\"\n",
    "    out_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# Example usage (requires Ollama running locally):\n",
    "# req = OllamaGenerateRequest(model=\"llama3.1\", prompt=\"Say hello in one sentence\")\n",
    "# out = call_ollama(req)\n",
    "# print(json.dumps(out, indent=2))\n",
    "# print(\"saved to\", save_result(out, out_dir=Path(\"output/ollama_runs\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c452d",
   "metadata": {},
   "source": [
    "## How to run (CLI)\n",
    "\n",
    "```bash\n",
    "python call_ollama.py --model llama3.1 --prompt \"Say hello in one sentence\"\n",
    "```\n",
    "\n",
    "If this works, you've proven local inference end-to-end.\n",
    "\n",
    "---\n",
    "\n",
    "## Common pitfalls\n",
    "\n",
    "- Ollama service not running\n",
    "- wrong model name\n",
    "- timeouts for slow hardware\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Ollama docs/issues: https://github.com/ollama/ollama\n",
    "- Requests timeouts: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b11a665",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `call_ollama_safe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee12822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def call_ollama_safe(req: OllamaGenerateRequest) -> Dict[str, Any]:\n",
    "    try:\n",
    "        out = call_ollama(req)\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"model\": out.get(\"model\", req.model),\n",
    "            \"latency_s\": out.get(\"latency_s\"),\n",
    "            \"response\": out.get(\"response\", \"\"),\n",
    "            \"error\": None,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"ok\": False,\n",
    "            \"model\": req.model,\n",
    "            \"latency_s\": None,\n",
    "            \"response\": \"\",\n",
    "            \"error\": \"%s: %s\" % (type(e).__name__, e),\n",
    "        }\n",
    "\n",
    "\n",
    "print(call_ollama_safe(req))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
