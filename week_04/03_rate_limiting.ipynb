{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Part 03: Rate Limiting Lab\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on production constraints:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Chapter 5: Resource Monitoring and Containerization](../self_learn/Chapters/5/Chapter5.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You can handle HTTP 429 (rate limit) errors gracefully.\n",
    "- You can implement rate limiting in your client.\n",
    "- You can back off when rate limits are hit.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook:\n",
    "- You can detect rate limit errors\n",
    "- You can implement backoff strategies\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Handle rate limit errors (HTTP 429)\n",
    "- Implement rate limiting strategies\n",
    "- Back off gracefully when limits are hit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd5254",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook covers **rate limiting** — what happens when you send requests too fast, and how to handle it gracefully.\n",
    "\n",
    "**HTTP 429 \"Too Many Requests\"** is the server's way of saying \"slow down.\" Every LLM provider has rate limits:\n",
    "- **RPM** (requests per minute) — how many calls you can make\n",
    "- **TPM** (tokens per minute) — how many tokens you can send/receive\n",
    "- **Concurrent** — how many simultaneous requests are allowed\n",
    "\n",
    "**Two responses to a 429:**\n",
    "1. **Wait and retry** — if the `Retry-After` header tells you how long to wait, respect it\n",
    "2. **Degrade** — serve a cheaper/faster result rather than making the user wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5917ef",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Rate limits protect providers and enforce fair usage.\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- interpret HTTP 429 and `Retry-After`\n",
    "- implement a simple local token bucket limiter\n",
    "- practice deterministic graceful degradation (shrink prompt / fallback model)\n",
    "\n",
    "If you want the deeper rate-limiter theory, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9401d81d",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements `parse_retry_after_seconds()` — parses the `Retry-After` HTTP header into a float number of seconds.\n",
    "\n",
    "**Why parse this header?** When a server returns 429, it often includes `Retry-After: 5` meaning \"wait 5 seconds before retrying.\" Respecting this is both polite and practical — it avoids wasting retries before the limit resets.\n",
    "\n",
    "**What to notice:** The function handles the integer-seconds format (most common for LLM APIs). The HTTP spec also allows an HTTP-date format (`Wed, 21 Oct 2015 07:28:00 GMT`) — the test at the bottom shows this returns `None` since we don't support it here. In production, you'd want to handle both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882e6b1",
   "metadata": {},
   "source": [
    "## HTTP 429\n",
    "\n",
    "429 means “Too Many Requests”.\n",
    "\n",
    "Client behavior:\n",
    "\n",
    "- respect the `Retry-After` header if present\n",
    "- otherwise backoff and retry\n",
    "\n",
    "Graceful degradation options (choose based on your product):\n",
    "\n",
    "- return a clear “busy, try later” message\n",
    "- fall back to a cheaper/faster model\n",
    "- reduce prompt size / requested output length\n",
    "- serve a cached result if correctness allows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009f0be",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements a **Token Bucket** rate limiter — a classic algorithm for client-side rate limiting.\n",
    "\n",
    "**How it works:**\n",
    "- The bucket holds up to `capacity` tokens\n",
    "- Tokens refill at `refill_per_s` tokens per second\n",
    "- Each request costs `cost` tokens (default 1.0)\n",
    "- If the bucket has enough tokens → allow the request and deduct\n",
    "- If not → deny the request (caller must wait or degrade)\n",
    "\n",
    "**Why use a token bucket?** It allows short bursts (up to `capacity` requests at once) while enforcing a long-term average rate. This matches how most LLM provider limits work — you can burst briefly but must stay within the per-minute average.\n",
    "\n",
    "**What to notice in the output:** The first 5 requests are allowed immediately (bucket starts full at capacity=5). Then requests are denied until tokens refill at 1/second. After 0.2s × 5 = 1s, one token has refilled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bac34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def parse_retry_after_seconds(value: str) -> Optional[float]:\n",
    "    \"\"\"Parse Retry-After header.\n",
    "\n",
    "    For Foundations Course, we support the most common form: integer seconds.\n",
    "    (HTTP also allows an HTTP-date; production clients should support both.)\n",
    "    \"\"\"\n",
    "    v = value.strip()\n",
    "    if not v:\n",
    "        return None\n",
    "    try:\n",
    "        seconds = int(v)\n",
    "        return max(0.0, float(seconds))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(parse_retry_after_seconds(\"2\"))\n",
    "print(parse_retry_after_seconds(\"0\"))\n",
    "print(parse_retry_after_seconds(\"\"))\n",
    "print(parse_retry_after_seconds(\"Wed, 21 Oct 2015 07:28:00 GMT\"))  # unsupported here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82d578",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements `decide_on_429()` — the decision logic for what to do when you receive a 429 response.\n",
    "\n",
    "**Decision tree:**\n",
    "1. Does the response have a `Retry-After` header?\n",
    "2. Is the wait time ≤ `max_wait_s`? → **wait** that long, then retry\n",
    "3. Otherwise → **degrade** (return a cheaper/faster result now rather than making the user wait)\n",
    "\n",
    "**What to notice in the output:**\n",
    "- `Retry-After: 2` with `max_wait_s=5` → `(\"wait\", 2.0)` — 2 seconds is acceptable\n",
    "- `Retry-After: 60` with `max_wait_s=5` → `(\"degrade\", None)` — 60 seconds is too long to wait\n",
    "- No `Retry-After` → `(\"degrade\", None)` — unknown wait time, degrade immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenBucket:\n",
    "    capacity: float\n",
    "    refill_per_s: float\n",
    "    tokens: float\n",
    "    last_refill_s: float\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, capacity: float, refill_per_s: float) -> \"TokenBucket\":\n",
    "        now = time.time()\n",
    "        return cls(capacity=capacity, refill_per_s=refill_per_s, tokens=capacity, last_refill_s=now)\n",
    "\n",
    "    def _refill(self) -> None:\n",
    "        now = time.time()\n",
    "        dt = max(0.0, now - self.last_refill_s)\n",
    "        self.tokens = min(self.capacity, self.tokens + dt * self.refill_per_s)\n",
    "        self.last_refill_s = now\n",
    "\n",
    "    def allow(self, cost: float = 1.0) -> bool:\n",
    "        self._refill()\n",
    "        if self.tokens >= cost:\n",
    "            self.tokens -= cost\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "bucket = TokenBucket.create(capacity=5, refill_per_s=1.0)\n",
    "\n",
    "for i in range(12):\n",
    "    ok = bucket.allow(cost=1.0)\n",
    "    print(f\"i={i:02d} allowed={ok} tokens_left={bucket.tokens:.2f}\")\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc18f1",
   "metadata": {},
   "source": [
    "## Graceful degradation (what to do when limited)\n",
    "\n",
    "When you hit rate limits, you typically have two broad options:\n",
    "\n",
    "- **Wait and retry** (respecting `Retry-After` if provided)\n",
    "- **Degrade** (return something cheaper/faster or less precise)\n",
    "\n",
    "Common degradation choices:\n",
    "\n",
    "- Return a clear “busy, try later” message\n",
    "- Fall back to a cheaper/faster model\n",
    "- Reduce prompt size (trim history, remove low-value context)\n",
    "- Reduce requested output length\n",
    "- Serve a cached result (only if acceptable for correctness)\n",
    "\n",
    "The correct choice depends on your product:\n",
    "\n",
    "- For interactive UX, a fast partial answer may be better than waiting.\n",
    "- For offline pipelines, retrying may be preferable to degrading quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RateLimitResponse:\n",
    "    status_code: int\n",
    "    retry_after: Optional[str] = None\n",
    "\n",
    "\n",
    "def decide_on_429(\n",
    "    resp: RateLimitResponse,\n",
    "    *,\n",
    "    max_wait_s: float,\n",
    ") -> Tuple[str, Optional[float]]:\n",
    "    \"\"\"Return (action, wait_seconds).\n",
    "\n",
    "    action is one of: \"wait\", \"degrade\".\n",
    "    \"\"\"\n",
    "    if resp.retry_after is not None:\n",
    "        s = parse_retry_after_seconds(resp.retry_after)\n",
    "        if s is not None and s <= max_wait_s:\n",
    "            return (\"wait\", s)\n",
    "    return (\"degrade\", None)\n",
    "\n",
    "\n",
    "print(decide_on_429(RateLimitResponse(429, retry_after=\"2\"), max_wait_s=5.0))\n",
    "print(decide_on_429(RateLimitResponse(429, retry_after=\"60\"), max_wait_s=5.0))\n",
    "print(decide_on_429(RateLimitResponse(429, retry_after=None), max_wait_s=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_request(prompt: str, *, max_chars: int) -> str:\n",
    "    # TODO: implement a deterministic prompt shrinking strategy.\n",
    "    # Requirements:\n",
    "    # - Never exceed max_chars.\n",
    "    # - Preserve the beginning of the prompt (often contains instructions).\n",
    "    # - Consider preserving the end too (often contains the latest user input).\n",
    "    if max_chars <= 0:\n",
    "        return \"\"\n",
    "    if len(prompt) <= max_chars:\n",
    "        return prompt\n",
    "\n",
    "    head = max(0, max_chars // 2)\n",
    "    tail = max_chars - head\n",
    "    if tail <= 0:\n",
    "        return prompt[:max_chars]\n",
    "    return prompt[:head] + prompt[-tail:]\n",
    "\n",
    "\n",
    "def choose_fallback_model(primary: str) -> str:\n",
    "    # TODO: implement a simple fallback mapping.\n",
    "    # Example behavior:\n",
    "    # - if primary is \"gpt-4\", fallback to \"gpt-4-mini\"\n",
    "    # - otherwise fallback to a safe default\n",
    "    mapping = {\n",
    "        \"gpt-4\": \"gpt-4-mini\",\n",
    "        \"gpt-4o\": \"gpt-4o-mini\",\n",
    "    }\n",
    "    return mapping.get(primary, \"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "print(\"degraded:\", degrade_request(\"INSTRUCTIONS...\" + (\"x\" * 200) + \"...LATEST\", max_chars=60))\n",
    "print(\"fallback:\", choose_fallback_model(\"gpt-4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466a40b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- HTTP 429: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2954",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for `degrade_request` and `choose_fallback_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_request(prompt: str, *, max_chars: int) -> str:\n",
    "    if max_chars <= 0:\n",
    "        return \"\"\n",
    "    if len(prompt) <= max_chars:\n",
    "        return prompt\n",
    "\n",
    "    if max_chars <= 10:\n",
    "        return prompt[:max_chars]\n",
    "\n",
    "    keep_head = max(1, int(max_chars * 0.6))\n",
    "    keep_tail = max_chars - keep_head\n",
    "    return prompt[:keep_head] + prompt[-keep_tail:]\n",
    "\n",
    "\n",
    "def choose_fallback_model(primary: str) -> str:\n",
    "    mapping = {\n",
    "        \"gpt-4\": \"gpt-4-mini\",\n",
    "        \"gpt-4o\": \"gpt-4o-mini\",\n",
    "        \"claude-3-opus\": \"claude-3-sonnet\",\n",
    "    }\n",
    "    return mapping.get(primary, \"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "print(\"degraded:\", degrade_request(\"INSTRUCTIONS...\" + (\"x\" * 200) + \"...LATEST\", max_chars=60))\n",
    "print(\"fallback:\", choose_fallback_model(\"gpt-4\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
