{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Part 04: Caching and Logging Lab\n",
    "\n",
    "**Estimated time:** 90–120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on observability and operations:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Chapter 5: Resource Monitoring and Containerization](../self_learn/Chapters/5/Chapter5.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 04)\n",
    "\n",
    "- You can implement response caching to reduce costs.\n",
    "- You can add structured logging for observability.\n",
    "- You can trace requests through your system.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook:\n",
    "- You can cache LLM responses\n",
    "- You can log requests and responses\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement response caching\n",
    "- Add structured logging\n",
    "- Trace requests for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354d5b5",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook covers two practical tools for production LLM clients:\n",
    "\n",
    "- **Caching** — avoid making the same expensive API call twice. A cache hit saves money and reduces latency.\n",
    "- **Logging** — record what happened, when, and how long it took. Good logs turn debugging from guesswork into inspection.\n",
    "\n",
    "**The critical rule for caching:** The cache key must include **every parameter that can change the output**. If you forget to include `temperature` in the key, you might return a cached deterministic response for a request that should be stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061e502",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Two practical realities of LLM APIs:\n",
    "\n",
    "- calls can be expensive\n",
    "- failures are hard to debug without logs\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- build a safe cache key (include every field that can change output)\n",
    "- demonstrate cache hit vs miss\n",
    "- emit a minimal request log line (request_id, latency, success/failure)\n",
    "\n",
    "If you want the deeper caching theory, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da4dc4",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `LLMRequest` — a frozen dataclass representing one LLM call — and `make_cache_key()` which converts it to a stable SHA-256 hash.\n",
    "\n",
    "**Why a frozen dataclass?** Frozen means immutable — you can't accidentally modify a request after creating it. This is important for caching: the key must represent the request at the moment it was made, not a later mutated version.\n",
    "\n",
    "**Why SHA-256 of sorted JSON?** `json.dumps(..., sort_keys=True)` produces deterministic output regardless of dict insertion order. SHA-256 compresses it to a fixed-length string suitable as a dict key or filename.\n",
    "\n",
    "**What to notice:** Change any field in `LLMRequest` (even `temperature` from `0.0` to `0.1`) and the cache key changes completely. This is correct behavior — different temperature means different expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e70112",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "Cache when:\n",
    "\n",
    "- the same request repeats\n",
    "- you are iterating on downstream code\n",
    "\n",
    "Cache key must include everything that changes output:\n",
    "\n",
    "- model name\n",
    "- system prompt\n",
    "- user prompt\n",
    "- temperature\n",
    "\n",
    "Common cache pitfalls:\n",
    "\n",
    "- forgetting system prompt / tool context in the key\n",
    "- caching when temperature is high (outputs are intentionally stochastic)\n",
    "- caching errors (you accidentally “remember” a failure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86183a46",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements `SimpleMemoryCache` — an in-memory dict-based cache — and demonstrates a cache miss followed by a cache hit.\n",
    "\n",
    "**When to use memory cache vs file cache:**\n",
    "- **Memory cache** — fast, zero I/O, but lost when the process exits. Good for within-session deduplication (e.g., same prompt called multiple times in one pipeline run).\n",
    "- **File cache** (`SimpleFileCache` below) — persists across runs. Good for development iteration where you want to avoid re-calling the LLM every time you restart the notebook.\n",
    "\n",
    "**Common pitfall:** Never cache errors. If an API call fails and you cache the failure, future calls will return the error immediately without retrying — even after the underlying issue is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LLMRequest:\n",
    "    model: str\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "    temperature: float = 0.0\n",
    "\n",
    "\n",
    "def make_cache_key(req: LLMRequest) -> str:\n",
    "    # Key must include every field that can change the output.\n",
    "    raw = json.dumps(asdict(req), sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "req = LLMRequest(model=\"demo\", system_prompt=\"You are helpful.\", user_prompt=\"Hello\", temperature=0.0)\n",
    "print(\"cache_key=\", make_cache_key(req)[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca45b9",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements `SimpleFileCache` — a JSON-file-backed cache that persists across notebook restarts.\n",
    "\n",
    "**How it works:** Reads the entire JSON file on every `get()` and `set()`. This is intentionally simple — not suitable for high-throughput production use, but perfect for development iteration where you want to avoid re-calling the LLM every time you restart.\n",
    "\n",
    "**What to check:** After running, open `output/cache/llm_cache.json` — you should see the cached entry. Restart the kernel and run only the `get()` call — it should still return the cached value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class SimpleMemoryCache:\n",
    "    def __init__(self) -> None:\n",
    "        self._store = {}\n",
    "\n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        return self._store.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._store[key] = value\n",
    "\n",
    "\n",
    "cache = SimpleMemoryCache()\n",
    "key = \"k1\"\n",
    "print(cache.get(key))\n",
    "cache.set(key, \"value\")\n",
    "print(cache.get(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330b3ea",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements `logged_call()` — wraps an LLM call with structured logging that records `request_id`, `model`, `latency_s`, `prompt_len`, and `attempt`.\n",
    "\n",
    "**Minimum viable log fields:**\n",
    "- `request_id` — unique ID per call, so you can trace a specific request through logs\n",
    "- `model` — which model was called (important when you have fallbacks)\n",
    "- `latency_s` — how long the call took (spot slow calls immediately)\n",
    "- `success/failure` — error rate tracking\n",
    "- `error_type` — distinguish network errors from parsing errors from validation errors\n",
    "\n",
    "**Why structured logging?** Plain `print()` statements are hard to filter and parse. Structured logs (key=value pairs or JSON) can be queried: \"show me all calls where `latency_s > 10`\" or \"show me all `SCHEMA_ERROR` failures in the last hour.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "class SimpleFileCache:\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        self.path = path\n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.path.exists():\n",
    "            self.path.write_text(\"{}\", encoding=\"utf-8\")\n",
    "\n",
    "    def _read(self) -> Dict[str, str]:\n",
    "        return json.loads(self.path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    def _write(self, data: Dict[str, str]) -> None:\n",
    "        self.path.write_text(json.dumps(data, ensure_ascii=False, sort_keys=True, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        data = self._read()\n",
    "        return data.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        data = self._read()\n",
    "        data[key] = value\n",
    "        self._write(data)\n",
    "\n",
    "\n",
    "file_cache = SimpleFileCache(Path(\"output/cache/llm_cache.json\"))\n",
    "k = \"demo\"\n",
    "print(file_cache.get(k))\n",
    "file_cache.set(k, \"hello\")\n",
    "print(file_cache.get(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe37600",
   "metadata": {},
   "source": [
    "## Logging (minimum viable request log)\n",
    "\n",
    "A minimal request log should include:\n",
    "\n",
    "- request id\n",
    "- model\n",
    "- latency\n",
    "- success/failure\n",
    "- failure location (network vs parsing vs validation)\n",
    "\n",
    "Two extra fields that help later:\n",
    "\n",
    "- prompt length (or token estimate)\n",
    "- retry attempt count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9282397",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"demo\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def fake_llm_call(text: str) -> str:\n",
    "    time.sleep(0.05)\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "def logged_call(request_id: str, req: LLMRequest) -> str:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        out = fake_llm_call(req.user_prompt)\n",
    "        logger.info(\n",
    "            \"llm_call_ok\",\n",
    "            extra={\n",
    "                \"request_id\": request_id,\n",
    "                \"model\": req.model,\n",
    "                \"latency_s\": time.time() - t0,\n",
    "                \"prompt_len\": len(req.system_prompt) + len(req.user_prompt),\n",
    "                \"attempt\": 0,\n",
    "            },\n",
    "        )\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        logger.warning(\n",
    "            \"llm_call_failed\",\n",
    "            extra={\n",
    "                \"request_id\": request_id,\n",
    "                \"model\": req.model,\n",
    "                \"latency_s\": time.time() - t0,\n",
    "                \"attempt\": 0,\n",
    "                \"error_type\": type(e).__name__,\n",
    "            },\n",
    "        )\n",
    "        raise\n",
    "\n",
    "\n",
    "print(logged_call(\"req_001\", req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5cf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_call(cache_obj: SimpleMemoryCache, req: LLMRequest) -> str:\n",
    "    key = make_cache_key(req)\n",
    "    hit = cache_obj.get(key)\n",
    "    if hit is not None:\n",
    "        logger.info(\"llm_cache_hit\", extra={\"model\": req.model})\n",
    "        return hit\n",
    "\n",
    "    out = fake_llm_call(req.user_prompt)\n",
    "    cache_obj.set(key, out)\n",
    "    logger.info(\"llm_cache_set\", extra={\"model\": req.model})\n",
    "    return out\n",
    "\n",
    "\n",
    "cache2 = SimpleMemoryCache()\n",
    "print(cached_call(cache2, req))\n",
    "print(cached_call(cache2, req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cache_key_todo(req: LLMRequest) -> str:\n",
    "    # TODO: extend the key so it would remain correct if you add fields like:\n",
    "    # - top_p\n",
    "    # - max_tokens\n",
    "    # - tool schema / tool definitions\n",
    "    # - few-shot examples\n",
    "    return make_cache_key(req)\n",
    "\n",
    "\n",
    "def should_cache(req: LLMRequest) -> bool:\n",
    "    # TODO: implement policy, e.g.\n",
    "    # - cache only if temperature == 0.0\n",
    "    # - avoid caching very large prompts\n",
    "    return req.temperature == 0.0 and (len(req.system_prompt) + len(req.user_prompt)) <= 10_000\n",
    "\n",
    "\n",
    "print(\"make_cache_key_todo:\", make_cache_key_todo(req)[:12])\n",
    "print(\"should_cache:\", should_cache(req))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3488d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- `functools.lru_cache`: https://docs.python.org/3/library/functools.html#functools.lru_cache\n",
    "- Python logging: https://docs.python.org/3/library/logging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086885d",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for `make_cache_key_todo` and `should_cache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b32c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cache_key_todo(req: LLMRequest) -> str:\n",
    "    # Safe approach: serialize all known fields deterministically.\n",
    "    raw = json.dumps(asdict(req), sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def should_cache(req: LLMRequest) -> bool:\n",
    "    # Conservative policy for reproducibility + correctness.\n",
    "    if req.temperature != 0.0:\n",
    "        return False\n",
    "    prompt_len = len(req.system_prompt) + len(req.user_prompt)\n",
    "    return prompt_len <= 10_000\n",
    "\n",
    "\n",
    "print(\"solution key:\", make_cache_key_todo(req)[:12])\n",
    "print(\"solution should_cache:\", should_cache(req))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
