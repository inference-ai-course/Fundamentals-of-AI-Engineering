{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Part 05: LLM Client Skeleton Lab\n",
    "\n",
    "**Estimated time:** 120–150 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on reliability patterns:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Chapter 5: Resource Monitoring and Containerization](../self_learn/Chapters/5/Chapter5.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 05)\n",
    "\n",
    "- You have a reusable LLM client with:\n",
    "  - Timeouts\n",
    "  - Retries with backoff\n",
    "  - Rate limiting\n",
    "  - Caching\n",
    "  - Logging\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook:\n",
    "- You can call the LLM client with retries\n",
    "- You can demonstrate cache hits\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Build a production-ready LLM client\n",
    "- Combine timeouts, retries, caching, and logging\n",
    "- Create reusable client code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4df20",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook assembles all the reliability layers from Parts 01–04 into a **single reusable `LLMClient` class** — your building block for every LLM project going forward.\n",
    "\n",
    "The client combines:\n",
    "- **Cache check** (Part 04) — return cached result if available\n",
    "- **Provider call** with timeout (Part 01)\n",
    "- **Retry loop** with backoff (Part 02)\n",
    "- **Structured logging** (Part 04)\n",
    "\n",
    "**Design principle:** The provider-specific API call is isolated in `_provider_call()`. Everything else (caching, retries, logging) is provider-agnostic. Swapping from OpenAI to Ollama to Anthropic only requires changing `_provider_call()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36e3aa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Your goal is a single module you can reuse across projects that provides:\n",
    "\n",
    "- timeouts\n",
    "- retries + backoff\n",
    "- basic rate limit handling\n",
    "- basic caching\n",
    "- logging\n",
    "\n",
    "In this lab you’ll implement a provider-agnostic skeleton and keep the provider-specific API call isolated in `_provider_call`.\n",
    "\n",
    "If you want the deeper reliability-boundary discussion, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841061a",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `LLMRequest`, `make_cache_key()`, `SimpleMemoryCache`, and the full `LLMClient` class.\n",
    "\n",
    "**Walk through `LLMClient.call()` step by step:**\n",
    "1. Generate a unique `request_id` (UUID) for tracing\n",
    "2. Compute the cache key from the request\n",
    "3. Check cache — if hit, log and return immediately\n",
    "4. Enter retry loop (up to `max_retries + 1` attempts):\n",
    "   - Call `_provider_call()` with timeout\n",
    "   - On success: log, cache the result, return\n",
    "   - On failure: log the error, sleep with backoff, retry\n",
    "5. After all retries exhausted: raise `RuntimeError`\n",
    "\n",
    "**Your task:** Implement `_provider_call()` using your chosen provider's SDK (OpenAI, Ollama, etc.). Everything else in the client is already production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0ef1d",
   "metadata": {},
   "source": [
    "## Skeleton design\n",
    "\n",
    "We’ll define:\n",
    "\n",
    "- a request payload (model + prompt + settings)\n",
    "- a stable cache key\n",
    "- a `call()` method\n",
    "\n",
    "The cache key must represent the “effective input” to the model. If two requests differ in any setting that can change output, they must not share a key.\n",
    "\n",
    "Next you’ll implement a provider-agnostic skeleton you can adapt later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc4ca2",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `add_jitter()` and `backoff_delay()` — two helpers for improving the retry backoff strategy.\n",
    "\n",
    "**`backoff_delay(attempt)`** — exponential backoff: `min(cap, base * 2^(attempt-1))` → 0.2s, 0.4s, 0.8s, 1.6s, 2.0s (capped)\n",
    "\n",
    "**`add_jitter(delay_s)`** — randomizes the delay within `[0, delay_s]` (\"full jitter\"). This prevents multiple clients from retrying at exactly the same moment after a shared failure.\n",
    "\n",
    "**Your task:** Implement both functions. The current stubs return `0.0` for jitter and `0.0` for backoff — replace them with the real formulas. The solution is in the Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56bc70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import uuid\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LLMRequest:\n",
    "    model: str\n",
    "    prompt: str\n",
    "    temperature: float = 0.0\n",
    "\n",
    "\n",
    "def make_cache_key(req: LLMRequest) -> str:\n",
    "    raw = json.dumps(asdict(req), sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "class SimpleMemoryCache:\n",
    "    def __init__(self) -> None:\n",
    "        self._store = {}\n",
    "\n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        return self._store.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._store[key] = value\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, cache: Optional[SimpleMemoryCache] = None) -> None:\n",
    "        self._cache = cache or SimpleMemoryCache()\n",
    "\n",
    "    def _provider_call(self, req: LLMRequest, *, timeout_s: float) -> str:\n",
    "        # TODO: Implement provider-specific HTTP/API call.\n",
    "        # This method must:\n",
    "        # - respect timeout_s\n",
    "        # - raise clear exceptions for retry classification\n",
    "        time.sleep(min(0.05, float(timeout_s)))\n",
    "        return \"echo: %s\" % req.prompt\n",
    "\n",
    "    def call(self, req: LLMRequest, *, timeout_s: float = 30.0, max_retries: int = 2) -> str:\n",
    "        request_id = str(uuid.uuid4())\n",
    "        cache_key = make_cache_key(req)\n",
    "\n",
    "        cached = self._cache.get(cache_key)\n",
    "        if cached is not None:\n",
    "            logger.info(\"llm_cache_hit\", extra={\"request_id\": request_id, \"model\": req.model})\n",
    "            return cached\n",
    "\n",
    "        last_err = None\n",
    "        for attempt in range(max_retries + 1):\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                text = self._provider_call(req, timeout_s=timeout_s)\n",
    "                logger.info(\n",
    "                    \"llm_call_ok\",\n",
    "                    extra={\n",
    "                        \"request_id\": request_id,\n",
    "                        \"model\": req.model,\n",
    "                        \"latency_s\": time.time() - t0,\n",
    "                        \"attempt\": attempt,\n",
    "                    },\n",
    "                )\n",
    "                self._cache.set(cache_key, text)\n",
    "                return text\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                logger.warning(\n",
    "                    \"llm_call_failed\",\n",
    "                    extra={\n",
    "                        \"request_id\": request_id,\n",
    "                        \"model\": req.model,\n",
    "                        \"latency_s\": time.time() - t0,\n",
    "                        \"attempt\": attempt,\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                    },\n",
    "                )\n",
    "                if attempt < max_retries:\n",
    "                    time.sleep(min(2 ** attempt, 4))\n",
    "\n",
    "        raise RuntimeError(\"LLM call failed after retries: %s\" % last_err)\n",
    "\n",
    "\n",
    "# Quick sanity call (still provider-stubbed)\n",
    "client = LLMClient()\n",
    "print(client.call(LLMRequest(model=\"demo\", prompt=\"hello\"), timeout_s=1.0, max_retries=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8b83b",
   "metadata": {},
   "source": [
    "## Practice exercises\n",
    "\n",
    "1) Extend `LLMRequest` to include `system_prompt` and update `make_cache_key()` accordingly.\n",
    "\n",
    "2) Add jitter to the backoff so many clients do not retry at the same times.\n",
    "\n",
    "3) Add a simple 429 handler:\n",
    "\n",
    "- if `Retry-After` is present and small enough, sleep that long\n",
    "- otherwise backoff\n",
    "\n",
    "4) Add structured output validation (from Week 3) as an optional mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def add_jitter(delay_s: float) -> float:\n",
    "    # TODO: implement jitter.\n",
    "    # Example: \"full jitter\" uniform(0, delay_s).\n",
    "    return random.uniform(0.0, max(0.0, float(delay_s)))\n",
    "\n",
    "\n",
    "def backoff_delay(attempt: int, *, base: float = 0.5, cap: float = 8.0) -> float:\n",
    "    # TODO: implement exponential backoff with cap.\n",
    "    raw = base * (2 ** max(0, attempt - 1))\n",
    "    return min(cap, float(raw))\n",
    "\n",
    "\n",
    "for a in range(0, 5):\n",
    "    d = backoff_delay(a + 1, base=0.2, cap=2.0)\n",
    "    print(\"attempt\", a + 1, \"delay\", d, \"jittered\", add_jitter(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fd77e",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Implement `_provider_call()` using your chosen provider SDK.\n",
    "- Add structured output validation from Week 3.\n",
    "- Use this client in later pipeline/capstone work.\n",
    "\n",
    "## References\n",
    "\n",
    "- Python logging: https://docs.python.org/3/library/logging.html\n",
    "- Tenacity (for more robust retries): https://tenacity.readthedocs.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9512b7",
   "metadata": {},
   "source": [
    "## Exercise: Persist raw failures\n",
    "\n",
    "Goal:\n",
    "\n",
    "- If the provider call fails (e.g. timeout), persist a short JSON record under `output/`.\n",
    "- Return the written path.\n",
    "\n",
    "Checkpoint:\n",
    "\n",
    "- Trigger a failure (`force_error=True`) and confirm `output/raw_failure.json` exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e13158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def persist_raw_failure_todo(payload: dict, err: Exception, *, filename: str = \"raw_failure.json\") -> Path:\n",
    "    # TODO: implement\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(\"TODO\\n\", encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "try:\n",
    "    client.call(LLMRequest(model=\"demo\", prompt=\"this will fail\"), timeout_s=0.001, max_retries=0)\n",
    "except Exception as e:\n",
    "    p = persist_raw_failure_todo({\"model\": \"demo\", \"prompt\": \"this will fail\"}, e)\n",
    "    print(\"saved failure to\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9edcb2",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for `add_jitter`, `backoff_delay`, and `persist_raw_failure_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ce3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jitter(delay_s: float) -> float:\n",
    "    return random.uniform(0.0, max(0.0, float(delay_s)))\n",
    "\n",
    "\n",
    "def backoff_delay(attempt: int, *, base: float = 0.5, cap: float = 8.0) -> float:\n",
    "    raw = base * (2 ** max(0, attempt - 1))\n",
    "    return min(cap, float(raw))\n",
    "\n",
    "\n",
    "for a in range(0, 5):\n",
    "    d = backoff_delay(a + 1, base=0.2, cap=2.0)\n",
    "    print(\"solution attempt\", a + 1, \"delay\", d, \"jittered\", add_jitter(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_raw_failure_todo(payload: dict, err: Exception, *, filename: str = \"raw_failure.json\") -> Path:\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    record = {\n",
    "        \"payload\": payload,\n",
    "        \"error_type\": type(err).__name__,\n",
    "        \"error\": str(err),\n",
    "    }\n",
    "    out_path.write_text(json.dumps(record, indent=2), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "try:\n",
    "    client.call(LLMRequest(model=\"demo\", prompt=\"this will fail\"), timeout_s=0.001, max_retries=0)\n",
    "except Exception as e:\n",
    "    p = persist_raw_failure_todo({\"model\": \"demo\", \"prompt\": \"this will fail\"}, e, filename=\"raw_failure_solution.json\")\n",
    "    print(\"saved failure to\", p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
