{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Foundamental Course — Week 4 Practice (Starter Notebook)\n",
        "\n",
        "---\n",
        "\n",
        "## Pre-study (Self-learn)\n",
        "\n",
        "Foundamental Course assumes Self-learn is complete. If you need a refresher on production constraints and operational habits:\n",
        "\n",
        "- [Foundamental Course Pre-study index](../PRESTUDY.md)\n",
        "- [Self-learn — Chapter 5: Resource Monitoring and Containerization](../../self_learn/Chapters/5/Chapter5.md)\n",
        "\n",
        "---\n",
        "\n",
        "Starter LLM client skeleton: timeouts, retries, caching, and logs.\n",
        "\n",
        "## What success looks like (end of practice)\n",
        "\n",
        "- You can make a call through `llm_call(...)` and see retry behavior.\n",
        "- You can demonstrate a cache hit.\n",
        "- You can point to at least one raw failure saved under `output/`.\n",
        "\n",
        "### Checkpoint\n",
        "\n",
        "- `llm_call('hello', ...)` returns a response dict.\n",
        "- Re-running the same call yields a cache hit.\n",
        "\n",
        "## References (docs)\n",
        "- `requests` timeouts: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts\n",
        "- Tenacity: https://tenacity.readthedocs.io/\n",
        "- Python `logging`: https://docs.python.org/3/library/logging.html\n",
        "- HTTP 429 Too Many Requests (MDN): https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
        "- Twelve-Factor App: https://12factor.net/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "This notebook demonstrates patterns without requiring a real API key.\n",
        "Replace `fake_provider_call(...)` with a real provider call later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict\n",
        "\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s %(message)s')\n",
        "logger = logging.getLogger('llm_client')\n",
        "\n",
        "CACHE: Dict[str, Any] = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stable cache keys\n",
        "\n",
        "Cache keys should include everything that changes output: model, prompt, temperature, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_cache_key(payload: dict) -> str:\n",
        "    raw = json.dumps(payload, sort_keys=True, ensure_ascii=False).encode('utf-8')\n",
        "    return hashlib.sha256(raw).hexdigest()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Provider call stub\n",
        "\n",
        "Simulate transient failures so you can test retries/timeouts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fake_provider_call(payload: dict, timeout_s: float) -> dict:\n",
        "    if payload.get('force_error'):\n",
        "        raise requests.Timeout('Simulated timeout')\n",
        "    time.sleep(0.05)\n",
        "    return {\n",
        "        'text': 'echo: ' + str(payload.get('prompt', '')) ,\n",
        "        'model': payload.get('model', 'fake'),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM client skeleton\n",
        "\n",
        "Implements timeout + retry/backoff + caching + logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class LLMConfig:\n",
        "    model: str = 'fake-model'\n",
        "    timeout_s: float = 10.0\n",
        "    max_retries: int = 3\n",
        "\n",
        "cfg = LLMConfig()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def llm_call(prompt: str, *, config: LLMConfig, force_error: bool = False) -> dict:\n",
        "    payload = {\n",
        "        'model': config.model,\n",
        "        'prompt': prompt,\n",
        "        'force_error': force_error,\n",
        "    }\n",
        "    cache_key = make_cache_key(payload)\n",
        "    if cache_key in CACHE:\n",
        "        logger.info('cache_hit')\n",
        "        return CACHE[cache_key]\n",
        "\n",
        "    @retry(stop=stop_after_attempt(config.max_retries), wait=wait_exponential(multiplier=0.5, min=0.5, max=4.0))\n",
        "    def _call_once():\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            return fake_provider_call(payload, timeout_s=config.timeout_s)\n",
        "        finally:\n",
        "            logger.info('latency_ms=%s' % int((time.time()-t0)*1000))\n",
        "\n",
        "    resp = _call_once()\n",
        "    CACHE[cache_key] = resp\n",
        "    return resp\n",
        "\n",
        "llm_call('hello', config=cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: persist raw failures (TODO)\n",
        "\n",
        "Implement the TODO function below.\n",
        "\n",
        "Goal:\n",
        "\n",
        "- If the provider call fails (e.g. timeout), persist a short JSON record under `output/`.\n",
        "- Return the written path.\n",
        "\n",
        "Checkpoint:\n",
        "\n",
        "- Trigger a failure (`force_error=True`) and confirm `output/raw_failure.json` exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40b785b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "\n",
        "OUTPUT_DIR = Path(\"output\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def persist_raw_failure_todo(payload: dict, err: Exception, *, filename: str = \"raw_failure.json\") -> Path:\n",
        "    # TODO: implement\n",
        "    out_path = OUTPUT_DIR / filename\n",
        "    out_path.write_text(\"TODO\\n\", encoding=\"utf-8\")\n",
        "    return out_path\n",
        "\n",
        "\n",
        "try:\n",
        "    llm_call(\"this will fail\", config=cfg, force_error=True)\n",
        "except Exception as e:\n",
        "    p = persist_raw_failure_todo({\"model\": cfg.model, \"prompt\": \"this will fail\"}, e)\n",
        "    print(\"saved failure to\", p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28920153",
      "metadata": {},
      "source": [
        "## Appendix: Solutions (peek only after trying)\n",
        "\n",
        "Reference implementation for `persist_raw_failure_todo`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0401fba6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def persist_raw_failure_todo(payload: dict, err: Exception, *, filename: str = \"raw_failure.json\") -> Path:\n",
        "    out_path = OUTPUT_DIR / filename\n",
        "    record = {\n",
        "        \"payload\": payload,\n",
        "        \"error_type\": type(err).__name__,\n",
        "        \"error\": str(err),\n",
        "    }\n",
        "    out_path.write_text(json.dumps(record, indent=2), encoding=\"utf-8\")\n",
        "    return out_path\n",
        "\n",
        "\n",
        "try:\n",
        "    llm_call(\"this will fail\", config=cfg, force_error=True)\n",
        "except Exception as e:\n",
        "    p = persist_raw_failure_todo({\"model\": cfg.model, \"prompt\": \"this will fail\"}, e, filename=\"raw_failure_solution.json\")\n",
        "    print(\"saved failure to\", p)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
