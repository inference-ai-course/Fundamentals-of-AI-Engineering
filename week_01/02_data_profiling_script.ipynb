{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 — Part 02: Data profiling script (CSV → JSON/Markdown)\n",
    "\n",
    "**Estimated time:** 90–120 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Treat real-world CSV data as untrusted input\n",
    "- Build a deterministic profiling artifact (`profile.json` + `profile.md`)\n",
    "- Fail fast with clear errors for missing/empty inputs\n",
    "- Add optional schema/required-column checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84523fca",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In AI/ML/LLM projects, most pain starts with data issues:\n",
    "\n",
    "- wrong column names\n",
    "- unexpected types\n",
    "- empty files\n",
    "- missing values\n",
    "\n",
    "A data profiling script makes these issues visible early.\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundamental Course assumes Self-learn is complete. If you need a refresher on modules, exceptions, file I/O, or JSON:\n",
    "\n",
    "- [Foundamental Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Modules and exception handling](../self_learn/Chapters/2/02_modules_exceptions.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 02)\n",
    "\n",
    "Given the same input CSV, your code should always produce:\n",
    "\n",
    "- `output/profile.json` (machine-readable)\n",
    "- `output/profile.md` (human-readable)\n",
    "\n",
    "And it should fail with clear errors for:\n",
    "\n",
    "- missing file\n",
    "- empty file\n",
    "- missing required columns (optional extension)\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After you run the notebook end-to-end, you should see `output/profile.json` and `output/profile.md` on disk.\n",
    "\n",
    "Key reproducibility detail: keep outputs deterministic so diffs are meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "## Output contract (recap)\n",
    "\n",
    "Given the same input CSV, the script should always produce:\n",
    "\n",
    "- `output/profile.json`\n",
    "- `output/profile.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91574df",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines the core data structures and helper functions for the profiling script:\n",
    "\n",
    "- **`Profile` dataclass** — a typed container for all profile fields (row count, column types, missing value counts). Using a dataclass makes it easy to serialize to JSON with `asdict()`.\n",
    "- **`load_csv(path)`** — validates the file exists and is non-empty *before* reading it. This is \"fail fast\" defensive programming: catch problems at the boundary, not deep inside your logic.\n",
    "- **`make_profile(df)`** — computes the profile from a DataFrame. Notice `int(...)` casts — pandas returns numpy integers which are not JSON-serializable by default.\n",
    "\n",
    "**Why `sort_keys=True` matters:** Running the same script twice on the same input should produce byte-for-byte identical JSON. Sorted keys guarantee that — without it, dict ordering can vary between Python versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42db880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception as e:  # pragma: no cover\n",
    "    pd = None\n",
    "    _pd_import_error = e\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Profile:\n",
    "    rows: int\n",
    "    cols: int\n",
    "    columns: List[str]\n",
    "    dtypes: Dict[str, str]\n",
    "    missing_by_column: Dict[str, int]\n",
    "\n",
    "\n",
    "def load_csv(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\"Input file not found: %s\" % path)\n",
    "    if path.stat().st_size == 0:\n",
    "        raise ValueError(\"Input file is empty: %s\" % path)\n",
    "    if pd is None:\n",
    "        raise RuntimeError(\"pandas is required: %s\" % _pd_import_error)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def make_profile(df) -> Profile:\n",
    "    missing = df.isna().sum().to_dict()\n",
    "    dtypes = {col: str(dtype) for col, dtype in df.dtypes.to_dict().items()}\n",
    "    return Profile(\n",
    "        rows=int(df.shape[0]),\n",
    "        cols=int(df.shape[1]),\n",
    "        columns=list(df.columns),\n",
    "        dtypes=dtypes,\n",
    "        missing_by_column={k: int(v) for k, v in missing.items()},\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aaa8d8",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Creates a small sample CSV file with intentional data quality issues (a `None` in `age`, a `None` in `country`) so you can see how the profiler handles missing values.\n",
    "\n",
    "**Why synthetic data?** Using a known dataset lets you verify the profiler's output is correct — you know exactly how many missing values to expect. Real data often has surprises; start with controlled inputs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4b32ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote sample: output/sample_profile.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a small sample CSV for profiling (non-verbatim example)\n",
    "if pd is not None:\n",
    "    sample_path = OUTPUT_DIR / \"sample_profile.csv\"\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"user_id\": [1, 2, 3, 4],\n",
    "            \"age\": [22, None, 35, 29],\n",
    "            \"country\": [\"US\", \"SG\", None, \"US\"],\n",
    "        }\n",
    "    )\n",
    "    df.to_csv(sample_path, index=False)\n",
    "    print(\"wrote sample:\", sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d5697",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `profile_to_markdown()` — converts the `Profile` dataclass into a human-readable Markdown table — then runs the full pipeline: load → profile → write both `profile.json` and `profile.md`.\n",
    "\n",
    "**Why two output formats?**\n",
    "- `profile.json` is machine-readable: downstream scripts can parse it programmatically.\n",
    "- `profile.md` is human-readable: you can open it in any editor or GitHub to quickly inspect results.\n",
    "\n",
    "**Reproducibility check:** Run this cell twice on the same input. The output files should be byte-for-byte identical. If they differ, something in your pipeline is non-deterministic (e.g., unsorted dict keys, timestamps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a1a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote: output/profile.json\n",
      "wrote: output/profile.md\n"
     ]
    }
   ],
   "source": [
    "def profile_to_markdown(p: Profile) -> str:\n",
    "    lines = []\n",
    "    lines.append(\"# Data Profile\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Rows: {p.rows}\")\n",
    "    lines.append(f\"- Columns: {p.cols}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Columns\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"| column | dtype | missing |\")\n",
    "    lines.append(\"|---|---|---:|\")\n",
    "    for col in p.columns:\n",
    "        lines.append(f\"| {col} | {p.dtypes.get(col, '')} | {p.missing_by_column.get(col, 0)} |\")\n",
    "    lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "if pd is not None:\n",
    "    df2 = load_csv(sample_path)\n",
    "    p = make_profile(df2)\n",
    "    (OUTPUT_DIR / \"profile.json\").write_text(json.dumps(asdict(p), indent=2, sort_keys=True), encoding=\"utf-8\")\n",
    "    (OUTPUT_DIR / \"profile.md\").write_text(profile_to_markdown(p), encoding=\"utf-8\")\n",
    "    print(\"wrote:\", OUTPUT_DIR / \"profile.json\")\n",
    "    print(\"wrote:\", OUTPUT_DIR / \"profile.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b84b8",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `require_columns()` — a validator that checks whether all required columns are present — and a `numeric_summary_todo()` stub for you to implement.\n",
    "\n",
    "**Why validate columns explicitly?** Downstream code often assumes specific column names exist. Without this check, a missing column causes a cryptic `KeyError` deep inside your logic. Validating at the boundary gives a clear, actionable error message.\n",
    "\n",
    "**Your task:** Implement `numeric_summary_todo()` to return min/mean/max for each numeric column. Hint: `df.select_dtypes(include='number').describe()` gives you most of what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba15cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO: extend with required columns + numeric summaries\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "def require_columns(df, required: List[str]) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\"Missing required columns: %s\" % missing)\n",
    "\n",
    "\n",
    "def numeric_summary_todo(df) -> Dict[str, Any]:\n",
    "    # TODO: implement basic numeric summaries (mean/min/max) per numeric column.\n",
    "    # Hint: df.select_dtypes(include='number').describe().to_dict() is a good starting point.\n",
    "    return {}\n",
    "\n",
    "\n",
    "print(\"TODO: extend with required columns + numeric summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28308b7",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for the numeric summaries extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_summary_todo(df) -> dict:\n",
    "    if pd is None:\n",
    "        raise RuntimeError(f\"pandas is required: {_pd_import_error}\")\n",
    "    return df.select_dtypes(include='number').describe().to_dict()\n",
    "\n",
    "\n",
    "if pd is not None:\n",
    "    df2 = load_csv(sample_path)\n",
    "    print(list(numeric_summary_todo(df2).keys())[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
