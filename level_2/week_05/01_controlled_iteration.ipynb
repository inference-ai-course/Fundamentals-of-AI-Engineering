{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 5 - 01 Controlled Iteration\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Change one variable at a time\n",
        "- Track runs with run_id\n",
        "- Record metrics per run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "RAG systems improve through disciplined iteration.\n",
        "\n",
        "If you change multiple things at once, you can’t explain improvements.\n",
        "\n",
        "## The underlying theory: metrics are noisy signals\n",
        "\n",
        "When you run an evaluation on a finite set of questions, a metric is an estimate of true performance.\n",
        "\n",
        "If a metric is an average of per-item outcomes $x_i$ (e.g. hit=0/1), then:\n",
        "\n",
        "$$\n",
        "\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
        "$$\n",
        "\n",
        "Small $n$ means more randomness: a single hard item can move the metric.\n",
        "\n",
        "### Confidence intuition (rule-of-thumb)\n",
        "\n",
        "For a 0/1 metric (a proportion) like hit rate, a rough standard error is:\n",
        "\n",
        "$$\n",
        "\\mathrm{SE}(\\hat{p}) \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n",
        "$$\n",
        "\n",
        "So if $\\hat{p}=0.5$ and $n=20$, $\\mathrm{SE}\\approx 0.11$.\n",
        "\n",
        "Practical implication: a small change (e.g. +0.03) may be sampling noise on small eval sets.\n",
        "\n",
        "## Controlled iteration: isolate cause → effect\n",
        "\n",
        "Freeze everything except one variable (chunk size/overlap, top_k, embedding model, rerank on/off). Keep the eval set and prompt template fixed.\n",
        "\n",
        "## Practical run artifacts (minimum)\n",
        "\n",
        "Per run id, save:\n",
        "\n",
        "- config used\n",
        "- metrics summary\n",
        "- top failures with evidence\n",
        "\n",
        "This gives reproducibility and rollback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Config list and run_id pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "configs = [\n",
        "    {'chunk_size': 600, 'overlap': 100, 'top_k': 5},\n",
        "    {'chunk_size': 800, 'overlap': 100, 'top_k': 5},\n",
        "]\n",
        "\n",
        "run_id = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "print('run_id', run_id)\n",
        "print('configs', configs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Add a metrics placeholder for each config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "for cfg in configs:\n",
        "    # TODO: run eval and fill metrics\n",
        "    results.append({'config': cfg, 'metrics': {}})\n",
        "\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Are you changing only one variable?\n",
        "- Do you keep run artifacts?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical run-id + folder layout\n",
        "\n",
        "A simple pattern:\n",
        "\n",
        "- `runs/2026-01-27_1400_chunk800_overlap150/`\n",
        "  - `config.json`\n",
        "  - `metrics.json`\n",
        "  - `failures.json`\n",
        "  - `samples.md`\n",
        "\n",
        "This gives you a paper-trail for comparisons and rollbacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: standard error intuition\n",
        "\n",
        "Compute the rule-of-thumb standard error for a proportion metric.\n",
        "\n",
        "Use this to sanity-check whether small metric deltas are plausibly noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def se_proportion(p_hat: float, n: int) -> float:\n",
        "    return math.sqrt((p_hat * (1.0 - p_hat)) / n)\n",
        "\n",
        "\n",
        "p_hat = 0.5\n",
        "n = 20\n",
        "print(\"SE:\", round(se_proportion(p_hat, n), 3))\n",
        "print(\"Typical 1-sigma band:\", (round(p_hat - se_proportion(p_hat, n), 3), round(p_hat + se_proportion(p_hat, n), 3)))\n",
        "\n",
        "# Example question:\n",
        "# If hit@k moves from 0.50 to 0.55 on n=20, is that obviously meaningful?\n",
        "print(\"delta=0.05 vs SE=\", round(se_proportion(p_hat, n), 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31750cd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class RunResult:\n",
        "    run_id: str\n",
        "    change: str\n",
        "    metric_before: float\n",
        "    metric_after: float\n",
        "    notes: str\n",
        "\n",
        "\n",
        "# Example experiment table (fill with your own numbers)\n",
        "results_table: list[RunResult] = [\n",
        "    RunResult(run_id=\"baseline\", change=\"none\", metric_before=0.45, metric_after=0.45, notes=\"baseline\"),\n",
        "    RunResult(run_id=\"chunk800\", change=\"chunk_size=800\", metric_before=0.45, metric_after=0.52, notes=\"improved recall, citations ok\"),\n",
        "]\n",
        "\n",
        "for r in results_table:\n",
        "    print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389f6024",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "- Choose one variable to change (e.g. `chunk_size`).\n",
        "- Keep everything else fixed.\n",
        "- Record:\n",
        "  - before/after metric values\n",
        "  - 3–5 concrete failure examples\n",
        "  - a short note describing the tradeoff (e.g. recall improved but citations got worse)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0362ba3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create your own run table entry.\n",
        "#\n",
        "# Example:\n",
        "# results_table.append(\n",
        "#     RunResult(run_id=\"...\", change=\"...\", metric_before=..., metric_after=..., notes=\"...\")\n",
        "# )\n",
        "#\n",
        "# Then print the updated table."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c70d94",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Did you change exactly one variable?\n",
        "- Is your eval set fixed across the tuning burst?\n",
        "- Can you reproduce a run from artifacts (config + outputs)?\n",
        "- Did you record at least one regression and a rollback decision?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
