{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 5 - 01 Controlled Iteration\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Change one variable at a time\n",
        "- Track runs with run_id\n",
        "- Record metrics per run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "Controlled iteration prevents confusion about why metrics changed.\n",
        "Keep the eval set fixed while you test one change.\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Define a list of configs.\n",
        "- Record run_id and metrics per config.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Config list and run_id pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "configs = [\n",
        "    {'chunk_size': 600, 'overlap': 100, 'top_k': 5},\n",
        "    {'chunk_size': 800, 'overlap': 100, 'top_k': 5},\n",
        "]\n",
        "\n",
        "run_id = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "print('run_id', run_id)\n",
        "print('configs', configs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Add a metrics placeholder for each config.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "results = []\n",
        "for cfg in configs:\n",
        "    # TODO: run eval and fill metrics\n",
        "    results.append({'config': cfg, 'metrics': {}})\n",
        "\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Are you changing only one variable?\n",
        "- Do you keep run artifacts?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Legacy practice content from practice.ipynb\n\n",
        "# Level 2 \u2014 Week 5 Practice: RAG Evaluation Basics\n",
        "\n",
        "**Estimated time:** 60\u201390 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Define a minimal evaluation set\n",
        "- Track simple quality metrics (coverage, refusal, non-empty)\n",
        "- Log failures for iteration\n",
        "- Produce a small evaluation summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Legacy practice content from practice.ipynb\n\n",
        "## Overview\n",
        "\n",
        "This practice builds a tiny evaluation loop. The goal is not perfect metrics,\n",
        "just a consistent routine for comparing changes.\n",
        "\n",
        "You will:\n",
        "\n",
        "1. Create a small evaluation dataset (10\u201320 items).\n",
        "2. Implement a loop that scores outputs and logs failures.\n",
        "3. Summarize results in a dictionary.\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Fill in the sample dataset below.\n",
        "- Implement evaluation checks.\n",
        "- Print a summary and a few failures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Legacy practice content\n",
        "TASK_5_1_GUIDE = \"\"\"\n",
        "Task 5.1: Evaluation loop\n",
        "\n",
        "Implement evaluation checks and track simple metrics.\n",
        "\n",
        "Checklist:\n",
        "- Define 10-20 eval items\n",
        "- Track answer_nonempty and citation_coverage\n",
        "- Log failures for review\n",
        "\"\"\"\n",
        "\n",
        "print(TASK_5_1_GUIDE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31750cd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Legacy practice content\n",
        "from typing import List, Dict\n",
        "\n",
        "EvalItem = Dict\n",
        "\n",
        "items: List[EvalItem] = [\n",
        "    {\"query\": \"What is the return policy?\", \"answer\": \"Returns in 30 days.\", \"citations\": [\"doc-1\"]},\n",
        "    {\"query\": \"Do you offer refunds?\", \"answer\": \"\", \"citations\": []},\n",
        "]\n",
        "\n",
        "def evaluate(items: List[EvalItem]) -> Dict:\n",
        "    failures = []\n",
        "    for item in items:\n",
        "        has_answer = bool(item.get(\"answer\"))\n",
        "        has_citations = bool(item.get(\"citations\"))\n",
        "        if not (has_answer and has_citations):\n",
        "            failures.append(item)\n",
        "    return {\n",
        "        \"n\": len(items),\n",
        "        \"answer_nonempty\": sum(bool(i.get(\"answer\")) for i in items),\n",
        "        \"citation_coverage\": sum(bool(i.get(\"citations\")) for i in items),\n",
        "        \"failures\": failures,\n",
        "    }\n",
        "\n",
        "summary = evaluate(items)\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389f6024",
      "metadata": {},
      "source": [
        "Legacy practice content from practice.ipynb\n\n",
        "### Task 5.2: Failure review\n",
        "\n",
        "Print a small subset of failures for quick iteration notes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0362ba3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Legacy practice content\n",
        "for item in summary[\"failures\"][:3]:\n",
        "    print(\"failure:\", item)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c70d94",
      "metadata": {},
      "source": [
        "Legacy practice content from practice.ipynb\n\n",
        "## Self-check\n",
        "\n",
        "- Is your dataset small but representative?\n",
        "- Are failures visible and easy to inspect?\n",
        "- Can you compare runs consistently?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}