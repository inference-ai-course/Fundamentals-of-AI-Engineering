{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 5 - 03 Eval Script and Failure Analysis\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Compute metrics from eval items\n",
        "- Log failures with evidence\n",
        "- Save run artifacts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "A good `eval_rag.py` does two things:\n",
        "\n",
        "- prints metrics\n",
        "- prints failures with evidence\n",
        "\n",
        "## Minimum metrics (recommended)\n",
        "\n",
        "Define each metric precisely before coding it.\n",
        "\n",
        "- retrieval hit rate / hit@k (or recall@k if you have graded relevance)\n",
        "- citation coverage rate\n",
        "- refusal correctness rate\n",
        "\n",
        "Example definition:\n",
        "\n",
        "- citation coverage rate = fraction of `mode=answer` responses that have >= 1 valid citation\n",
        "\n",
        "## Metric formalization (examples)\n",
        "\n",
        "Let the eval set have $n$ items.\n",
        "\n",
        "### Hit@k (retrieval)\n",
        "\n",
        "Let $h_i \\in \\{0,1\\}$ indicate whether item $i$ retrieved at least one relevant chunk in top-k.\n",
        "\n",
        "$$\n",
        "\\mathrm{Hit@k} = \\frac{1}{n} \\sum_{i=1}^{n} h_i\n",
        "$$\n",
        "\n",
        "### Mode / refusal correctness\n",
        "\n",
        "Let $r_i \\in \\{0,1\\}$ indicate whether the predicted mode matches the expected mode.\n",
        "\n",
        "$$\n",
        "\\mathrm{ModeCorrect} = \\frac{1}{n} \\sum_{i=1}^{n} r_i\n",
        "$$\n",
        "\n",
        "Writing definitions down prevents “metric drift” where different runs compute different things.\n",
        "\n",
        "## Uncertainty intuition (don’t overfit)\n",
        "\n",
        "With only 10–20 items, you can overfit by tuning until the eval looks good without truly improving.\n",
        "\n",
        "Practical guardrails:\n",
        "\n",
        "- keep a small hidden set (even 5 items) you don’t look at during tuning\n",
        "- or periodically compare to a frozen baseline configuration\n",
        "\n",
        "## Failure labeling (root cause)\n",
        "\n",
        "For each failure, label one primary cause:\n",
        "\n",
        "- `retrieval_miss`\n",
        "- `context_too_noisy`\n",
        "- `prompt_ambiguous`\n",
        "- `citation_invalid`\n",
        "\n",
        "Always add one short note: “what would have made this succeed?”\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Implement metric calculations.\n",
        "- Emit failure records with evidence fields (so you can debug without rerunning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Minimal evaluation loop with failures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(items: list[dict]) -> dict:\n",
        "    failures = []\n",
        "    for item in items:\n",
        "        if item.get('actual_mode') != item.get('expected_mode'):\n",
        "            failures.append(item)\n",
        "    return {'n': len(items), 'failures': failures}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Implement metric calculations and failure records.\n",
        "\n",
        "Suggested per-item fields (minimal):\n",
        "\n",
        "- `id`, `question`\n",
        "- `expected_mode`, `actual_mode`\n",
        "- `relevant_chunk_ids`, `retrieved_chunk_ids`\n",
        "- `citations` (list of objects with at least `chunk_id`)\n",
        "- `label` (root cause)\n",
        "- `note` (what would have made this succeed?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "\n",
        "def hit_at_k(relevant_chunk_ids: list[str], retrieved_chunk_ids: list[str]) -> int:\n",
        "    if not relevant_chunk_ids:\n",
        "        return 0\n",
        "    relevant = set(relevant_chunk_ids)\n",
        "    return int(any(cid in relevant for cid in retrieved_chunk_ids))\n",
        "\n",
        "\n",
        "def citation_coverage_for_item(actual_mode: str, citations: list[dict]) -> int:\n",
        "    if actual_mode != \"answer\":\n",
        "        return 0\n",
        "    return int(len(citations) > 0)\n",
        "\n",
        "\n",
        "def mode_correct(expected_mode: str, actual_mode: str) -> int:\n",
        "    return int(expected_mode == actual_mode)\n",
        "\n",
        "\n",
        "def evaluate(items: list[dict]) -> dict:\n",
        "    failures: list[dict] = []\n",
        "\n",
        "    hit_sum = 0\n",
        "    mode_sum = 0\n",
        "    citation_cov_sum = 0\n",
        "    n_answer = 0\n",
        "\n",
        "    for it in items:\n",
        "        expected = it.get(\"expected_mode\", \"\")\n",
        "        actual = it.get(\"actual_mode\", \"\")\n",
        "        retrieved = it.get(\"retrieved_chunk_ids\", [])\n",
        "        relevant = it.get(\"relevant_chunk_ids\", [])\n",
        "        citations = it.get(\"citations\", [])\n",
        "\n",
        "        hit_sum += hit_at_k(relevant, retrieved)\n",
        "        mode_sum += mode_correct(expected, actual)\n",
        "\n",
        "        if actual == \"answer\":\n",
        "            n_answer += 1\n",
        "            citation_cov_sum += citation_coverage_for_item(actual, citations)\n",
        "\n",
        "        if expected != actual:\n",
        "            failures.append(\n",
        "                {\n",
        "                    \"id\": it.get(\"id\"),\n",
        "                    \"question\": it.get(\"question\"),\n",
        "                    \"expected_mode\": expected,\n",
        "                    \"actual_mode\": actual,\n",
        "                    \"retrieved_chunk_ids\": retrieved,\n",
        "                    \"citations\": citations,\n",
        "                    \"label\": it.get(\"label\", \"\"),\n",
        "                    \"note\": it.get(\"note\", \"\"),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    n = len(items)\n",
        "    return {\n",
        "        \"n_items\": n,\n",
        "        \"hit_at_k\": hit_sum / n if n else 0.0,\n",
        "        \"mode_correct\": mode_sum / n if n else 0.0,\n",
        "        \"citation_coverage\": (citation_cov_sum / n_answer) if n_answer else 0.0,\n",
        "        \"n_failures\": len(failures),\n",
        "        \"failures\": failures,\n",
        "    }\n",
        "\n",
        "\n",
        "# Minimal synthetic example (replace with real `/search` + `/chat` outputs)\n",
        "items = [\n",
        "    {\n",
        "        \"id\": \"q_001\",\n",
        "        \"question\": \"What endpoint shows service health?\",\n",
        "        \"expected_mode\": \"answer\",\n",
        "        \"actual_mode\": \"answer\",\n",
        "        \"relevant_chunk_ids\": [\"fastapi#001\"],\n",
        "        \"retrieved_chunk_ids\": [\"fastapi#001\", \"misc#123\"],\n",
        "        \"citations\": [{\"chunk_id\": \"fastapi#001\"}],\n",
        "        \"label\": \"\",\n",
        "        \"note\": \"\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q_003\",\n",
        "        \"question\": \"What is the weather in Tokyo tomorrow?\",\n",
        "        \"expected_mode\": \"refuse\",\n",
        "        \"actual_mode\": \"answer\",\n",
        "        \"relevant_chunk_ids\": [],\n",
        "        \"retrieved_chunk_ids\": [\"misc#123\"],\n",
        "        \"citations\": [],\n",
        "        \"label\": \"prompt_ambiguous\",\n",
        "        \"note\": \"Mode decision should be based on retrieval signals (empty/low score), not prompt vibes.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "summary = evaluate(items)\n",
        "print({k: v for k, v in summary.items() if k != \"failures\"})\n",
        "print(\"failures:\")\n",
        "for f in summary[\"failures\"]:\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Are metric definitions written down and implemented consistently?\n",
        "- Do failures include enough evidence to debug without rerunning?\n",
        "- Can you compare two runs with the same eval set and see stable behavior?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
