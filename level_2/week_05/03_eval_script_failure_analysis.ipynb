{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 5 - 03 Eval Script and Failure Analysis\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Compute metrics from eval items\n",
        "- Log failures with evidence\n",
        "- Save run artifacts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "Eval scripts should output metrics and failures.\n",
        "Artifacts make results reproducible.\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Implement metrics for hit rate and refusal correctness.\n",
        "- Store failures with labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Minimal evaluation loop with failures.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate(items: list[dict]) -> dict:\n",
        "    failures = []\n",
        "    for item in items:\n",
        "        if item.get('actual_mode') != item.get('expected_mode'):\n",
        "            failures.append(item)\n",
        "    return {'n': len(items), 'failures': failures}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Add metric calculations and failure labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: compute hit_rate, citation_coverage, refusal_correctness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Do failures include evidence fields?\n",
        "- Are metrics written to a file?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}