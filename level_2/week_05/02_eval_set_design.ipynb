{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 5 - 02 Eval Set Design\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Create a 10 to 20 item eval set\n",
        "- Cover in-KB and out-of-KB cases\n",
        "- Store expected modes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "A minimal eval set creates repeatable signals.\n",
        "\n",
        "You don’t need hundreds of examples to start.\n",
        "\n",
        "## The underlying theory: you are sampling from a distribution\n",
        "\n",
        "Your real users generate questions from some unknown distribution $D$.\n",
        "\n",
        "An eval set is a sample from $D$ (or an approximation). If your eval set is biased, you will optimize the wrong behavior.\n",
        "\n",
        "Practical implication:\n",
        "\n",
        "- include the failure modes you actually care about, not just easy questions\n",
        "\n",
        "## Coverage beats size (early on)\n",
        "\n",
        "With 10–20 items, your goal is not high statistical confidence. Your goal is to cover:\n",
        "\n",
        "- common intents\n",
        "- near-miss cases (where retrieval must be correct)\n",
        "- out-of-KB boundary\n",
        "- ambiguous queries that should trigger clarification\n",
        "\n",
        "## Suggested composition (10–20 items)\n",
        "\n",
        "- 50% obvious in-KB\n",
        "- 30% near-miss (requires the correct chunk)\n",
        "- 20% out-of-KB (should refuse/clarify)\n",
        "\n",
        "## What to store per item (keep it mechanically checkable)\n",
        "\n",
        "- `id`\n",
        "- `question`\n",
        "- `expected_mode`: `answer|clarify|refuse`\n",
        "- (recommended) `relevant_chunk_ids`: one or more acceptable chunk_ids\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Draft eval items with `expected_mode` and `relevant_chunk_ids`.\n",
        "- Include at least:\n",
        "  - one out-of-KB case\n",
        "  - one ambiguous case that should trigger `clarify`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Example eval item format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_items = [\n",
        "    {\n",
        "        'id': 'q_001',\n",
        "        'question': 'What endpoint shows service health?',\n",
        "        'expected_mode': 'answer',\n",
        "        'relevant_chunk_ids': ['fastapi#001'],\n",
        "    },\n",
        "]\n",
        "\n",
        "print(eval_items)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Add more eval items.\n",
        "\n",
        "Checklist:\n",
        "\n",
        "- include 1–2 items for each `expected_mode`\n",
        "- keep labels simple and checkable\n",
        "- list acceptable `relevant_chunk_ids` (even if it’s just one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ALLOWED_MODES = {\"answer\", \"clarify\", \"refuse\"}\n",
        "\n",
        "\n",
        "eval_items = [\n",
        "    {\n",
        "        \"id\": \"q_001\",\n",
        "        \"question\": \"What endpoint shows service health?\",\n",
        "        \"expected_mode\": \"answer\",\n",
        "        \"relevant_chunk_ids\": [\"fastapi#001\"],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q_002\",\n",
        "        \"question\": \"How do I configure retries?\",\n",
        "        \"expected_mode\": \"clarify\",\n",
        "        \"relevant_chunk_ids\": [],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"q_003\",\n",
        "        \"question\": \"What is the weather in Tokyo tomorrow?\",\n",
        "        \"expected_mode\": \"refuse\",\n",
        "        \"relevant_chunk_ids\": [],\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "def validate_eval_item(item: dict) -> None:\n",
        "    if not item.get(\"id\"):\n",
        "        raise ValueError(\"missing id\")\n",
        "    if not item.get(\"question\"):\n",
        "        raise ValueError(\"missing question\")\n",
        "    if item.get(\"expected_mode\") not in ALLOWED_MODES:\n",
        "        raise ValueError(f\"invalid expected_mode: {item.get('expected_mode')}\")\n",
        "    if \"relevant_chunk_ids\" not in item:\n",
        "        raise ValueError(\"missing relevant_chunk_ids\")\n",
        "    if not isinstance(item[\"relevant_chunk_ids\"], list):\n",
        "        raise ValueError(\"relevant_chunk_ids must be a list\")\n",
        "\n",
        "\n",
        "for it in eval_items:\n",
        "    validate_eval_item(it)\n",
        "\n",
        "print(\"n_eval_items:\", len(eval_items))\n",
        "print(eval_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Does your eval set include failure cases?\n",
        "- Are labels checkable?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
