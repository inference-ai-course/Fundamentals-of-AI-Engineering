# Week 1 — Part 01: Requirements → architecture → API contracts

## Overview

In Level 2, you are no longer building “a script”. You’re building a **service**.

A service starts with:

- a user problem
- constraints (cost, latency, privacy)
- failure modes

Your job is to turn that into:

- modules with clear boundaries
- API endpoints with stable request/response schemas

---

## Step 1: Translate requirements into journeys

A practical way to do system design without getting lost:

- **Happy path user journey**
  - user asks question
  - system retrieves context
  - system answers with citations

- **Admin journey**
  - ingest or update knowledge base

- **Failure journeys**
  - empty KB
  - retrieval returns nothing
  - model timeout
  - invalid input

Add one more journey that students often skip:

- **Observability journey**
  - when the answer is wrong, how do we debug?
  - what logs do we need?
  - what identifiers link logs across components?

What to produce for this step (student-friendly):

- Write the journeys as short bullet stories (3–6 bullets each).
- Add at least one concrete example question for each journey.
  - Happy path example: “What is RAG?” (should retrieve an intro chunk).
  - Failure example: “What is our 2026 refund policy?” (if not in KB, should clarify/refuse).
- For observability, name the minimum debug artifacts:
  - a `request_id`
  - logged retrieval hit IDs/scores
  - the final `mode` decision (answer/clarify/refuse)

---

## Step 2: Convert journeys into minimal endpoints

A minimal contract for a RAG-style service usually includes:

- `GET /health`
  - for demo + readiness checks

- `POST /search`
  - retrieval-only endpoint
  - returns ranked chunks + metadata

- `POST /chat`
  - generation endpoint
  - uses `/search` results

Practical verification for Step 2:

- You should be able to point to exactly one endpoint responsible for retrieval (`/search`).
- You should be able to simulate a failure in each endpoint and describe what the user sees.
  - Example: invalid JSON input → 400 with a clear message.
  - Example: provider timeout → 502/503 with a `request_id`.

## What is RAG? (Retrieval-Augmented Generation)

### Definition

**RAG** is a system pattern where you:

1. **Retrieve** relevant context (documents/chunks) for a user question.
2. **Generate** an answer conditioned on that retrieved context.

The key idea is that the model should not answer from “memory” alone. It should answer using **evidence** returned by retrieval.

### Why it is needed (what problem it solves)

LLMs have two common failure modes in real systems:

- **Knowledge gap**: the model does not have your private/company/course knowledge.
- **Hallucination**: the model produces plausible-sounding but unsupported claims.

Retrieval adds an explicit “evidence supply” step:

- If the answer exists in your indexed data, retrieval can fetch it.
- If retrieval returns nothing or weak evidence, the system can refuse or ask clarifying questions.

### Minimal dataflow (the RAG loop)

```text
user question
  -> embed(question)
  -> vector search (top-k chunks)
  -> build CONTEXT (chunk texts + metadata)
  -> LLM(prompt + CONTEXT)
  -> answer + citations
```

### Practical implications for API design

- You want `/search` separate from `/chat` so you can debug retrieval independently.
- You want stable chunk identifiers (`chunk_id`) and metadata (`doc_id`, `url`, etc.) so you can cite sources.
- You want deterministic behavior when evidence is missing (clarify/refuse rules).

Optional (but common) additions you may want later:

- `POST /ingest` (admin)
- `GET /docs` (auto-generated by FastAPI)
- `GET /metrics` (if you add basic monitoring)

---

## Step 3: Define the API contract (examples first)

Before coding, write one example request/response per endpoint.

What to verify for Step 3:

- Requests include only what is needed (avoid “kitchen sink” schemas early).
- Responses are stable and debuggable:
  - retrieval returns `chunk_id`, `doc_id`, `score`, `metadata`
  - chat returns `mode` and citations (or a refusal/clarification message)

### Example: `/health`

Response:

```json
{"status": "ok"}
```

Design note: keep retrieval results rich enough for debugging.

- `chunk_id` lets you find the exact chunk in storage
- `score` lets you reason about ranking
- `metadata` lets you filter and cite

### Example: `/search`

Request:

```json
{"query": "What is RAG?", "top_k": 5, "filters": {"source": "docs"}}
```

Response:

```json
{
  "query": "What is RAG?",
  "hits": [
    {
      "doc_id": "rag_intro",
      "chunk_id": "rag_intro#02",
      "score": 0.82,
      "text": "Retrieval-Augmented Generation (RAG) ...",
      "metadata": {"source": "docs"}
    }
  ]
}
```

### Example: `/chat`

Request:

```json
{"question": "What is RAG?", "top_k": 5}
```

Response:

```json
{
  "answer": "RAG is ...",
  "citations": [
    {"doc_id": "rag_intro", "chunk_id": "rag_intro#02", "snippet": "Retrieval-Augmented Generation ..."}
  ]
}
```

---

## Step 4: Standardize error payloads

A simple, practical error shape:

```json
{
  "error": {
    "type": "invalid_request",
    "message": "top_k must be between 1 and 20"
  },
  "request_id": "c3b7f2c0-acde-4a5a-8db0-0fe2b72b2b6f"
}
```

Why this matters:

- clients can display `message`
- you can search logs by `request_id`
- your service contract stays stable even if internals change

Practical tips:

- Always include a human-actionable `message` (what to do next).
- Keep `type` values finite and predictable so clients can handle them (e.g., `invalid_request`, `rate_limited`, `provider_timeout`).
- Log the same `request_id` in your server logs so you can correlate a user report to internal traces.

If you want a standardized format later, you can align with RFC 7807 (Problem Details).

---

## Step 5: Write down boundaries (a tiny architecture doc)

You don’t need a huge system design doc. A half-page is enough:

- Components: `api`, `retrieval`, `generation`, `storage`
- Inputs/outputs: what each component consumes/produces
- Failure modes: what can go wrong and what the user sees

The discipline here is what prevents “spaghetti endpoints”.

What to verify for Step 5:

- Each component has a single primary responsibility.
  - retrieval does retrieval; chat does generation + validation; ingestion updates the index.
- You can say what happens when a component fails (and what the user sees).
- You can name the key “contracts” between components (input/output shapes), not just code files.

---

## Common pitfalls

- Coding without example request/response payloads.
- Mixing ingestion/retrieval/chat in one endpoint (hard to debug).
- No explicit failure behaviors.

---

## References

- HTTP status codes: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status
- FastAPI: https://fastapi.tiangolo.com/
- OpenAPI spec: https://spec.openapis.org/oas/latest.html
- SLOs: https://sre.google/sre-book/service-level-objectives/
- RFC 7807 (Problem Details for HTTP APIs): https://www.rfc-editor.org/rfc/rfc7807
- Pinecone RAG overview: https://www.pinecone.io/learn/retrieval-augmented-generation/
