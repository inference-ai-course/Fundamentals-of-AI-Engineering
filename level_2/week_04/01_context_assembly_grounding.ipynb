{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 4 - 01 Context Assembly and Grounding\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Assemble context blocks from hits\n",
        "- Control context length\n",
        "- Keep doc_id and chunk_id visible\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "In RAG, you are controlling *what the model is allowed to use*.\n",
        "\n",
        "Your goal:\n",
        "\n",
        "- the model answers from retrieved context\n",
        "- it does not invent unsupported claims\n",
        "\n",
        "## Grounding theory (what you are doing mathematically)\n",
        "\n",
        "### Grounding as constrained generation\n",
        "\n",
        "In an ideal grounded system, the modelâ€™s answer should be a function of:\n",
        "\n",
        "- the user question $q$\n",
        "- the retrieved evidence $E = \\{e_1, \\ldots, e_k\\}$\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "$$\n",
        "\\text{answer} \\approx g(q, E)\n",
        "$$\n",
        "\n",
        "Practical implication:\n",
        "\n",
        "- if $E$ is empty or irrelevant, the system should not guess\n",
        "- refusal/clarification is a decision based on retrieval signals\n",
        "\n",
        "### Context budget constraint\n",
        "\n",
        "Even before the model starts answering, you spend tokens on structure:\n",
        "\n",
        "$$\n",
        "C \\approx T_{\\text{system}} + T_{\\text{context}} + T_{\\text{question}} + T_{\\text{answer}}\n",
        "$$\n",
        "\n",
        "So your context packing should keep $T_{\\text{context}}$ high-signal:\n",
        "\n",
        "- keep metadata small but stable (`chunk_id`, `doc_id`, `url`)\n",
        "- truncate chunk text if needed, but keep enough to preserve key sentences\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Assemble a `CONTEXT` block from ranked hits.\n",
        "- Keep ordering stable (rank order).\n",
        "- Enforce a max length (chars as a proxy for tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Structured context assembly with metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assemble_context(hits: list[dict], max_chars: int = 800, per_chunk_max_chars: int = 300) -> str:\n",
        "    parts: list[str] = [\"CONTEXT:\"]\n",
        "    total = len(parts[0])\n",
        "\n",
        "    for idx, h in enumerate(hits, start=1):\n",
        "        doc_id = h.get(\"doc_id\", \"\")\n",
        "        chunk_id = h.get(\"chunk_id\", \"\")\n",
        "        url = h.get(\"url\", \"\")\n",
        "        text = h.get(\"text\", \"\") or \"\"\n",
        "\n",
        "        if per_chunk_max_chars > 0 and len(text) > per_chunk_max_chars:\n",
        "            text = text[:per_chunk_max_chars] + \"...\"\n",
        "\n",
        "        meta = f\"doc_id={doc_id} chunk_id={chunk_id}\"\n",
        "        if url:\n",
        "            meta = meta + f\" url={url}\"\n",
        "\n",
        "        entry = f\"[{idx}] {meta} text=\\\"{text}\\\"\"\n",
        "        if total + 1 + len(entry) > max_chars:\n",
        "            break\n",
        "\n",
        "        parts.append(entry)\n",
        "        total += 1 + len(entry)\n",
        "\n",
        "    return \"\\n\".join(parts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Assemble a context block\n",
        "\n",
        "Provide a list of ranked hits and inspect the packed `CONTEXT` output.\n",
        "\n",
        "Verify:\n",
        "\n",
        "- `chunk_id` values are preserved (for citations)\n",
        "- order is stable (rank order)\n",
        "- output respects your `max_chars` budget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hits = [\n",
        "    {\"doc_id\": \"fastapi\", \"chunk_id\": \"fastapi#00001\", \"url\": \"https://fastapi.tiangolo.com/\", \"text\": \"FastAPI is a modern, fast web framework for building APIs with Python.\"},\n",
        "    {\"doc_id\": \"openapi\", \"chunk_id\": \"openapi#00002\", \"url\": \"https://spec.openapis.org/oas/latest.html\", \"text\": \"OpenAPI is a specification for describing REST APIs in a machine-readable format.\"},\n",
        "]\n",
        "\n",
        "print(assemble_context(hits, max_chars=260, per_chunk_max_chars=80))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Are `chunk_id` values preserved in the context?\n",
        "- Is ordering stable (rank order)?\n",
        "- Is the context length capped?\n",
        "- Do you log the final set of `chunk_id`s passed to the model (not just the retrieved set)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical usage: what to log for grounding failures\n",
        "\n",
        "To debug hallucinations and citation issues, log:\n",
        "\n",
        "- the search query\n",
        "- retrieved chunk ids + scores\n",
        "- the exact chunk ids passed into the prompt (after any truncation)\n",
        "- the exact prompt (or a hash + saved prompt artifact)\n",
        "\n",
        "Logging the final prompt (or an artifact you can reproduce) is often the fastest way to debug grounding failures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Record a prompt artifact shape\n",
        "\n",
        "Write a minimal function that produces a prompt string with:\n",
        "\n",
        "- short rules\n",
        "- a CONTEXT block\n",
        "- the question\n",
        "\n",
        "Then print it (or compute a hash) so you can compare prompt changes across runs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2da2cb8",
      "metadata": {},
      "source": [
        "# Exercise 2: Record a prompt artifact shape\n",
        "#\n",
        "# Goal: produce a prompt string with:\n",
        "# - short rules\n",
        "# - a CONTEXT block\n",
        "# - the question\n",
        "#\n",
        "# You can print it (or compute a hash) so you can compare prompt changes across runs.\n",
        "#\n",
        "# Note: the runnable reference implementation is provided in a later code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1483d0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_citations(hits: list[dict], snippet_chars: int = 200) -> list[dict]:\n",
        "    out: list[dict] = []\n",
        "    for h in hits:\n",
        "        text = h.get(\"text\", \"\") or \"\"\n",
        "        out.append(\n",
        "            {\n",
        "                \"doc_id\": h.get(\"doc_id\", \"\"),\n",
        "                \"chunk_id\": h.get(\"chunk_id\", \"\"),\n",
        "                \"snippet\": text[:snippet_chars],\n",
        "            }\n",
        "        )\n",
        "    return out\n",
        "\n",
        "\n",
        "print(format_citations(hits))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e77d23d",
      "metadata": {},
      "source": [
        "# (Moved) Citation formatting implementation is now in the code cell above.\n",
        "#\n",
        "# Keep citations mechanically checkable:\n",
        "# - preserve chunk_id exactly\n",
        "# - snippet should be copied from the chunk text (or a substring)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d17f81dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def decide_mode_minimal(hits: list[dict]) -> str:\n",
        "    return \"clarify\" if not hits else \"answer\"\n",
        "\n",
        "\n",
        "print(decide_mode_minimal(hits))\n",
        "print(decide_mode_minimal([]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b99f5e",
      "metadata": {},
      "source": [
        "# (Moved) Minimal mode decision implementation is now in the code cell above.\n",
        "#\n",
        "# Next step (Week 4 Part 03): replace this with a score-based threshold rule and calibrate the threshold on labeled in-KB vs out-of-KB questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55930590",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Student fill-in\n",
        "#\n",
        "# - Change max_chars and observe which chunks get dropped.\n",
        "# - Add a third hit with long text and confirm per-chunk truncation works.\n",
        "# - Decide which fields you want to include for citations (e.g., url) and update the context/citation shape accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "\n",
        "def build_prompt(question: str, context_block: str) -> str:\n",
        "    return (\n",
        "        \"You are a grounded assistant.\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"- Use only the CONTEXT.\\n\"\n",
        "        \"- If the CONTEXT is insufficient, respond with mode=clarify or mode=refuse.\\n\"\n",
        "        \"- For every factual claim, include a citation referencing one of the chunk_id values.\\n\\n\"\n",
        "        f\"{context_block}\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "context_block = assemble_context(hits, max_chars=400, per_chunk_max_chars=120)\n",
        "prompt = build_prompt(\"What is FastAPI?\", context_block)\n",
        "print(prompt)\n",
        "print(\"prompt_sha256:\", hashlib.sha256(prompt.encode(\"utf-8\")).hexdigest())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
