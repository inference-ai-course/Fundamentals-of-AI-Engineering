{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 4 - 03 Refusal and Clarification\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Decide mode based on retrieval signals\n",
        "- Use a deterministic threshold\n",
        "- Keep modes explicit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "Don’t rely on prompt wording to decide refusal.\n",
        "\n",
        "Implement a deterministic rule first, then use prompting to format the response.\n",
        "\n",
        "## Deterministic rules (examples)\n",
        "\n",
        "- If no chunks retrieved:\n",
        "  - ask a clarifying question OR refuse\n",
        "- If top score < threshold:\n",
        "  - ask a clarifying question\n",
        "- If chunks conflict:\n",
        "  - present both with citations OR ask clarification\n",
        "\n",
        "## Underlying theory: refusal is a decision under uncertainty\n",
        "\n",
        "Treat the top retrieval score $s$ as a signal of “is the KB likely relevant?”.\n",
        "\n",
        "A simple decision rule:\n",
        "\n",
        "$$\n",
        "\\text{mode} =\n",
        "\\begin{cases}\n",
        "\\text{answer} & s \\ge \\tau \\\\\n",
        "\\text{clarify/refuse} & s < \\tau\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where $\\tau$ is a threshold you choose.\n",
        "\n",
        "### False positives vs false negatives\n",
        "\n",
        "- If $\\tau$ is too low:\n",
        "  - you answer out-of-KB questions (hallucination risk)\n",
        "- If $\\tau$ is too high:\n",
        "  - you refuse in-KB questions (bad UX)\n",
        "\n",
        "So threshold choice is a product tradeoff, not a universal constant.\n",
        "\n",
        "### Why score thresholds are model- and metric-specific\n",
        "\n",
        "Scores are not comparable across:\n",
        "\n",
        "- different embedding models\n",
        "- different distance metrics (cosine vs dot vs L2)\n",
        "- different vector DB implementations\n",
        "- different chunking strategies\n",
        "\n",
        "So any threshold must be calibrated on your own data.\n",
        "\n",
        "## Recommended modes (make it explicit)\n",
        "\n",
        "- `answer`: you have enough evidence in retrieved context\n",
        "- `clarify`: you need the user to specify something to retrieve the right info\n",
        "- `refuse`: question is out-of-scope/out-of-KB or unsafe to answer without evidence\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Implement `decide_mode` based on hits and top score.\n",
        "- Calibrate a threshold with labeled in-KB vs out-of-KB questions.\n",
        "- Log `top_score` and `threshold` alongside the chosen mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Mode decision based on top score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decide_mode(hits: list[dict], threshold: float) -> str:\n",
        "    if not hits:\n",
        "        return 'clarify'\n",
        "    top_score = hits[0].get('score', 0.0)\n",
        "    return 'answer' if top_score >= threshold else 'refuse'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "1. Pick a threshold that matches your domain and embedding setup.\n",
        "2. Test behavior on three kinds of questions:\n",
        "\n",
        "- in-KB\n",
        "- ambiguous\n",
        "- out-of-KB\n",
        "\n",
        "Expected behavior table:\n",
        "\n",
        "- in-KB → `answer` with citations\n",
        "- ambiguous → `clarify`\n",
        "- out-of-KB → `refuse`\n",
        "\n",
        "Then record a small labeled set (5–10 items) and ensure your system behaves consistently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hits_good = [{'score': 0.82}]\n",
        "hits_bad = [{'score': 0.12}]\n",
        "\n",
        "# TODO: pick a threshold based on your data\n",
        "print(decide_mode(hits_good, threshold=0.5))\n",
        "print(decide_mode(hits_bad, threshold=0.5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86fa0c22",
      "metadata": {},
      "source": [
        "### Exercise: Threshold calibration from labeled scores\n",
        "\n",
        "Collect two small lists:\n",
        "\n",
        "- `in_kb_scores`: top scores for questions you *know* are answerable from the KB\n",
        "- `out_kb_scores`: top scores for questions you *know* are not in the KB\n",
        "\n",
        "Your goal is to choose a threshold that reduces out-of-KB answers while keeping in-KB refusals acceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63a6de6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_threshold(in_kb_scores: list[float], out_kb_scores: list[float], threshold: float) -> dict:\n",
        "    in_kb_refuse = sum(s < threshold for s in in_kb_scores)\n",
        "    out_kb_answer = sum(s >= threshold for s in out_kb_scores)\n",
        "    return {\n",
        "        \"threshold\": threshold,\n",
        "        \"in_kb_total\": len(in_kb_scores),\n",
        "        \"out_kb_total\": len(out_kb_scores),\n",
        "        \"in_kb_refuse\": in_kb_refuse,\n",
        "        \"out_kb_answer\": out_kb_answer,\n",
        "    }\n",
        "\n",
        "\n",
        "in_kb_scores = [0.83, 0.79, 0.74, 0.71, 0.68]\n",
        "out_kb_scores = [0.42, 0.31, 0.28, 0.19, 0.05]\n",
        "\n",
        "for t in [0.3, 0.5, 0.65, 0.7]:\n",
        "    print(evaluate_threshold(in_kb_scores, out_kb_scores, threshold=t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Is mode decision deterministic?\n",
        "- Do you log top_score and threshold?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
