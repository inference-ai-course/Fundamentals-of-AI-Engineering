{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 3 - 01 Retrieval as API\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Define /search contract\n",
        "- Use typed request validation\n",
        "- Keep response fields debuggable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "Retrieval should be a first-class endpoint.\n",
        "Typed models make failures consistent.\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Implement SearchRequest and SearchResponse.\n",
        "- Stub the search handler.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Minimal Pydantic models with constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Dict, List\n",
        "\n",
        "class SearchRequest(BaseModel):\n",
        "    query: str = Field(min_length=1)\n",
        "    top_k: int = Field(default=5, ge=1, le=50)\n",
        "    filters: Dict | None = None\n",
        "\n",
        "class SearchHit(BaseModel):\n",
        "    doc_id: str\n",
        "    chunk_id: str\n",
        "    score: float\n",
        "    text: str\n",
        "    metadata: Dict\n",
        "\n",
        "class SearchResponse(BaseModel):\n",
        "    query: str\n",
        "    hits: List[SearchHit]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Add a stub search handler that returns empty hits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_handler(req: SearchRequest) -> SearchResponse:\n",
        "    # TODO: replace with real vector DB query\n",
        "    return SearchResponse(query=req.query, hits=[])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Do request fields have validation?\n",
        "- Does response include chunk_id and metadata?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Underlying theory: retrieval is nearest-neighbor search with a contract\n",
        "\n",
        "### Retrieval as a function\n",
        "\n",
        "In a RAG system, retrieval can be modeled as a function:\n",
        "\n",
        "$$\n",
        "R(q; \\theta) \\rightarrow E\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $q$ is the user query text\n",
        "- $\\theta$ are retrieval parameters (embedding model, distance metric, index settings, filters, `top_k`)\n",
        "- $E$ is the evidence set (your returned chunk hits)\n",
        "\n",
        "Making `/search` a separate endpoint is a practical way to make $R$ observable and testable.\n",
        "\n",
        "### Nearest-neighbor definition (what the vector DB is doing)\n",
        "\n",
        "Let:\n",
        "\n",
        "- $f(\\cdot)$ be the embedding function mapping text to $\\mathbb{R}^d$\n",
        "- $s(\\cdot,\\cdot)$ be a similarity score (larger is more similar) or a distance $d(\\cdot,\\cdot)$ (smaller is closer)\n",
        "\n",
        "Compute:\n",
        "\n",
        "$$\n",
        "\\mathbf{q} = f(q), \\quad \\mathbf{x}_i = f(e_i)\n",
        "$$\n",
        "\n",
        "Then top-k retrieval returns the $k$ evidence items with best scores:\n",
        "\n",
        "$$\n",
        "\\mathrm{TopK}(q) = \\operatorname{argsort}_{i} \\; s(\\mathbf{q}, \\mathbf{x}_i) \\; [1:k]\n",
        "$$\n",
        "\n",
        "Or equivalently (for distances):\n",
        "\n",
        "$$\n",
        "\\mathrm{TopK}(q) = \\operatorname{argsort}_{i} \\; d(\\mathbf{q}, \\mathbf{x}_i) \\; [1:k]\n",
        "$$\n",
        "\n",
        "Key implication:\n",
        "\n",
        "- retrieval returns what is *numerically close* under your embedding + metric, not what is *factually correct*\n",
        "\n",
        "### Practical meaning\n",
        "\n",
        "Your goal in Week 3 is not to maximize quality; it is to make failures attributable:\n",
        "\n",
        "- if `/search` looks wrong, fix embedding/chunking/filters/top_k\n",
        "- if `/search` looks right but `/chat` is wrong, fix context assembly, prompting, or output validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical usage: what to log so retrieval is debuggable\n",
        "\n",
        "For every `/search` call, log at least:\n",
        "\n",
        "- query text (or a hash if sensitive)\n",
        "- `top_k`\n",
        "- filters\n",
        "- embedding model name\n",
        "- distance/similarity metric used by the store\n",
        "- returned hits: `rank`, `chunk_id`, `doc_id/source`, and `score`\n",
        "\n",
        "This is what lets you answer: “Is the system failing because retrieval is wrong, or because generation ignored good evidence?”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1: Deterministic retrieval stub\n",
        "#\n",
        "# Implement a deterministic stub that returns top_k hits with stable ids and descending scores.\n",
        "#\n",
        "# Goal:\n",
        "# - make the /search response shape testable without needing a real vector DB yet\n",
        "# - ensure repeated runs return identical results for the same input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f69f1a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "def retrieve_stub(query: str, top_k: int = 5) -> List[SearchHit]:\n",
        "    hits: List[SearchHit] = []\n",
        "    for i in range(top_k):\n",
        "        hits.append(\n",
        "            SearchHit(\n",
        "                doc_id=f\"doc-{i % 2}\",\n",
        "                chunk_id=f\"chunk-{i}\",\n",
        "                score=1.0 - (i * 0.1),\n",
        "                text=f\"Stub text for query={query} (rank={i+1})\",\n",
        "                metadata={\"source\": \"stub\"},\n",
        "            )\n",
        "        )\n",
        "    return hits\n",
        "\n",
        "\n",
        "req = SearchRequest(query=\"example query\", top_k=3)\n",
        "res = SearchResponse(query=req.query, hits=retrieve_stub(req.query, req.top_k))\n",
        "print(\"n_hits:\", len(res.hits))\n",
        "print(\"top_hit:\", res.hits[0].chunk_id if res.hits else None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae5c4c3",
      "metadata": {},
      "source": [
        "### Exercise 2: Minimal retrieval metrics\n",
        "\n",
        "Compute quick metrics that help you debug retrieval regressions without reading long outputs.\n",
        "\n",
        "At minimum, compute:\n",
        "\n",
        "- number of hits\n",
        "- min/max score\n",
        "\n",
        "Optional:\n",
        "\n",
        "- average score\n",
        "- list of returned `chunk_id`s in rank order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a594dc01",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "\n",
        "def retrieval_metrics(hits: list[SearchHit]) -> dict:\n",
        "    if not hits:\n",
        "        return {\"count\": 0, \"min_score\": None, \"max_score\": None, \"avg_score\": None, \"chunk_ids\": []}\n",
        "\n",
        "    scores = [h.score for h in hits]\n",
        "    return {\n",
        "        \"count\": len(hits),\n",
        "        \"min_score\": min(scores),\n",
        "        \"max_score\": max(scores),\n",
        "        \"avg_score\": sum(scores) / len(scores),\n",
        "        \"chunk_ids\": [h.chunk_id for h in hits],\n",
        "    }\n",
        "\n",
        "\n",
        "print(retrieval_metrics(res.hits))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "957514f0",
      "metadata": {},
      "source": [
        "### Exercise 3: Add a debug-print format\n",
        "\n",
        "Print each hit in a stable, human-debuggable way:\n",
        "\n",
        "- rank\n",
        "- score\n",
        "- `doc_id`\n",
        "- `chunk_id`\n",
        "- short text preview\n",
        "\n",
        "This is the same shape you’ll eventually want in logs (even if you later log JSON instead of printing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b46cc1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_hits(hits: list[SearchHit], preview_chars: int = 80) -> None:\n",
        "    for i, h in enumerate(hits):\n",
        "        text = h.text\n",
        "        preview = (text[:preview_chars] + \"...\") if len(text) > preview_chars else text\n",
        "        print(f\"#{i+1} score={h.score:.3f} doc_id={h.doc_id} chunk_id={h.chunk_id}\\n{preview}\\n\")\n",
        "\n",
        "\n",
        "print_hits(res.hits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f592845",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Is your `/search` output shape stable (same inputs => same outputs)?\n",
        "- Do you return enough fields to debug retrieval (`chunk_id`, `doc_id`, `score`, `metadata`, `text`)?\n",
        "- If `/chat` answers are wrong, can you use `/search` output to localize the failure (retrieval vs generation)?\n",
        "- Are you logging/printing the retrieval knobs (`top_k`, filters, embedding model, metric)?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
