{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 3 - 03 Retrieval Evaluation\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Define a small eval set\n",
        "- Compute hit rate and recall\n",
        "- Save misses for inspection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "A saved eval set makes retrieval changes measurable.\n",
        "Start with 10 to 20 queries.\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Define eval items with relevant_chunk_ids.\n",
        "- Compute hit rate and recall_at_k.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Minimal eval loop with misses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "\n",
        "def hit_at_k(relevant: set[str], retrieved: list[str], k: int) -> int:\n",
        "    if k <= 0:\n",
        "        raise ValueError(\"k must be > 0\")\n",
        "    topk = retrieved[:k]\n",
        "    return int(any(cid in relevant for cid in topk))\n",
        "\n",
        "\n",
        "def recall_at_k(relevant: set[str], retrieved: list[str], k: int) -> float:\n",
        "    if k <= 0:\n",
        "        raise ValueError(\"k must be > 0\")\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    topk = retrieved[:k]\n",
        "    hits = [cid for cid in topk if cid in relevant]\n",
        "    return len(hits) / len(relevant)\n",
        "\n",
        "\n",
        "def mrr(relevant: set[str], retrieved: list[str]) -> float:\n",
        "    # Reciprocal rank of the first relevant chunk (0.0 if none)\n",
        "    for i, cid in enumerate(retrieved, start=1):\n",
        "        if cid in relevant:\n",
        "            return 1.0 / i\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "# Minimal eval set: record which chunk_ids count as a \"correct\" retrieval for each query.\n",
        "# In a real system, you save this as JSONL so it is repeatable and diffable.\n",
        "eval_items: list[dict] = [\n",
        "    {\"id\": \"q_001\", \"query\": \"What is RAG?\", \"relevant_chunk_ids\": [\"rag_intro#02\", \"rag_overview#01\"]},\n",
        "    {\"id\": \"q_002\", \"query\": \"How do I start the API?\", \"relevant_chunk_ids\": [\"fastapi#001\", \"uvicorn#003\"]},\n",
        "    {\"id\": \"q_003\", \"query\": \"What is the weather in Tokyo tomorrow?\", \"relevant_chunk_ids\": []},\n",
        "]\n",
        "\n",
        "\n",
        "def run_search_stub(query: str, top_k: int) -> list[str]:\n",
        "    # Deterministic stub so repeated runs are stable.\n",
        "    # Replace this with a real call to /search or your vector DB client.\n",
        "    if \"rag\" in query.lower():\n",
        "        base = [\"rag_intro#02\", \"other#99\", \"rag_overview#01\", \"noise#00\"]\n",
        "    elif \"api\" in query.lower() or \"start\" in query.lower():\n",
        "        base = [\"noise#10\", \"uvicorn#003\", \"fastapi#001\", \"noise#11\"]\n",
        "    else:\n",
        "        base = [\"noise#20\", \"noise#21\", \"noise#22\", \"noise#23\"]\n",
        "\n",
        "    return base[:top_k]\n",
        "\n",
        "\n",
        "def evaluate(items: list[dict], k: int = 5) -> dict:\n",
        "    hit_scores: list[int] = []\n",
        "    recall_scores: list[float] = []\n",
        "    mrr_scores: list[float] = []\n",
        "    misses: list[dict] = []\n",
        "\n",
        "    for item in items:\n",
        "        query = item[\"query\"]\n",
        "        relevant = set(item.get(\"relevant_chunk_ids\", []))\n",
        "        retrieved = run_search_stub(query=query, top_k=k)\n",
        "\n",
        "        h = hit_at_k(relevant=relevant, retrieved=retrieved, k=k)\n",
        "        r = recall_at_k(relevant=relevant, retrieved=retrieved, k=k)\n",
        "        rr = mrr(relevant=relevant, retrieved=retrieved)\n",
        "\n",
        "        hit_scores.append(h)\n",
        "        recall_scores.append(r)\n",
        "        mrr_scores.append(rr)\n",
        "\n",
        "        if relevant and h == 0:\n",
        "            misses.append(\n",
        "                {\n",
        "                    \"id\": item.get(\"id\"),\n",
        "                    \"query\": query,\n",
        "                    \"relevant_chunk_ids\": sorted(list(relevant)),\n",
        "                    \"retrieved_chunk_ids\": retrieved,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    def mean(xs: list[float]) -> float:\n",
        "        return sum(xs) / max(len(xs), 1)\n",
        "\n",
        "    metrics = {\n",
        "        \"k\": k,\n",
        "        \"n_items\": len(items),\n",
        "        \"hit_rate\": mean([float(x) for x in hit_scores]),\n",
        "        \"avg_recall_at_k\": mean(recall_scores),\n",
        "        \"mrr\": mean(mrr_scores),\n",
        "        \"n_misses\": len(misses),\n",
        "    }\n",
        "\n",
        "    return {\"metrics\": metrics, \"misses\": misses}\n",
        "\n",
        "\n",
        "result = evaluate(eval_items, k=3)\n",
        "print(\"metrics:\", result[\"metrics\"])\n",
        "print(\"misses:\", result[\"misses\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Add more eval items and track misses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: add 5-10 eval items with relevant_chunk_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Is the eval set saved and repeatable?\n",
        "- Do you keep misses for debugging?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
