{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 2 - Week 3 - 03 Retrieval Evaluation\n",
        "\n",
        "**Estimated time:** 60-90 minutes\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Define a small eval set\n",
        "- Compute hit rate and recall\n",
        "- Save misses for inspection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "A saved eval set makes retrieval changes measurable.\n",
        "Start with 10 to 20 queries.\n",
        "\n",
        "## Practice Steps\n",
        "\n",
        "- Define eval items with relevant_chunk_ids.\n",
        "- Compute hit rate and recall_at_k.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample code\n",
        "\n",
        "Minimal eval loop with misses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def recall_at_k(retrieved: list[str], relevant: set[str]) -> float:\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    return 1.0 if any(r in relevant for r in retrieved) else 0.0\n",
        "\n",
        "items = [\n",
        "    {'query': 'What is RAG?', 'relevant_chunk_ids': ['rag_intro#02']},\n",
        "]\n",
        "\n",
        "# TODO: replace with real search\n",
        "\n",
        "def run_search(query: str, top_k: int) -> list[str]:\n",
        "    return []\n",
        "\n",
        "scores = []\n",
        "misses = []\n",
        "for item in items:\n",
        "    retrieved = run_search(item['query'], top_k=5)\n",
        "    relevant = set(item['relevant_chunk_ids'])\n",
        "    s = recall_at_k(retrieved, relevant)\n",
        "    scores.append(s)\n",
        "    if s == 0.0:\n",
        "        misses.append({'query': item['query'], 'retrieved': retrieved, 'relevant': list(relevant)})\n",
        "\n",
        "print('recall_at_k', sum(scores) / max(len(scores), 1))\n",
        "print('misses', misses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Student fill-in\n",
        "\n",
        "Add more eval items and track misses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# TODO: add 5-10 eval items with relevant_chunk_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-check\n",
        "\n",
        "- Is the eval set saved and repeatable?\n",
        "- Do you keep misses for debugging?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}