{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 — Part 04: End-to-end capstone runner (one command)\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## What success looks like (end of Part 04)\n",
    "\n",
    "- You can describe a stable CLI interface for the capstone runner.\n",
    "- Running the runner produces `output/report.json` and `output/report.md` deterministically.\n",
    "- When something fails, you still have intermediate artifacts in `output/` to debug.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After reading/running the skeleton, you should be able to point to:\n",
    "\n",
    "- the CLI flags (`--input`, `--output_dir`, `--model`)\n",
    "- the output contract (`report.json`, `report.md`)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Design a stable CLI interface for the capstone\n",
    "- Define a clear output contract (report + intermediate artifacts)\n",
    "- Build a runner skeleton with argparse\n",
    "- Capture failure evidence for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5bac3",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook defines the **end-to-end capstone runner** — a single command that orchestrates all pipeline stages from CSV input to final report.\n",
    "\n",
    "**The goal:** `python run_capstone.py --input data.csv --output_dir output --model llama3.1`\n",
    "\n",
    "One command. Predictable outputs. Debuggable failures.\n",
    "\n",
    "**Why a runner matters:** Without a runner, you have to manually execute each notebook in order, passing outputs between them. A runner makes the pipeline reproducible, testable, and demo-ready — anyone can clone your repo and run it without asking you questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273eb263",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Your capstone should run with **one command**. That means:\n",
    "\n",
    "- clear CLI flags\n",
    "- predictable outputs\n",
    "- stable artifact locations\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: the runner is your system’s public interface\n",
    "\n",
    "From Week 1, reproducibility is an interface. The runner is the concrete version of that idea:\n",
    "\n",
    "$$\n",
    "\\text{outputs} = r(\\text{input},\\ \\text{config})\n",
    "$$\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- if the runner is stable, testing and demos become easy\n",
    "- if the runner requires manual steps, failures become non-reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fdeaa",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `run_capstone()` — the main pipeline function — and `build_parser()` — the CLI argument parser.\n",
    "\n",
    "**Walk through `run_capstone()`:**\n",
    "1. Create `output_dir` (with `mkdir(parents=True, exist_ok=True)` — safe to call even if it exists)\n",
    "2. TODO: implement the 5 pipeline stages (load → profile → compress → llm → report)\n",
    "3. Write `report.json` and `report.md` — the two required output artifacts\n",
    "\n",
    "**Walk through `build_parser()`:**\n",
    "- `--input` (required) — the CSV file to analyze\n",
    "- `--output_dir` (default: `\"output\"`) — where to write all artifacts\n",
    "- `--model` (required) — which LLM model to use\n",
    "\n",
    "**Your task:** Replace the `TODO` comment in `run_capstone()` with real stage implementations from Parts 01–03. Each stage should save an intermediate artifact before calling the next stage — so if the LLM call fails, you still have `profile.json` and `compressed_input.json` for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccc270",
   "metadata": {},
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def run_capstone(input_path: Path, output_dir: Path, model: str) -> Dict[str, Any]:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # TODO: implement pipeline stages (load -> profile -> compress -> llm -> report)\n",
    "    report: Dict[str, Any] = {\n",
    "        \"model\": model,\n",
    "        \"input\": str(input_path),\n",
    "        \"summary\": \"placeholder\",\n",
    "    }\n",
    "\n",
    "    (output_dir / \"report.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "    (output_dir / \"report.md\").write_text(\"# Report\\n\\nPlaceholder report\", encoding=\"utf-8\")\n",
    "    return report\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input\", required=True)\n",
    "    parser.add_argument(\"--output_dir\", default=\"output\")\n",
    "    parser.add_argument(\"--model\", required=True)\n",
    "    return parser\n",
    "\n",
    "\n",
    "# Example CLI usage:\n",
    "# python run_capstone.py --input data.csv --output_dir output --model llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d596168",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `validate_outputs()` — a post-run check that verifies the required output files actually exist.\n",
    "\n",
    "**Why validate outputs explicitly?** A pipeline can \"succeed\" (no exceptions raised) but still produce incomplete outputs if a stage silently skips writing a file. This validator catches that case and gives you a clear error: `\"missing outputs: [output/report.json]\"`.\n",
    "\n",
    "**Your task:** Extend `validate_outputs()` with schema checks — not just \"file exists\" but \"file contains valid JSON with the expected keys.\" For example, `report.json` should have at least `model` and `summary` fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested CLI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "python run_capstone.py --input data.csv --output_dir output --model <MODEL_NAME>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Output contract\n",
    "\n",
    "The command should write:\n",
    "\n",
    "- `output/report.json`\n",
    "- `output/report.md`\n",
    "\n",
    "Optionally:\n",
    "\n",
    "- `output/profile.json`\n",
    "- `output/compressed_input.json`\n",
    "\n",
    "Failure-mode design tip:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def validate_outputs(output_dir: Path) -> None:\n",
    "    required = [output_dir / \"report.json\", output_dir / \"report.md\"]\n",
    "    missing = [p for p in required if not p.exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"missing outputs: {missing}\")\n",
    "\n",
    "\n",
    "print(\"Implement validate_outputs() with extra checks if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe2ad7",
   "metadata": {},
   "source": [
    "## Self-check\n",
    "\n",
    "- Can you run from a fresh folder after following README steps?\n",
    "- If the model call fails, do you still get intermediate outputs?\n",
    "\n",
    "## References\n",
    "\n",
    "- Python `argparse`: https://docs.python.org/3/library/argparse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b7b6a",
   "metadata": {},
   "source": [
    "## Exercise: Required-columns guard\n",
    "\n",
    "Add a required-columns guard before running the pipeline.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Implement `assert_required_columns_todo(df, required)`.\n",
    "- Save the check result under `output/required_columns.json`.\n",
    "\n",
    "Checkpoint:\n",
    "\n",
    "- Calling the function with a missing column raises a clear `ValueError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8439d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'age': [23, None, 31, 45, 29, 35, None, 41],\n",
    "    'country': ['US', 'US', 'SG', None, 'CN', 'US', 'SG', 'CN'],\n",
    "    'purchase_amount': [12.5, 0.0, 7.99, 103.2, None, 5.0, 1000.0, 8.2],\n",
    "})\n",
    "\n",
    "\n",
    "def assert_required_columns_todo(df: pd.DataFrame, required: List[str]) -> None:\n",
    "    # TODO: implement\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\"missing required columns: %s\" % missing)\n",
    "\n",
    "\n",
    "required_cols = [\"user_id\", \"age\", \"country\", \"purchase_amount\"]\n",
    "assert_required_columns_todo(df, required_cols)\n",
    "\n",
    "(OUTPUT_DIR / \"required_columns.json\").write_text(\n",
    "    json.dumps({\"required\": required_cols, \"present\": list(df.columns)}, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "print(\"wrote:\", OUTPUT_DIR / \"required_columns.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6485b40",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `assert_required_columns_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62329ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_required_columns_todo(df: pd.DataFrame, required: List[str]) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\"missing required columns: %s\" % missing)\n",
    "\n",
    "\n",
    "try:\n",
    "    assert_required_columns_todo(df.drop(columns=[\"country\"]), required_cols)\n",
    "except ValueError as e:\n",
    "    (OUTPUT_DIR / \"required_columns_failure.json\").write_text(\n",
    "        json.dumps({\"error\": str(e)}, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    print(\"wrote:\", OUTPUT_DIR / \"required_columns_failure.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
