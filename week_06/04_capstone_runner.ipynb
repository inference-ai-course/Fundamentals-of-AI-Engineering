{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 \u2014 Part 04: End-to-end capstone runner (one command)\n",
    "\n",
    "**Estimated time:** 60\u201390 minutes\n",
    "\n",
    "## What success looks like (end of Part 04)\n",
    "\n",
    "- You can describe a stable CLI interface for the capstone runner.\n",
    "- Running the runner produces `output/report.json` and `output/report.md` deterministically.\n",
    "- When something fails, you still have intermediate artifacts in `output/` to debug.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After reading/running the skeleton, you should be able to point to:\n",
    "\n",
    "- the CLI flags (`--input`, `--output_dir`, `--model`)\n",
    "- the output contract (`report.json`, `report.md`)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Design a stable CLI interface for the capstone\n",
    "- Define a clear output contract (report + intermediate artifacts)\n",
    "- Build a runner skeleton with argparse\n",
    "- Capture failure evidence for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5bac3",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook defines the **end-to-end capstone runner** \u2014 a single command that orchestrates all pipeline stages from CSV input to final report.\n",
    "\n",
    "**The goal:** `python run_capstone.py --input data.csv --output_dir output --model llama3.1`\n",
    "\n",
    "One command. Predictable outputs. Debuggable failures.\n",
    "\n",
    "**Why a runner matters:** Without a runner, you have to manually execute each notebook in order, passing outputs between them. A runner makes the pipeline reproducible, testable, and demo-ready \u2014 anyone can clone your repo and run it without asking you questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273eb263",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Your capstone should run with **one command**. That means:\n",
    "\n",
    "- clear CLI flags\n",
    "- predictable outputs\n",
    "- stable artifact locations\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: the runner is your system\u2019s public interface\n",
    "\n",
    "From Week 1, reproducibility is an interface. The runner is the concrete version of that idea:\n",
    "\n",
    "$$\n",
    "\\text{outputs} = r(\\text{input},\\ \\text{config})\n",
    "$$\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- if the runner is stable, testing and demos become easy\n",
    "- if the runner requires manual steps, failures become non-reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fdeaa",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `run_capstone()` \u2014 the main pipeline function \u2014 and `build_parser()` \u2014 the CLI argument parser.\n",
    "\n",
    "**Walk through `run_capstone()`:**\n",
    "1. Create `output_dir` (with `mkdir(parents=True, exist_ok=True)` \u2014 safe to call even if it exists)\n",
    "2. TODO: implement the 5 pipeline stages (load \u2192 profile \u2192 compress \u2192 llm \u2192 report)\n",
    "3. Write `report.json` and `report.md` \u2014 the two required output artifacts\n",
    "\n",
    "**Walk through `build_parser()`:**\n",
    "- `--input` (required) \u2014 the CSV file to analyze\n",
    "- `--output_dir` (default: `\"output\"`) \u2014 where to write all artifacts\n",
    "- `--model` (required) \u2014 which LLM model to use\n",
    "\n",
    "**Your task:** Replace the `TODO` comment in `run_capstone()` with real stage implementations from Parts 01\u201303. Each stage should save an intermediate artifact before calling the next stage \u2014 so if the LLM call fails, you still have `profile.json` and `compressed_input.json` for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "f5ccc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def run_capstone(input_path: Path, output_dir: Path, model: str) -> Dict[str, Any]:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # TODO: implement pipeline stages (load -> profile -> compress -> llm -> report)\n",
    "    report: Dict[str, Any] = {\n",
    "        \"model\": model,\n",
    "        \"input\": str(input_path),\n",
    "        \"summary\": \"placeholder\",\n",
    "    }\n",
    "\n",
    "    (output_dir / \"report.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "    (output_dir / \"report.md\").write_text(\"# Report\\n\\nPlaceholder report\", encoding=\"utf-8\")\n",
    "    return report\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input\", required=True)\n",
    "    parser.add_argument(\"--output_dir\", default=\"output\")\n",
    "    parser.add_argument(\"--model\", required=True)\n",
    "    return parser\n",
    "\n",
    "\n",
    "# Example CLI usage:\n",
    "# python run_capstone.py --input data.csv --output_dir output --model llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d596168",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `validate_outputs()` \u2014 a post-run check that verifies the required output files actually exist.\n",
    "\n",
    "**Why validate outputs explicitly?** A pipeline can \"succeed\" (no exceptions raised) but still produce incomplete outputs if a stage silently skips writing a file. This validator catches that case and gives you a clear error: `\"missing outputs: [output/report.json]\"`.\n",
    "\n",
    "**Your task:** Extend `validate_outputs()` with schema checks \u2014 not just \"file exists\" but \"file contains valid JSON with the expected keys.\" For example, `report.json` should have at least `model` and `summary` fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested CLI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python run_capstone.py --input data.csv --output_dir output --model <MODEL_NAME>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Output contract\n",
    "\n",
    "The command should write:\n",
    "\n",
    "- `output/report.json`\n",
    "- `output/report.md`\n",
    "\n",
    "Optionally:\n",
    "\n",
    "- `output/profile.json`\n",
    "- `output/compressed_input.json`\n",
    "\n",
    "Failure-mode design tip:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def validate_outputs(output_dir: Path) -> None:\n",
    "    required = [output_dir / \"report.json\", output_dir / \"report.md\"]\n",
    "    missing = [p for p in required if not p.exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"missing outputs: {missing}\")\n",
    "\n",
    "\n",
    "print(\"Implement validate_outputs() with extra checks if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe2ad7",
   "metadata": {},
   "source": [
    "## Self-check\n",
    "\n",
    "- Can you run from a fresh folder after following README steps?\n",
    "- If the model call fails, do you still get intermediate outputs?\n",
    "\n",
    "## References\n",
    "\n",
    "- Python `argparse`: https://docs.python.org/3/library/argparse.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}