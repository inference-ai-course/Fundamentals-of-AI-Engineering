{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 — Part 04: End-to-end capstone runner (one command)\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## What success looks like (end of Part 04)\n",
    "\n",
    "- You can describe a stable CLI interface for the capstone runner.\n",
    "- Running the runner produces `output/report.json` and `output/report.md` deterministically.\n",
    "- When something fails, you still have intermediate artifacts in `output/` to debug.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After reading/running the skeleton, you should be able to point to:\n",
    "\n",
    "- the CLI flags (`--input`, `--output_dir`, `--model`)\n",
    "- the output contract (`report.json`, `report.md`)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Design a stable CLI interface for the capstone\n",
    "- Define a clear output contract (report + intermediate artifacts)\n",
    "- Build a runner skeleton with argparse\n",
    "- Capture failure evidence for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273eb263",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Your capstone should run with **one command**. That means:\n",
    "\n",
    "- clear CLI flags\n",
    "- predictable outputs\n",
    "- stable artifact locations\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: the runner is your system’s public interface\n",
    "\n",
    "From Week 1, reproducibility is an interface. The runner is the concrete version of that idea:\n",
    "\n",
    "$$\n",
    "\\text{outputs} = r(\\text{input},\\ \\text{config})\n",
    "$$\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- if the runner is stable, testing and demos become easy\n",
    "- if the runner requires manual steps, failures become non-reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccc270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def run_capstone(input_path: Path, output_dir: Path, model: str) -> Dict[str, Any]:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # TODO: implement pipeline stages (load -> profile -> compress -> llm -> report)\n",
    "    report: Dict[str, Any] = {\n",
    "        \"model\": model,\n",
    "        \"input\": str(input_path),\n",
    "        \"summary\": \"placeholder\",\n",
    "    }\n",
    "\n",
    "    (output_dir / \"report.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "    (output_dir / \"report.md\").write_text(\"# Report\\n\\nPlaceholder report\", encoding=\"utf-8\")\n",
    "    return report\n",
    "\n",
    "\n",
    "def build_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input\", required=True)\n",
    "    parser.add_argument(\"--output_dir\", default=\"output\")\n",
    "    parser.add_argument(\"--model\", required=True)\n",
    "    return parser\n",
    "\n",
    "\n",
    "# Example CLI usage:\n",
    "# python run_capstone.py --input data.csv --output_dir output --model llama3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6934c1",
   "metadata": {},
   "source": [
    "## Suggested CLI\n",
    "\n",
    "```bash\n",
    "python run_capstone.py --input data.csv --output_dir output --model <MODEL_NAME>\n",
    "```\n",
    "\n",
    "## Output contract\n",
    "\n",
    "The command should write:\n",
    "\n",
    "- `output/report.json`\n",
    "- `output/report.md`\n",
    "\n",
    "Optionally:\n",
    "\n",
    "- `output/profile.json`\n",
    "- `output/compressed_input.json`\n",
    "\n",
    "Failure-mode design tip:\n",
    "\n",
    "- write intermediate artifacts *before* the LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_outputs(output_dir: Path) -> None:\n",
    "    required = [output_dir / \"report.json\", output_dir / \"report.md\"]\n",
    "    missing = [p for p in required if not p.exists()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"missing outputs: {missing}\")\n",
    "\n",
    "\n",
    "print(\"Implement validate_outputs() with extra checks if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe2ad7",
   "metadata": {},
   "source": [
    "## Self-check\n",
    "\n",
    "- Can you run from a fresh folder after following README steps?\n",
    "- If the model call fails, do you still get intermediate outputs?\n",
    "\n",
    "## References\n",
    "\n",
    "- Python `argparse`: https://docs.python.org/3/library/argparse.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
