{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 \u2014 Part 03: Chunking long text + synthesizing summaries\n",
    "\n",
    "**Estimated time:** 60\u2013100 minutes\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You can split long text into overlapping chunks deterministically.\n",
    "- You can produce a small per-chunk schema and synthesize a final summary.\n",
    "- You can show the number of chunks created and inspect at least one chunk.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should see:\n",
    "\n",
    "- `chunks=...` printed\n",
    "- a final synthesized summary dict printed\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement a simple chunking utility for long text\n",
    "- Define a per-chunk summary schema\n",
    "- Synthesize chunk outputs into a final summary\n",
    "- Understand chunk-boundary pitfalls and overlap strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42b94f",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook covers **chunking** \u2014 splitting long text into overlapping pieces so each piece fits within the LLM's context window \u2014 and **synthesis** \u2014 combining per-chunk summaries into a final result.\n",
    "\n",
    "**When do you need chunking?** When your text is longer than the context window allows. For example, a 50-page document at ~500 tokens/page = 25,000 tokens \u2014 too large for many models.\n",
    "\n",
    "**The chunking trade-off:**\n",
    "- Too large chunks \u2192 may exceed context window\n",
    "- Too small chunks \u2192 lose cross-sentence context, more API calls\n",
    "- Overlap between chunks \u2192 prevents losing information at boundaries (a sentence split across two chunks is captured by both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548b687",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "When text is too long for the context window:\n",
    "\n",
    "1. **Split into chunks**\n",
    "2. **Process each chunk** into a small structured summary\n",
    "3. **Synthesize** a final summary from per-chunk outputs\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: chunking is a strategy for bounded-context reasoning\n",
    "\n",
    "If the document is longer than what you can send at once, you have two options:\n",
    "\n",
    "- **lossy compression**: summarize first (risk losing details)\n",
    "- **chunking**: split and process pieces (risk missing cross-chunk dependencies)\n",
    "\n",
    "Chunking introduces a boundary effect: information near chunk boundaries can be separated.\n",
    "\n",
    "Practical implication: if correctness depends on cross-paragraph links, you may need overlap or a second pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9593f",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines three functions and runs a demo:\n",
    "\n",
    "- **`chunk_text_simple()`** \u2014 splits text into non-overlapping chunks of `chunk_size` characters\n",
    "- **`chunk_text_overlap()`** \u2014 splits with overlap: each chunk starts `(chunk_size - overlap)` characters after the previous one. This means the last `overlap` characters of chunk N are also the first `overlap` characters of chunk N+1.\n",
    "- **`summarize_chunk_stub()`** \u2014 a placeholder that returns a minimal summary dict (replace with a real LLM call)\n",
    "- **`synthesize_summaries()`** \u2014 combines per-chunk summaries into a final result by merging bullet lists\n",
    "\n",
    "**What to notice in the demo:** With `chunk_size=1800` and `overlap=150`, a 4800-character text produces 3 chunks. The overlap ensures no information is lost at chunk boundaries \u2014 a sentence that spans the boundary appears in both adjacent chunks."
   ]
  },
  {
   "cell_type": "code",
   "id": "3d01acfe",
   "metadata": {},
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "def chunk_text_simple(text: str, *, chunk_size: int = 1800) -> List[str]:\n",
    "    return [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "\n",
    "def chunk_text_overlap(text: str, *, chunk_size: int = 1800, overlap: int = 150) -> List[str]:\n",
    "    chunks: List[str] = []\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    for i in range(0, len(text), step):\n",
    "        chunks.append(text[i : i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def summarize_chunk_stub(chunk: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"summary_bullets\": [chunk[:40] + \"...\"] if chunk else [],\n",
    "        \"key_entities\": [\"<todo>\"] if chunk else [],\n",
    "        \"open_questions\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "def synthesize_summaries(summaries: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    bullets = [b for s in summaries for b in s.get(\"summary_bullets\", [])]\n",
    "    return {\"summary\": bullets[:5], \"note\": \"synthesized from chunk summaries\"}\n",
    "\n",
    "\n",
    "text = \"B\" * 4800\n",
    "chunks = chunk_text_overlap(text, chunk_size=1800, overlap=150)\n",
    "per_chunk = [summarize_chunk_stub(c) for c in chunks]\n",
    "final_summary = synthesize_summaries(per_chunk)\n",
    "print(\"chunks=\", len(chunks))\n",
    "print(final_summary)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b2eae396",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `chunk_text_todo()` and `make_chunk_prompt()` \u2014 two functions for you to implement.\n",
    "\n",
    "**`chunk_text_todo()`** should implement chunking with overlap. The key formula: `step = chunk_size - overlap`. Each chunk starts at index `i`, `i+step`, `i+2*step`, etc. Each chunk covers `text[i : i+chunk_size]`.\n",
    "\n",
    "**`make_chunk_prompt()`** should build a prompt that asks the LLM to summarize one chunk into a structured JSON with keys: `summary_bullets`, `key_entities`, `open_questions`. Keeping the schema small and consistent makes synthesis easier \u2014 you can merge lists across chunks without complex logic.\n",
    "\n",
    "**Your task:** Implement both functions. The solution is in the Appendix if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f12d9",
   "metadata": {},
   "source": [
    "## Synthesis pattern (high-level)\n",
    "\n",
    "- For each chunk: ask the model for a short structured summary.\n",
    "- After all chunks: ask the model to combine those summaries.\n",
    "\n",
    "This is a simple form of hierarchical summarization:\n",
    "\n",
    "- level 1: per-chunk summaries\n",
    "- level 2: global synthesis\n",
    "\n",
    "Practical implication: enforcing a small per-chunk schema helps reduce drift and makes synthesis more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def chunk_text_todo(text: str, chunk_size: int = 1500, overlap: int = 200) -> List[str]:\n",
    "    # TODO: implement chunking with overlap.\n",
    "    if chunk_size <= 0:\n",
    "        return []\n",
    "    if overlap < 0:\n",
    "        overlap = 0\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    return [text[i : i + chunk_size] for i in range(0, len(text), step)]\n",
    "\n",
    "\n",
    "def make_chunk_prompt(chunk: str) -> str:\n",
    "    # TODO: create a prompt template for per-chunk summaries.\n",
    "    return \"\".join(\n",
    "        [\n",
    "            \"You are summarizing one chunk of a longer document.\\n\",\n",
    "            \"Return JSON with keys: summary_bullets (list of short strings), key_entities (list), open_questions (list).\\n\",\n",
    "            \"Chunk:\\n\",\n",
    "            chunk,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "_demo_chunks = chunk_text_todo(\"ABC\" * 1200, chunk_size=1500, overlap=200)\n",
    "print(\"demo_chunks=\", len(_demo_chunks))\n",
    "print(make_chunk_prompt(_demo_chunks[0])[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae57cfd",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for the TODO functions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01823a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_todo(text: str, chunk_size: int = 1500, overlap: int = 200) -> List[str]:\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size must be > 0\")\n",
    "    if overlap >= chunk_size:\n",
    "        raise ValueError(\"overlap must be < chunk_size\")\n",
    "\n",
    "    step = chunk_size - overlap\n",
    "    return [text[i : i + chunk_size] for i in range(0, len(text), step)]\n",
    "\n",
    "\n",
    "def make_chunk_prompt(chunk: str) -> str:\n",
    "    return (\n",
    "        \"You are summarizing one chunk of a longer document.\\n\"\n",
    "        \"Return STRICT JSON with keys:\\n\"\n",
    "        \"- summary_bullets: list of short strings (max 3)\\n\"\n",
    "        \"- key_entities: list of strings\\n\"\n",
    "        \"- open_questions: list of strings\\n\\n\"\n",
    "        \"Chunk:\\n\"\n",
    "        + chunk\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"solution_prompt_preview=\", make_chunk_prompt(\"hello\")[:120] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}