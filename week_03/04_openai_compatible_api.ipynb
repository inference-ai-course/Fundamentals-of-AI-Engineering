{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 — Part 04: OpenAI Compatible API Lab\n",
    "\n",
    "**Estimated time:** 45–60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 04)\n",
    "\n",
    "- You can explain what OpenAI Compatible API means.\n",
    "- You can configure the OpenAI SDK for multiple providers.\n",
    "- You can make API calls to different backends with the same code.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the OpenAI API compatibility standard\n",
    "- Configure clients for multiple providers\n",
    "- Switch providers by changing only base_url and api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we begin, we need to:\n",
    "\n",
    "1. **Import the OpenAI SDK** - This is the official Python client library that works with any OpenAI-compatible provider\n",
    "2. **Configure our providers** - Each provider needs:\n",
    "   - `base_url`: The API endpoint URL\n",
    "   - `api_key`: Your authentication key\n",
    "   - `default_model`: The model to use by default\n",
    "\n",
    "The key insight is that **all OpenAI-compatible providers use the same SDK**. You just change the `base_url` and `api_key` to switch between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Provider configurations - FREE providers (no credit card required)\n",
    "# Get your API keys:\n",
    "# - Groq: https://console.groq.com/keys\n",
    "# - OpenRouter: https://openrouter.ai/keys\n",
    "\n",
    "PROVIDERS = {\n",
    "    # Groq - Ultra-fast inference, generous free tier\n",
    "    # Docs: https://console.groq.com/docs/openai\n",
    "    # Base URL: The API endpoint that accepts OpenAI-format requests\n",
    "    # API Key: Your personal key from console.groq.com/keys\n",
    "    \"groq\": {\n",
    "        \"base_url\": \"https://api.groq.com/openai/v1\",\n",
    "        \"api_key\": os.environ.get(\"GROQ_API_KEY\", \"your-groq-api-key\"),\n",
    "        \"default_model\": \"llama-3.3-70b-versatile\"  # 70B parameter model\n",
    "    },\n",
    "    \n",
    "    # OpenRouter - 100+ models, free tier available\n",
    "    # Docs: https://openrouter.ai/docs/quickstart\n",
    "    # Free models have :free suffix in their name\n",
    "    \"openrouter\": {\n",
    "        \"base_url\": \"https://openrouter.ai/api/v1\",\n",
    "        \"api_key\": os.environ.get(\"OPENROUTER_API_KEY\", \"your-openrouter-api-key\"),\n",
    "        \"default_model\": \"meta-llama/llama-3.3-70b-instruct:free\"\n",
    "    },\n",
    "    \n",
    "    # Ollama (local) - requires Ollama running on localhost\n",
    "    # See self_learn documentation for setup\n",
    "    # \"ollama\": {\n",
    "    #     \"base_url\": \"http://localhost:11434/v1\",\n",
    "    #     \"api_key\": \"ollama\",  # Any value works - local only\n",
    "    #     \"default_model\": \"llama3.2\"\n",
    "    # },\n",
    "}\n",
    "\n",
    "print(f\"Configured providers: {list(PROVIDERS.keys())}\")\n",
    "print(\"\\nTo get API keys:\")\n",
    "print(\"  Groq: https://console.groq.com/keys\")\n",
    "print(\"  OpenRouter: https://openrouter.ai/keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: List Available Models\n",
    "\n",
    "The `/v1/models` endpoint returns a list of all models available from a provider. This is useful for:\n",
    "\n",
    "- **Discovering what models are available** before making requests\n",
    "- **Checking model names** (they vary between providers)\n",
    "- **Verifying API connectivity** - if this works, your credentials are valid\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. Create an `OpenAI` client with the provider's `base_url` and `api_key`\n",
    "2. Call `client.models.list()` - this hits the `/v1/models` endpoint\n",
    "3. Extract model IDs from the response\n",
    "\n",
    "The response format is standardized across all OpenAI-compatible providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(provider_name: str) -> list:\n",
    "    \"\"\"\n",
    "    List available models from a provider.\n",
    "    \n",
    "    This function demonstrates the provider-agnostic nature of the OpenAI SDK.\n",
    "    The same code works for Groq, OpenRouter, Ollama, or any OpenAI-compatible API.\n",
    "    \n",
    "    Args:\n",
    "        provider_name: Key from the PROVIDERS dict (e.g., \"groq\", \"openrouter\")\n",
    "    \n",
    "    Returns:\n",
    "        List of model ID strings\n",
    "    \"\"\"\n",
    "    # Step 1: Get the configuration for this provider\n",
    "    config = PROVIDERS.get(provider_name)\n",
    "    if not config:\n",
    "        print(f\"Unknown provider: {provider_name}\")\n",
    "        return []\n",
    "    \n",
    "    # Step 2: Create an OpenAI client configured for this provider\n",
    "    # The magic happens here - same OpenAI class, different base_url\n",
    "    client = OpenAI(\n",
    "        base_url=config[\"base_url\"],\n",
    "        api_key=config[\"api_key\"]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Call the models.list() endpoint\n",
    "    # This sends GET request to {base_url}/models\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        # Extract just the model IDs from the response objects\n",
    "        return [m.id for m in models.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models for {provider_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# List models from each configured provider\n",
    "# This loop tests connectivity to all providers at once\n",
    "for provider in PROVIDERS:\n",
    "    print(f\"\\n=== {provider.upper()} ===\")\n",
    "    models = list_models(provider)\n",
    "    if models:\n",
    "        print(f\"Found {len(models)} models\")\n",
    "        for m in models[:5]:\n",
    "            print(f\"  - {m}\")\n",
    "        if len(models) > 5:\n",
    "            print(f\"  ... and {len(models) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Make a Chat Completion\n",
    "\n",
    "The `/v1/chat/completions` endpoint is the core of the OpenAI API. This is what you'll use 99% of the time.\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Purpose |\n",
    "|-----------|---------|\n",
    "| `model` | Which model to use (e.g., \"llama-3.3-70b-versatile\") |\n",
    "| `messages` | Conversation history as a list of role/content pairs |\n",
    "| `max_tokens` | Maximum length of the response |\n",
    "| `temperature` | Randomness (0 = deterministic, 1 = creative) |\n",
    "\n",
    "### Message Roles:\n",
    "\n",
    "- `system`: Instructions for the assistant's behavior\n",
    "- `user`: Input from the user\n",
    "- `assistant`: Previous responses (for multi-turn conversations)\n",
    "\n",
    "### The Point of This Exercise:\n",
    "\n",
    "Notice that **the same function works with any provider**. We just pass different configuration. This is the power of the OpenAI-compatible standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat(provider_name: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Make a simple chat completion request.\n",
    "    \n",
    "    This function demonstrates the minimal code needed to get a response\n",
    "    from any OpenAI-compatible provider.\n",
    "    \n",
    "    Args:\n",
    "        provider_name: Key from PROVIDERS dict\n",
    "        prompt: The user's question/message\n",
    "    \n",
    "    Returns:\n",
    "        The assistant's response text (or error message)\n",
    "    \"\"\"\n",
    "    # Get provider configuration\n",
    "    config = PROVIDERS[provider_name]\n",
    "    \n",
    "    # Create client for this provider\n",
    "    # Same OpenAI class works for all providers!\n",
    "    client = OpenAI(\n",
    "        base_url=config[\"base_url\"],\n",
    "        api_key=config[\"api_key\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Make the chat completion request\n",
    "        # The messages array builds the conversation context:\n",
    "        # - system: Sets the assistant's personality/behavior\n",
    "        # - user: The actual question\n",
    "        response = client.chat.completions.create(\n",
    "            model=config[\"default_model\"],\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100  # Limit response length\n",
    "        )\n",
    "        \n",
    "        # Extract the text from the response\n",
    "        # response.choices is a list (can have multiple if n > 1)\n",
    "        # Each choice has a message with role and content\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test with each configured provider\n",
    "# Using a simple math question to verify basic functionality\n",
    "test_prompt = \"What is 2 + 2? Answer briefly.\"\n",
    "\n",
    "for provider in PROVIDERS:\n",
    "    print(f\"\\n=== {provider.upper()} ===\")\n",
    "    result = simple_chat(provider, test_prompt)\n",
    "    print(f\"Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Inspect Response Structure\n",
    "\n",
    "Understanding the response structure is crucial for building robust applications. All OpenAI-compatible providers return the same JSON format.\n",
    "\n",
    "### Response Object Structure:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"id\": \"chatcmpl-xxx\",           # Unique ID for this completion\n",
    "  \"model\": \"llama-3.3-70b-versatile\",  # Model used\n",
    "  \"choices\": [{                   # Array of completions (usually 1)\n",
    "    \"index\": 0,                   # Position in choices array\n",
    "    \"message\": {\n",
    "      \"role\": \"assistant\",        # Always \"assistant\"\n",
    "      \"content\": \"...\"            # The actual response text\n",
    "    },\n",
    "    \"finish_reason\": \"stop\"       # Why generation stopped\n",
    "  }],\n",
    "  \"usage\": {                      # Token counts for cost tracking\n",
    "    \"prompt_tokens\": 10,\n",
    "    \"completion_tokens\": 8,\n",
    "    \"total_tokens\": 18\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Finish Reasons:\n",
    "\n",
    "- `stop`: Natural end (model finished its response)\n",
    "- `length`: Hit max_tokens limit (response was cut off)\n",
    "- `tool_calls`: Model wants to call a function (advanced usage)\n",
    "\n",
    "### Why This Matters:\n",
    "\n",
    "- **Usage tracking**: Monitor costs by counting tokens\n",
    "- **Error handling**: Check `finish_reason` to detect truncated responses\n",
    "- **Logging**: Store `response.id` for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_response(provider_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Make a request and return the full response structure.\n",
    "    \n",
    "    This function extracts all the key fields from the response object\n",
    "    so you can see exactly what data is available.\n",
    "    \n",
    "    Args:\n",
    "        provider_name: Key from PROVIDERS dict\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with id, model, content, finish_reason, and usage\n",
    "    \"\"\"\n",
    "    config = PROVIDERS[provider_name]\n",
    "    client = OpenAI(\n",
    "        base_url=config[\"base_url\"],\n",
    "        api_key=config[\"api_key\"]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Make a simple request\n",
    "        response = client.chat.completions.create(\n",
    "            model=config[\"default_model\"],\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'hello'\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        \n",
    "        # Extract all the important fields\n",
    "        # This shows you what's available in the response object\n",
    "        return {\n",
    "            # Unique identifier for this completion\n",
    "            \"id\": response.id,\n",
    "            \n",
    "            # The model that was actually used\n",
    "            # (may differ from requested if provider aliases it)\n",
    "            \"model\": response.model,\n",
    "            \n",
    "            # The actual text response\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \n",
    "            # Why the model stopped generating\n",
    "            # \"stop\" = natural end, \"length\" = hit max_tokens\n",
    "            \"finish_reason\": response.choices[0].finish_reason,\n",
    "            \n",
    "            # Token usage - important for cost tracking\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": response.usage.prompt_tokens,      # Input tokens\n",
    "                \"completion_tokens\": response.usage.completion_tokens,  # Output tokens\n",
    "                \"total_tokens\": response.usage.total_tokens          # Sum of both\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Inspect response from each provider\n",
    "# Notice how the structure is identical across providers!\n",
    "for provider in PROVIDERS:\n",
    "    print(f\"\\n=== {provider.upper()} Response ===\")\n",
    "    result = inspect_response(provider)\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: TODO - Multi-Provider Client\n",
    "\n",
    "Now it's your turn! Implement a `MultiProviderClient` class that:\n",
    "\n",
    "1. **Stores multiple provider configurations**\n",
    "2. **Creates OpenAI clients for each provider** at initialization\n",
    "3. **Provides a simple `chat()` method** that routes to the right provider\n",
    "4. **Handles errors gracefully**\n",
    "\n",
    "### Why This Pattern Matters:\n",
    "\n",
    "In production applications, you often want to:\n",
    "- **Use different providers for different tasks** (e.g., cheap model for simple tasks, powerful model for complex ones)\n",
    "- **Implement fallback logic** (if one provider fails, try another)\n",
    "- **Compare responses** from different models\n",
    "- **Load balance** across providers\n",
    "\n",
    "### Hints:\n",
    "\n",
    "- Store clients in a dictionary: `self._clients = {\"groq\": OpenAI(...), ...}`\n",
    "- The `chat()` method should look up the client and make the request\n",
    "- Use `kwargs.pop(\"model\", config[\"default_model\"])` to allow model override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MultiProviderClient\n",
    "#\n",
    "# This is a skeleton for you to complete. Read through the class\n",
    "# and implement the missing parts marked with TODO comments.\n",
    "#\n",
    "# GOAL: Create a class that manages multiple API providers and lets\n",
    "# you switch between them with a single method call.\n",
    "\n",
    "class MultiProviderClient:\n",
    "    \"\"\"A client that can switch between multiple providers.\"\"\"\n",
    "    \n",
    "    def __init__(self, providers: dict):\n",
    "        \"\"\"\n",
    "        Initialize with provider configurations.\n",
    "        \n",
    "        Args:\n",
    "            providers: Dict mapping provider names to their config\n",
    "                      e.g., {\"groq\": {\"base_url\": \"...\", \"api_key\": \"...\"}}\n",
    "        \n",
    "        TODO:\n",
    "        1. Store the providers dict as self.providers\n",
    "        2. Create an OpenAI client for each provider\n",
    "        3. Store clients in self._clients dict\n",
    "        \"\"\"\n",
    "        # TODO: Store provider configs\n",
    "        # self.providers = providers\n",
    "        \n",
    "        # TODO: Create OpenAI client for each provider\n",
    "        # self._clients = {}\n",
    "        # for name, config in providers.items():\n",
    "        #     self._clients[name] = OpenAI(...)\n",
    "        pass\n",
    "    \n",
    "    def chat(self, provider_name: str, messages: list, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Send a chat request to the specified provider.\n",
    "        \n",
    "        Args:\n",
    "            provider_name: Which provider to use (e.g., \"groq\")\n",
    "            messages: List of message dicts with role and content\n",
    "            **kwargs: Additional options (model, max_tokens, temperature, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            The assistant's response text\n",
    "        \n",
    "        TODO:\n",
    "        1. Check if provider_name exists in self._clients\n",
    "        2. Get the client and config for this provider\n",
    "        3. Get the model (use default if not specified in kwargs)\n",
    "        4. Call client.chat.completions.create()\n",
    "        5. Return the response content\n",
    "        \"\"\"\n",
    "        # TODO: Get the client for this provider\n",
    "        # TODO: Make the chat completion request\n",
    "        # TODO: Return the response content\n",
    "        pass\n",
    "    \n",
    "    def list_providers(self) -> list:\n",
    "        \"\"\"\n",
    "        Return list of available provider names.\n",
    "        \n",
    "        Returns:\n",
    "            List of provider name strings\n",
    "        \n",
    "        TODO:\n",
    "        Return the keys from self.providers\n",
    "        \"\"\"\n",
    "        # TODO: Return list of provider names\n",
    "        # return list(self.providers.keys())\n",
    "        pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "# Uncomment these lines after implementing the class:\n",
    "# client = MultiProviderClient(PROVIDERS)\n",
    "# print(\"Available providers:\", client.list_providers())\n",
    "# response = client.chat(\"groq\", [{\"role\": \"user\", \"content\": \"Hi\"}])\n",
    "# print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Solution\n",
    "\n",
    "Here's a complete implementation of the `MultiProviderClient` class. Compare this to your implementation to see how it all fits together.\n",
    "\n",
    "### Key Design Decisions:\n",
    "\n",
    "1. **Pre-create all clients at initialization** - This avoids creating a new client object on every request (more efficient)\n",
    "\n",
    "2. **Store both providers and clients** - We need the original config to access `default_model`\n",
    "\n",
    "3. **Use `kwargs.pop()` for model** - This allows callers to override the default model while keeping it optional\n",
    "\n",
    "4. **Raise ValueError for unknown providers** - This makes debugging easier than failing silently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiProviderClient:\n",
    "    \"\"\"\n",
    "    A client that can switch between multiple providers.\n",
    "    \n",
    "    This class demonstrates a common pattern in production AI applications:\n",
    "    managing multiple API providers through a single interface.\n",
    "    \n",
    "    Benefits:\n",
    "    - Single interface for multiple providers\n",
    "    - Easy switching between providers\n",
    "    - Pre-initialized clients (no overhead per request)\n",
    "    - Centralized configuration management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, providers: dict):\n",
    "        \"\"\"\n",
    "        Initialize with provider configurations.\n",
    "        \n",
    "        Creates an OpenAI client for each provider upfront.\n",
    "        This is more efficient than creating clients on each request.\n",
    "        \"\"\"\n",
    "        # Store the original config (needed for default_model)\n",
    "        self.providers = providers\n",
    "        \n",
    "        # Create a client for each provider\n",
    "        # These are stored privately (underscore prefix) to prevent direct access\n",
    "        self._clients = {}\n",
    "        \n",
    "        for name, config in providers.items():\n",
    "            # Each client is configured with the provider's base_url and api_key\n",
    "            self._clients[name] = OpenAI(\n",
    "                base_url=config[\"base_url\"],\n",
    "                api_key=config[\"api_key\"]\n",
    "            )\n",
    "    \n",
    "    def chat(self, provider_name: str, messages: list, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Send a chat request to a specific provider.\n",
    "        \n",
    "        Args:\n",
    "            provider_name: Which provider to use (must be in self._clients)\n",
    "            messages: Conversation as list of {\"role\": \"...\", \"content\": \"...\"}\n",
    "            **kwargs: Optional parameters like max_tokens, temperature, model\n",
    "        \n",
    "        Returns:\n",
    "            The assistant's response text\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If provider_name is not recognized\n",
    "        \"\"\"\n",
    "        # Validate provider exists\n",
    "        if provider_name not in self._clients:\n",
    "            raise ValueError(f\"Unknown provider: {provider_name}\")\n",
    "        \n",
    "        # Get the pre-configured client for this provider\n",
    "        client = self._clients[provider_name]\n",
    "        \n",
    "        # Get the config (needed for default_model)\n",
    "        config = self.providers[provider_name]\n",
    "        \n",
    "        # Allow model override via kwargs, otherwise use default\n",
    "        # pop() removes the key if present, returns default if not\n",
    "        model = kwargs.pop(\"model\", config[\"default_model\"])\n",
    "        \n",
    "        # Make the API call\n",
    "        # Any remaining kwargs (max_tokens, temperature, etc.) are passed through\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Extract and return just the text content\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def list_providers(self) -> list:\n",
    "        \"\"\"\n",
    "        Return list of available provider names.\n",
    "        \n",
    "        Useful for UI displays or validation.\n",
    "        \"\"\"\n",
    "        return list(self.providers.keys())\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "# This demonstrates the class in action with real API calls\n",
    "if PROVIDERS:\n",
    "    # Create the multi-provider client\n",
    "    client = MultiProviderClient(PROVIDERS)\n",
    "    print(\"Available providers:\", client.list_providers())\n",
    "    \n",
    "    # Test each provider with a simple request\n",
    "    for provider in client.list_providers():\n",
    "        try:\n",
    "            response = client.chat(\n",
    "                provider,\n",
    "                [{\"role\": \"user\", \"content\": \"Say 'test'\"}],\n",
    "                max_tokens=5\n",
    "            )\n",
    "            print(f\"{provider}: {response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{provider}: Error - {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
