{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 — Part 03: Structured Outputs and Validation Lab\n",
    "\n",
    "**Estimated time:** 90–120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on structured outputs and validation:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Structured outputs and schemas](../self_learn/Chapters/3/01_function_calling_structured_outputs.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You can parse LLM outputs into structured objects.\n",
    "- You can validate outputs against schemas.\n",
    "- You can implement retry logic for invalid outputs.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook:\n",
    "- You can parse JSON outputs into Pydantic models\n",
    "- You can validate and retry on schema failures\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Parse LLM outputs into structured formats\n",
    "- Validate outputs against schemas\n",
    "- Implement retry patterns for validation failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428e59b",
   "metadata": {},
   "source": [
    "### What this part covers\n",
    "This notebook builds a **parse → validate → retry/repair** pipeline for LLM outputs.\n",
    "\n",
    "Even with a strict contract prompt, models sometimes return:\n",
    "- prose before/after the JSON\n",
    "- single quotes instead of double quotes\n",
    "- missing required keys\n",
    "- extra keys\n",
    "\n",
    "This notebook shows how to detect each failure mode and repair it automatically — with a hard retry cap so you don't loop forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf92eb",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Models can produce valid JSON, or almost-JSON (extra prose, single quotes, trailing commas).\n",
    "\n",
    "This lab builds a deterministic wrapper:\n",
    "\n",
    "1. ask for strict JSON\n",
    "2. parse it\n",
    "3. validate schema\n",
    "4. retry/repair on failure (capped)\n",
    "\n",
    "Key habit: save raw output when parsing/validation fails so debugging is inspection, not guesswork.\n",
    "\n",
    "If you need more background on schemas/validation, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a20e2ba",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines the two-stage validation pipeline:\n",
    "\n",
    "- **`parse_json_strict()`** — attempts `json.loads()`. Fails if the text is not valid JSON (e.g., prose before the JSON, single quotes, trailing commas).\n",
    "- **`validate_shape()`** — checks that the parsed dict has exactly the right keys and types. Fails if keys are missing, extra, or the wrong type.\n",
    "\n",
    "Then tests both against 3 common bad outputs:\n",
    "1. Prose before the JSON (`\"Here is the JSON: {...}\"`) — parse fails\n",
    "2. Single quotes (`{'person': ...}`) — parse fails (Python dict syntax ≠ JSON)\n",
    "3. Missing key (`{\"person\": \"Ada\"}`) — shape validation fails\n",
    "\n",
    "**Key insight:** Parse failure and schema failure have different root causes. Parse failure = the model ignored the \"return ONLY JSON\" instruction. Schema failure = the model returned valid JSON but with the wrong structure. Different fixes for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc696f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def parse_json_strict(text: str) -> dict:\n",
    "    data = json.loads(text)\n",
    "    if not isinstance(data, dict):\n",
    "        raise ValueError(\"expected a JSON object\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def validate_shape(data: dict) -> None:\n",
    "    allowed = {\"person\", \"company\"}\n",
    "    extra = set(data.keys()) - allowed\n",
    "    missing = allowed - set(data.keys())\n",
    "    if missing:\n",
    "        raise ValueError(f\"missing keys: {sorted(missing)}\")\n",
    "    if extra:\n",
    "        raise ValueError(f\"extra keys: {sorted(extra)}\")\n",
    "\n",
    "    for k in [\"person\", \"company\"]:\n",
    "        v = data[k]\n",
    "        if v is not None and not isinstance(v, str):\n",
    "            raise ValueError(f\"{k} must be string or null\")\n",
    "\n",
    "\n",
    "print(validate_shape(parse_json_strict('{\"person\": \"Ada\", \"company\": null}')))\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838ce84",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Defines `call_llm_stub()` — a simulated LLM that always returns the \"prose + JSON\" failure pattern on the first call, then returns clean JSON when the prompt contains \"REPAIR\".\n",
    "\n",
    "**Why simulate failures?** You need to test your retry logic without burning real API credits. This stub lets you exercise the full repair loop deterministically.\n",
    "\n",
    "**What to notice:** The stub returns `\"Here is the JSON: {...}\"` — a very common failure where the model adds explanatory text before the JSON. Your parser must handle this gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb814a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_validate(text: str) -> dict:\n",
    "    data = parse_json_strict(text)\n",
    "    validate_shape(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "bad_outputs = [\n",
    "    \"Here is the JSON: {\\\"person\\\": \\\"Ada\\\", \\\"company\\\": null}\",\n",
    "    \"{'person': 'Ada', 'company': null}\",\n",
    "    '{\"person\": \"Ada\"}',\n",
    "]\n",
    "\n",
    "for raw in bad_outputs:\n",
    "    try:\n",
    "        parse_and_validate(raw)\n",
    "        print(\"OK\", raw)\n",
    "    except Exception as e:\n",
    "        print(\"FAIL\", type(e).__name__, \"->\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb72f35",
   "metadata": {},
   "source": [
    "### What this cell does\n",
    "Implements `extract_with_repair()` — the full retry/repair loop:\n",
    "\n",
    "1. Build the base prompt\n",
    "2. Call the LLM\n",
    "3. Try to parse and validate the output\n",
    "4. If it fails: build a **repair prompt** that tells the model exactly what went wrong, then retry\n",
    "5. Cap retries at `max_retries` to avoid infinite loops\n",
    "\n",
    "**Key design:** The repair prompt includes the invalid output AND the error message. This gives the model concrete feedback: \"your output was `{...}` and the error was `missing keys: ['company']`.\" Vague repair prompts (\"try again\") are much less effective.\n",
    "\n",
    "**Your task:** Implement `extract_with_repair_todo()` with two improvements: (1) save the raw output to disk on failure so you can inspect it, and (2) distinguish parse errors from schema errors in the repair message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771dd89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_stub(prompt: str) -> str:\n",
    "    # Simulate a model that sometimes returns almost-JSON.\n",
    "    if \"REPAIR\" in prompt:\n",
    "        return '{\"person\": \"Ada Lovelace\", \"company\": null}'\n",
    "    return \"Here is the JSON: {\\\"person\\\": \\\"Ada Lovelace\\\", \\\"company\\\": null}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af3eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_repair(text: str, call_llm, *, max_retries: int = 2) -> dict:\n",
    "    base_prompt = (\n",
    "        \"Return ONLY JSON with keys person, company (null when unknown).\\n\"\n",
    "        f\"INPUT:\\n{text}\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = base_prompt\n",
    "    last_err = None\n",
    "    for attempt in range(max_retries + 1):\n",
    "        raw = call_llm(prompt)\n",
    "        try:\n",
    "            return parse_and_validate(raw)\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "            prompt = (\n",
    "                \"REPAIR: Your previous output was invalid.\\n\"\n",
    "                \"Return ONLY JSON with keys person, company.\\n\"\n",
    "                f\"Invalid output:\\n{raw}\\n\\n\"\n",
    "                f\"Error:\\n{last_err}\\n\"\n",
    "            )\n",
    "\n",
    "    raise ValueError(f\"Failed after retries. Last error: {last_err}\")\n",
    "\n",
    "\n",
    "print(extract_with_repair(\"Ada Lovelace\", call_llm_stub, max_retries=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad43f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_repair_todo(text: str, call_llm, *, max_retries: int = 2) -> dict:\n",
    "    # TODO:\n",
    "    # - Add raw-output persistence to output/llm_raw.txt on failure.\n",
    "    # - Separate parse failures from schema failures in the error message.\n",
    "    # - Keep retries capped (max_retries).\n",
    "    return extract_with_repair(text, call_llm, max_retries=max_retries)\n",
    "\n",
    "\n",
    "print(\"Implement extract_with_repair_todo().\")\n",
    "print(extract_with_repair_todo(\"Ada Lovelace\", call_llm_stub, max_retries=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d01db5",
   "metadata": {},
   "source": [
    "## Common pitfalls\n",
    "\n",
    "- Asking for JSON but not banning extra text\n",
    "- Not separating parse failure vs schema failure\n",
    "- No retry cap\n",
    "- Mixing business logic with parsing/validation\n",
    "\n",
    "## References\n",
    "\n",
    "- Python `json`: https://docs.python.org/3/library/json.html\n",
    "- Pydantic (optional): https://docs.pydantic.dev/latest/\n",
    "- JSON Schema: https://json-schema.org/\n",
    "- Tenacity: https://tenacity.readthedocs.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e35ee",
   "metadata": {},
   "source": [
    "## Exercise: Persist raw failures\n",
    "\n",
    "Implement the TODO function below.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- When parsing/validation fails, persist the raw output under `output/`.\n",
    "- Return the output path so you can reference it in a report/debugging.\n",
    "\n",
    "Checkpoint:\n",
    "\n",
    "- Running the cell creates a file like `output/raw_failure.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def persist_raw_failure_todo(raw_text: str, *, filename: str = \"raw_failure.txt\") -> Path:\n",
    "    # TODO: implement\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(\"TODO\\n\", encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "for raw in bad_outputs:\n",
    "    try:\n",
    "        parse_and_validate(raw)\n",
    "    except Exception:\n",
    "        p = persist_raw_failure_todo(raw)\n",
    "        print(\"saved raw failure to\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfb02c7",
   "metadata": {},
   "source": [
    "### Solution: Persist raw failures\n",
    "\n",
    "Reference implementation for `persist_raw_failure_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e00e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_raw_failure_todo(raw_text: str, *, filename: str = \"raw_failure.txt\") -> Path:\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(raw_text, encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "for raw in bad_outputs:\n",
    "    try:\n",
    "        parse_and_validate(raw)\n",
    "    except Exception:\n",
    "        p = persist_raw_failure_todo(raw)\n",
    "        print(\"saved raw failure to\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba78a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_validate(text: str) -> dict:\n",
    "    data = parse_json_strict(text)\n",
    "    validate_shape(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "bad_outputs = [\n",
    "    \"Here is the JSON: {\\\"person\\\": \\\"Ada\\\", \\\"company\\\": null}\",\n",
    "    \"{'person': 'Ada', 'company': null}\",\n",
    "    '{\"person\": \"Ada\"}',\n",
    "]\n",
    "\n",
    "for raw in bad_outputs:\n",
    "    try:\n",
    "        parse_and_validate(raw)\n",
    "        print(\"OK\", raw)\n",
    "    except Exception as e:\n",
    "        print(\"FAIL\", type(e).__name__, \"->\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56b1c0",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `extract_with_repair_todo` that persists raw outputs and clarifies error stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd72104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_repair_todo(text: str, call_llm, *, max_retries: int = 2) -> dict:\n",
    "    base_prompt = (\n",
    "        \"Return ONLY JSON with keys person, company (null when unknown).\\n\"\n",
    "        f\"INPUT:\\n{text}\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = base_prompt\n",
    "    last_err = None\n",
    "    last_raw = None\n",
    "\n",
    "    for attempt in range(max_retries + 1):\n",
    "        raw = call_llm(prompt)\n",
    "        last_raw = raw\n",
    "        try:\n",
    "            data = parse_json_strict(raw)\n",
    "        except Exception as e:\n",
    "            last_err = f\"PARSE_ERROR: {e}\"\n",
    "            prompt = (\n",
    "                \"REPAIR: Your previous output was invalid JSON.\\n\"\n",
    "                \"Return ONLY JSON with keys person, company.\\n\"\n",
    "                f\"Invalid output:\\n{raw}\\n\\n\"\n",
    "                f\"Error:\\n{last_err}\\n\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            validate_shape(data)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            last_err = f\"SCHEMA_ERROR: {e}\"\n",
    "            prompt = (\n",
    "                \"REPAIR: Your previous output failed schema validation.\\n\"\n",
    "                \"Return ONLY JSON with keys person, company.\\n\"\n",
    "                f\"Invalid output:\\n{raw}\\n\\n\"\n",
    "                f\"Error:\\n{last_err}\\n\"\n",
    "            )\n",
    "\n",
    "    raw_path = OUTPUT_DIR / \"llm_raw.txt\"\n",
    "    raw_path.write_text(last_raw or \"\", encoding=\"utf-8\")\n",
    "    raise ValueError(f\"Failed after retries. {last_err}. Raw saved to {raw_path}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    extract_with_repair_todo(\"Ada Lovelace\", call_llm_stub, max_retries=1)\n",
    "except Exception as e:\n",
    "    print(\"expected failure path exercised:\", str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
