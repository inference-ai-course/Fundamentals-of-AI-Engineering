{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 — Part 01: Tokens and Context Windows Lab\n",
    "\n",
    "**Estimated time:** 90–120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundamental Course assumes Self-learn is complete. If you need a refresher on tokenization and prompt engineering:\n",
    "\n",
    "- [Foundamental Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Prompt engineering and evaluation](../../self_learn/Chapters/3/02_prompt_engineering_evaluation.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 01)\n",
    "\n",
    "- You can estimate token counts for a given text.\n",
    "- You can explain why long prompts fail (context window limits).\n",
    "- You can design prompts that fit within limits.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook:\n",
    "- You can measure prompt size in tokens\n",
    "- You can identify when a prompt exceeds typical context limits\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand tokenization basics\n",
    "- Measure token counts in prompts\n",
    "- Design prompts for context window constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b994fc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lab is about budgeting context so your prompts and structured outputs don’t fail unexpectedly.\n",
    "\n",
    "You will:\n",
    "\n",
    "- compare token counts across inputs\n",
    "- implement a simple truncation strategy\n",
    "- build a small token-budget summary helper (TODO)\n",
    "\n",
    "If you need more background on why prompts fail under long context, use the Self-learn links at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception as e:  # pragma: no cover\n",
    "    tiktoken = None\n",
    "    _tiktoken_error = e\n",
    "\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "\n",
    "sample = \"Hello, world! Token test.\"\n",
    "print(\"simple tokens:\", simple_tokenize(sample))\n",
    "\n",
    "if tiktoken is None:\n",
    "    print(\"tiktoken not available:\", _tiktoken_error)\n",
    "else:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(\"Token counting is useful for budgeting.\")\n",
    "    print(\"tiktoken count:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1919",
   "metadata": {},
   "source": [
    "## Truncation exercise\n",
    "\n",
    "Use a simple truncation strategy to fit text into a max token budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ae50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text_tokens(text: str, max_tokens: int, *, enc) -> str:\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    return enc.decode(tokens[:max_tokens])\n",
    "\n",
    "\n",
    "if tiktoken is not None:\n",
    "    long_text = \"data \" * 400\n",
    "    truncated = truncate_text_tokens(long_text, 200, enc=enc)\n",
    "    print(\"orig tokens:\", len(enc.encode(long_text)))\n",
    "    print(\"trunc tokens:\", len(enc.encode(truncated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec933c",
   "metadata": {},
   "source": [
    "## Exercise: token budgeting helper (TODO)\n",
    "\n",
    "Implement the TODO function below.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Given a piece of text, return a dict containing:\n",
    "  - `n_simple_tokens` using `simple_tokenize`\n",
    "  - `n_tiktoken_tokens` if `tiktoken` is available, otherwise `None`\n",
    "\n",
    "Checkpoint:\n",
    "\n",
    "- Running the cell prints token counts for at least 2 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe04a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_budget_summary_todo(text: str) -> dict:\n",
    "    # TODO: implement.\n",
    "    # Keep it runnable even if tiktoken is not installed.\n",
    "    return {\"n_simple_tokens\": len(simple_tokenize(text)), \"n_tiktoken_tokens\": None}\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"Hello world!\",\n",
    "    \"https://example.com/some/path?with=query&and=more\",\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(s, \"->\", token_budget_summary_todo(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e17957",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `token_budget_summary_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_budget_summary_todo(text: str) -> dict:\n",
    "    n_simple = len(simple_tokenize(text))\n",
    "\n",
    "    if tiktoken is None:\n",
    "        return {\"n_simple_tokens\": n_simple, \"n_tiktoken_tokens\": None}\n",
    "\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    n_tiktoken = len(enc.encode(text))\n",
    "    return {\"n_simple_tokens\": n_simple, \"n_tiktoken_tokens\": int(n_tiktoken)}\n",
    "\n",
    "\n",
    "for s in examples:\n",
    "    print(s, \"->\", token_budget_summary_todo(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
