{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations Course — Week 3 Practice (Starter Notebook)\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Self-learn)\n",
    "\n",
    "Foundations Course assumes Self-learn is complete. If you need a refresher on prompt engineering fundamentals and structured outputs:\n",
    "\n",
    "- [Foundations Course Pre-study index](../PRESTUDY.md)\n",
    "- [Self-learn — Prompt engineering and evaluation](../self_learn/Chapters/3/02_prompt_engineering_evaluation.md)\n",
    "- [Self-learn — Structured outputs and schemas](../self_learn/Chapters/3/01_function_calling_structured_outputs.md)\n",
    "\n",
    "---\n",
    "\n",
    "Starter code for structured outputs: JSON parsing + schema validation + retry/repair patterns.\n",
    "\n",
    "## What success looks like (end of practice)\n",
    "\n",
    "- You can turn raw model text into a validated object (or a clear failure).\n",
    "- You can retry/repair a failure with a hard cap.\n",
    "- You saved at least one raw failure output under `output/` for inspection.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "- `parse_validate_with_retry(...)` succeeds for at least one bad input.\n",
    "- You can point to an output file under `output/` that captures a failed raw string.\n",
    "\n",
    "## References (docs)\n",
    "- JSON Schema (official): https://json-schema.org/\n",
    "- Python `json` (official): https://docs.python.org/3/library/json.html\n",
    "- Pydantic (validation): https://docs.pydantic.dev/latest/\n",
    "- Tenacity (retries): https://tenacity.readthedocs.io/\n",
    "- Prompt Engineering Guide (community): https://www.promptingguide.ai/\n",
    "- Anthropic Cookbook (GitHub): https://github.com/anthropics/anthropic-cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec116ad9",
   "metadata": {},
   "source": [
    "## What is a Prompt?\n",
    "\n",
    "A **prompt** is the input text or instructions you send to a Large Language Model (LLM). It acts as the API contract between your code and the AI.\n",
    "\n",
    "Typically, when using an LLM API (like OpenAI's), a prompt is broken down into structured roles:\n",
    "- **System**: High-level instructions, persona, and rules (e.g., \"You are a helpful Python expert. Always return JSON\").\n",
    "- **User**: The specific request, task, or data the user wants processed.\n",
    "\n",
    "Let's look at a basic example of how to construct and send a prompt using the OpenAI API format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17536080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Make sure you have a .env file with your OPENAI_API_KEY\n",
    "load_dotenv()\n",
    "\n",
    "# We initialize the client\n",
    "client = OpenAI()\n",
    "\n",
    "def call_llm(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    A basic wrapper around an LLM API call demonstrating 'What is a Prompt?'.\n",
    "    We separate the 'System' (rules/persona) from the 'User' (task/data).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.0 # Keep it deterministic\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error calling API: {e}\"\n",
    "\n",
    "# Example of a System Prompt (the \"Contract\")\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful Python expert. \n",
    "Your goal is to explain concepts clearly to beginners.\n",
    "\"\"\"\n",
    "\n",
    "# Example of a User Prompt (the \"Input\")\n",
    "user_prompt = \"Explain what a Python dictionary is in one sentence.\"\n",
    "\n",
    "# Let's run it\n",
    "output = call_llm(system_prompt, user_prompt)\n",
    "print(\"=== LLM Response ===\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this in an environment with `pydantic` and `tenacity` installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a target schema\n",
    "\n",
    "This schema defines what downstream code can rely on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractionItem(BaseModel):\n",
    "    field: str\n",
    "    value: str\n",
    "\n",
    "class ExtractionResult(BaseModel):\n",
    "    items: List[ExtractionItem]\n",
    "    notes: Optional[str] = None\n",
    "\n",
    "schema_json = ExtractionResult.model_json_schema()\n",
    "schema_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate model output\n",
    "\n",
    "We simulate common failure cases: invalid JSON, and valid JSON with wrong shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_good = json.dumps({\n",
    "    'items': [{'field': 'company', 'value': 'Acme'}],\n",
    "    'notes': 'ok',\n",
    "}, ensure_ascii=False)\n",
    "raw_bad_json = 'items: [company=Acme]'\n",
    "raw_wrong_shape = json.dumps({'items': [{'field': 'company'}]}, ensure_ascii=False)\n",
    "raw_good, raw_bad_json, raw_wrong_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse + validate helper\n",
    "\n",
    "JSON parsing + schema validation turns model output into an explicit success/failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_validate(raw_text: str) -> ExtractionResult:\n",
    "    data = json.loads(raw_text)\n",
    "    return ExtractionResult.model_validate(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_and_validate(raw_good)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retry/repair wrapper (starter pattern)\n",
    "\n",
    "In production you might re-prompt the model using the schema and the invalid output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_repair(raw_text: str) -> str:\n",
    "    # TODO: replace with an LLM re-ask in the real project\n",
    "    if raw_text.startswith('items:'):\n",
    "        return json.dumps({\n",
    "            'items': [{'field': 'company', 'value': 'Acme'}],\n",
    "            'notes': 'repaired',\n",
    "        }, ensure_ascii=False)\n",
    "    return raw_text\n",
    "\n",
    "def parse_validate_with_retry(raw_text: str) -> ExtractionResult:\n",
    "    repaired = naive_repair(raw_text)\n",
    "    return parse_and_validate(repaired)\n",
    "\n",
    "parse_validate_with_retry(raw_bad_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: persist raw failures (TODO)\n",
    "\n",
    "Implement the TODO function below.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- When parsing/validation fails, persist the raw output under `output/`.\n",
    "- Return the output path so you can reference it in a report/debugging.\n",
    "\n",
    "Checkpoint:\n",
    "\n",
    "- Running the cell creates a file like `output/raw_failure.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def persist_raw_failure_todo(raw_text: str, *, filename: str = \"raw_failure.txt\") -> Path:\n",
    "    # TODO: implement\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(\"TODO\\n\", encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "for raw in [raw_bad_json, raw_wrong_shape]:\n",
    "    try:\n",
    "        parse_and_validate(raw)\n",
    "    except Exception:\n",
    "        p = persist_raw_failure_todo(raw)\n",
    "        print(\"saved raw failure to\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611732a",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `persist_raw_failure_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_raw_failure_todo(raw_text: str, *, filename: str = \"raw_failure.txt\") -> Path:\n",
    "    out_path = OUTPUT_DIR / filename\n",
    "    out_path.write_text(raw_text, encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "for raw in [raw_bad_json, raw_wrong_shape]:\n",
    "    try:\n",
    "        parse_and_validate(raw)\n",
    "    except Exception:\n",
    "        p = persist_raw_failure_todo(raw)\n",
    "        print(\"saved raw failure to\", p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
