{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Model Interfaces and Deployment\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Set up and configure local inference engines (Ollama, vLLM)\n",
    "- Implement OpenAI-compatible interfaces across different providers\n",
    "- Build production-ready deployment architectures\n",
    "- Compare performance metrics across deployment options\n",
    "- Implement security, monitoring, and scaling solutions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install openai huggingface_hub vllm requests python-dotenv psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's start by setting up our environment and API credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import psutil\n",
    "from typing import Dict, Any, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys (use environment variables in production)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"your-openai-key-here\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", \"your-huggingface-token-here\")\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"OpenAI API Key configured: {'Yes' if OPENAI_API_KEY != 'your-openai-key-here' else 'No'}\")\n",
    "print(f\"HuggingFace Token configured: {'Yes' if HF_TOKEN != 'your-huggingface-token-here' else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Universal OpenAI-Compatible Client\n",
    "\n",
    "### Why a Universal Client? The Provider Lock-in Problem\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "Different AI providers have **different APIs**:\n",
    "- OpenAI: `client.chat.completions.create(...)`\n",
    "- HuggingFace: `client.text_generation(...)`\n",
    "- Anthropic: `client.messages.create(...)`\n",
    "- Ollama: `requests.post(\"http://localhost:11434/api/generate\", ...)`\n",
    "\n",
    "**The Problem:**\n",
    "- **Vendor Lock-in**: Hard to switch providers\n",
    "- **Code Duplication**: Rewrite logic for each provider\n",
    "- **Testing Complexity**: Test each provider separately\n",
    "- **Maintenance Burden**: Update code in multiple places\n",
    "\n",
    "**The Solution: Universal Client**\n",
    "\n",
    "A **single interface** that works with multiple providers:\n",
    "- \u2705 **Same Code**: Write once, works everywhere\n",
    "- \u2705 **Easy Switching**: Change provider with one parameter\n",
    "- \u2705 **Unified Testing**: Test once, works for all\n",
    "- \u2705 **Future-Proof**: Add new providers easily\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding OpenAI-Compatible APIs\n",
    "\n",
    "**What is \"OpenAI-Compatible\"?**\n",
    "\n",
    "Many providers now offer **OpenAI-compatible APIs**:\n",
    "- Same endpoint structure: `/v1/chat/completions`\n",
    "- Same request format: `messages`, `model`, `temperature`\n",
    "- Same response format: `choices[0].message.content`\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Standardization**: Industry moving toward common interface\n",
    "- **Compatibility**: Can use OpenAI SDK with other providers\n",
    "- **Portability**: Easy to switch between providers\n",
    "- **Ecosystem**: Works with existing tools and libraries\n",
    "\n",
    "**Providers with OpenAI-Compatible APIs:**\n",
    "- OpenAI (original)\n",
    "- HuggingFace Inference API\n",
    "- Ollama (local)\n",
    "- vLLM (local)\n",
    "- Together AI\n",
    "- Anyscale\n",
    "- Many more...\n",
    "\n",
    "---\n",
    "\n",
    "### The Universal Client Architecture\n",
    "\n",
    "**Design Pattern: Adapter Pattern**\n",
    "\n",
    "```\n",
    "Your Application\n",
    "    \u2193\n",
    "Universal Client (adapter)\n",
    "    \u2193\n",
    "Provider-Specific Clients\n",
    "    \u2193\n",
    "OpenAI / HuggingFace / Ollama / etc.\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "**1. Provider Detection**\n",
    "```python\n",
    "if provider == \"openai\":\n",
    "    return OpenAI(api_key=api_key)\n",
    "elif provider == \"huggingface\":\n",
    "    return OpenAI(base_url=\"https://api-inference.huggingface.co/v1\")\n",
    "```\n",
    "**Why**: Each provider needs different initialization\n",
    "\n",
    "**2. Unified Interface**\n",
    "```python\n",
    "def chat_completion(messages, model, **kwargs):\n",
    "    # Same interface for all providers\n",
    "    return client.chat.completions.create(...)\n",
    "```\n",
    "**Why**: Your code doesn't need to know which provider\n",
    "\n",
    "**3. Response Normalization**\n",
    "```python\n",
    "return {\n",
    "    'success': True,\n",
    "    'response': response.choices[0].message.content,\n",
    "    'usage': {...},\n",
    "    'provider': self.provider\n",
    "}\n",
    "```\n",
    "**Why**: Consistent response format regardless of provider\n",
    "\n",
    "---\n",
    "\n",
    "### Handling Provider Differences\n",
    "\n",
    "**Challenge 1: Different Base URLs**\n",
    "```python\n",
    "base_urls = {\n",
    "    'openai': 'https://api.openai.com/v1',\n",
    "    'huggingface': 'https://api-inference.huggingface.co/v1',\n",
    "    'ollama': 'http://localhost:11434/v1',\n",
    "    'vllm': 'http://localhost:8000/v1'\n",
    "}\n",
    "```\n",
    "**Solution**: Map provider to base URL\n",
    "\n",
    "**Challenge 2: Different Model Names**\n",
    "```python\n",
    "default_models = {\n",
    "    'openai': 'gpt-3.5-turbo',\n",
    "    'huggingface': 'microsoft/DialoGPT-medium',\n",
    "    'ollama': 'llama2',\n",
    "    'vllm': 'microsoft/DialoGPT-medium'\n",
    "}\n",
    "```\n",
    "**Solution**: Provider-specific defaults\n",
    "\n",
    "**Challenge 3: Different Authentication**\n",
    "```python\n",
    "if provider == \"openai\":\n",
    "    client = OpenAI(api_key=api_key)\n",
    "elif provider == \"ollama\":\n",
    "    client = OpenAI(api_key=\"ollama\")  # No real auth needed\n",
    "```\n",
    "**Solution**: Handle auth per provider\n",
    "\n",
    "**Challenge 4: Different Response Formats**\n",
    "```python\n",
    "# Normalize all responses to same format\n",
    "response_data = {\n",
    "    'response': extract_content(response),\n",
    "    'usage': extract_usage(response),\n",
    "    'model': extract_model(response)\n",
    "}\n",
    "```\n",
    "**Solution**: Extract and normalize\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Universal Client\n",
    "\n",
    "**1. Flexibility**\n",
    "- Switch providers without code changes\n",
    "- Test with different providers easily\n",
    "- Use cheapest/fastest provider per use case\n",
    "\n",
    "**2. Reliability**\n",
    "- Automatic failover between providers\n",
    "- Redundancy for critical applications\n",
    "- Provider-agnostic error handling\n",
    "\n",
    "**3. Cost Optimization**\n",
    "- Route requests to cheapest provider\n",
    "- Use local models when possible\n",
    "- Balance cost vs quality\n",
    "\n",
    "**4. Development Speed**\n",
    "- Write code once\n",
    "- Test with multiple providers\n",
    "- Deploy with confidence\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "**1. Multi-Provider Strategy**\n",
    "```python\n",
    "# Try OpenAI first, fallback to HuggingFace\n",
    "try:\n",
    "    result = universal_client.chat_completion(..., provider=\"openai\")\n",
    "except:\n",
    "    result = universal_client.chat_completion(..., provider=\"huggingface\")\n",
    "```\n",
    "\n",
    "**2. Cost Optimization**\n",
    "```python\n",
    "# Use local Ollama for development\n",
    "# Use OpenAI for production\n",
    "provider = \"ollama\" if is_development else \"openai\"\n",
    "```\n",
    "\n",
    "**3. A/B Testing**\n",
    "```python\n",
    "# Test different providers\n",
    "for provider in [\"openai\", \"huggingface\", \"ollama\"]:\n",
    "    result = universal_client.chat_completion(..., provider=provider)\n",
    "    compare_results(result)\n",
    "```\n",
    "\n",
    "**4. Geographic Routing**\n",
    "```python\n",
    "# Use local provider in EU for GDPR compliance\n",
    "provider = \"ollama\" if user_location == \"EU\" else \"openai\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Error Handling**\n",
    "- Provider-specific error messages\n",
    "- Graceful degradation\n",
    "- Clear error reporting\n",
    "\n",
    "**2. Logging**\n",
    "- Log which provider was used\n",
    "- Track provider performance\n",
    "- Monitor provider availability\n",
    "\n",
    "**3. Configuration**\n",
    "- Environment variables for API keys\n",
    "- Configurable provider selection\n",
    "- Easy to add new providers\n",
    "\n",
    "**4. Testing**\n",
    "- Test with all providers\n",
    "- Mock providers for unit tests\n",
    "- Integration tests with real providers\n",
    "\n",
    "Now let's build the universal client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class UniversalAIClient:\n",
    "    \"\"\"Universal client for multiple AI providers with OpenAI-compatible APIs.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str, api_key: str, base_url: str = None):\n",
    "        self.provider = provider\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.client = self._initialize_client()\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize the appropriate client based on provider.\"\"\"\n",
    "        if self.provider == \"openai\":\n",
    "            return OpenAI(api_key=self.api_key)\n",
    "        elif self.provider in [\"huggingface\", \"hf\"]:\n",
    "            return OpenAI(\n",
    "                api_key=self.api_key,\n",
    "                base_url=self.base_url or \"https://api-inference.huggingface.co/v1\"\n",
    "            )\n",
    "        elif self.provider == \"ollama\":\n",
    "            return OpenAI(\n",
    "                api_key=\"ollama\",  # Ollama doesn't require auth\n",
    "                base_url=self.base_url or \"http://localhost:11434/v1\"\n",
    "            )\n",
    "        elif self.provider == \"vllm\":\n",
    "            return OpenAI(\n",
    "                api_key=\"vllm\",  # vLLM doesn't require auth\n",
    "                base_url=self.base_url or \"http://localhost:8000/v1\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {self.provider}\")\n",
    "    \n",
    "    def chat_completion(self, messages: List[Dict[str, str]], model: str = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Create chat completion with unified interface.\"\"\"\n",
    "        # Use appropriate model for each provider\n",
    "        if not model:\n",
    "            model = self._get_default_model()\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=kwargs.get('temperature', 0.7),\n",
    "                max_tokens=kwargs.get('max_tokens', 150),\n",
    "                top_p=kwargs.get('top_p', 1.0),\n",
    "                frequency_penalty=kwargs.get('frequency_penalty', 0),\n",
    "                presence_penalty=kwargs.get('presence_penalty', 0),\n",
    "                stream=kwargs.get('stream', False)\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'provider': self.provider,\n",
    "                'model': model,\n",
    "                'response': response.choices[0].message.content,\n",
    "                'usage': {\n",
    "                    'prompt_tokens': getattr(response.usage, 'prompt_tokens', 0),\n",
    "                    'completion_tokens': getattr(response.usage, 'completion_tokens', 0),\n",
    "                    'total_tokens': getattr(response.usage, 'total_tokens', 0)\n",
    "                },\n",
    "                'latency': kwargs.get('start_time', time.time()) - time.time()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'provider': self.provider,\n",
    "                'error': str(e),\n",
    "                'model': model\n",
    "            }\n",
    "    \n",
    "    def _get_default_model(self) -> str:\n",
    "        \"\"\"Get default model for each provider.\"\"\"\n",
    "        defaults = {\n",
    "            'openai': 'gpt-3.5-turbo',\n",
    "            'huggingface': 'microsoft/DialoGPT-medium',\n",
    "            'hf': 'microsoft/DialoGPT-medium',\n",
    "            'ollama': 'llama2',\n",
    "            'vllm': 'microsoft/DialoGPT-medium'\n",
    "        }\n",
    "        return defaults.get(self.provider, 'gpt-3.5-turbo')\n",
    "\n",
    "# Test the universal client\n",
    "print(\"Testing Universal AI Client...\")\n",
    "\n",
    "# Test with available providers\n",
    "providers_to_test = []\n",
    "if OPENAI_API_KEY != \"your-openai-key-here\":\n",
    "    providers_to_test.append((\"openai\", OPENAI_API_KEY))\n",
    "if HF_TOKEN != \"your-huggingface-token-here\":\n",
    "    providers_to_test.append((\"huggingface\", HF_TOKEN))\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "for provider, api_key in providers_to_test:\n",
    "    try:\n",
    "        print(f\"\\n--- Testing {provider.upper()} ---\")\n",
    "        client = UniversalAIClient(provider, api_key)\n",
    "        result = client.chat_completion(test_messages)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"\u2705 Response: {result['response'][:100]}...\")\n",
    "            print(f\"Tokens used: {result['usage']['total_tokens']}\")\n",
    "        else:\n",
    "            print(f\"\u274c Error: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {provider}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: HuggingFace Inference Provider Management\n",
    "\n",
    "### Why Provider Management? The Reliability Challenge\n",
    "\n",
    "**The Reality of Cloud Services:**\n",
    "\n",
    "Cloud AI providers are **not 100% reliable**:\n",
    "- **Rate Limits**: Too many requests \u2192 temporary blocks\n",
    "- **Service Outages**: Providers go down occasionally\n",
    "- **Geographic Issues**: Some regions slower than others\n",
    "- **Model Availability**: Models may be temporarily unavailable\n",
    "\n",
    "**The Impact:**\n",
    "- **User Experience**: Failed requests = unhappy users\n",
    "- **Business Impact**: Downtime = lost revenue\n",
    "- **Reliability**: Single point of failure = risky\n",
    "\n",
    "**The Solution: Provider Management**\n",
    "\n",
    "Intelligent routing and failover:\n",
    "- \u2705 **Multiple Providers**: Don't rely on just one\n",
    "- \u2705 **Automatic Failover**: Switch on failure\n",
    "- \u2705 **Performance Tracking**: Use best-performing provider\n",
    "- \u2705 **Load Balancing**: Distribute requests\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding HuggingFace Provider Options\n",
    "\n",
    "**HuggingFace Inference API Providers:**\n",
    "\n",
    "HuggingFace offers multiple inference providers:\n",
    "- **hf-inference**: HuggingFace's own infrastructure\n",
    "- **auto**: Automatically selects best available\n",
    "- **Custom endpoints**: Your own deployed models\n",
    "\n",
    "**Why Multiple Providers?**\n",
    "\n",
    "**1. Redundancy**\n",
    "- If one provider fails, others available\n",
    "- No single point of failure\n",
    "- Higher uptime\n",
    "\n",
    "**2. Performance**\n",
    "- Different providers have different speeds\n",
    "- Geographic proximity matters\n",
    "- Load balancing improves response times\n",
    "\n",
    "**3. Cost**\n",
    "- Some providers cheaper than others\n",
    "- Route based on cost requirements\n",
    "- Optimize spending\n",
    "\n",
    "---\n",
    "\n",
    "### The Failover Pattern\n",
    "\n",
    "**How Failover Works:**\n",
    "\n",
    "```\n",
    "Request 1: Try Provider A\n",
    "    \u2193\n",
    "Provider A fails\n",
    "    \u2193\n",
    "Request 2: Try Provider B (failover)\n",
    "    \u2193\n",
    "Provider B succeeds\n",
    "    \u2193\n",
    "Continue using Provider B\n",
    "```\n",
    "\n",
    "**Implementation Strategy:**\n",
    "\n",
    "**1. Preferred Provider List**\n",
    "```python\n",
    "preferred_providers = [\"hf-inference\", \"auto\"]\n",
    "```\n",
    "**Why**: Try best providers first\n",
    "\n",
    "**2. Retry Logic**\n",
    "```python\n",
    "for attempt in range(max_retries):\n",
    "    for provider in preferred_providers:\n",
    "        try:\n",
    "            response = client.chat_completion(..., provider=provider)\n",
    "            return response  # Success!\n",
    "        except Exception as e:\n",
    "            continue  # Try next provider\n",
    "```\n",
    "**Why**: Automatic retry with different providers\n",
    "\n",
    "**3. Exponential Backoff**\n",
    "```python\n",
    "time.sleep(2 ** attempt)  # Wait longer each retry\n",
    "```\n",
    "**Why**: Don't overwhelm failing providers\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Tracking\n",
    "\n",
    "**Why Track Performance?**\n",
    "\n",
    "- **Optimization**: Use fastest providers\n",
    "- **Monitoring**: Detect performance degradation\n",
    "- **Decision Making**: Data-driven provider selection\n",
    "- **Cost Analysis**: Balance speed vs cost\n",
    "\n",
    "**Metrics to Track:**\n",
    "\n",
    "**1. Success Rate**\n",
    "```python\n",
    "success_rate = successful_requests / total_requests * 100\n",
    "```\n",
    "**Why**: Reliability indicator\n",
    "\n",
    "**2. Average Latency**\n",
    "```python\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "```\n",
    "**Why**: Speed indicator\n",
    "\n",
    "**3. Percentile Latencies**\n",
    "```python\n",
    "p50_latency = median(latencies)  # Typical user experience\n",
    "p95_latency = percentile(latencies, 95)  # Worst-case for most users\n",
    "p99_latency = percentile(latencies, 99)  # Extreme cases\n",
    "```\n",
    "**Why**: Understand latency distribution\n",
    "\n",
    "**4. Error Types**\n",
    "```python\n",
    "error_counts = {\n",
    "    'timeout': 5,\n",
    "    'rate_limit': 2,\n",
    "    'model_unavailable': 1\n",
    "}\n",
    "```\n",
    "**Why**: Understand failure modes\n",
    "\n",
    "---\n",
    "\n",
    "### Provider Selection Strategy\n",
    "\n",
    "**Strategy 1: Fastest First**\n",
    "```python\n",
    "# Sort providers by average latency\n",
    "providers_sorted = sorted(providers, key=lambda p: avg_latency[p])\n",
    "```\n",
    "**Use when**: Speed is critical\n",
    "\n",
    "**Strategy 2: Most Reliable First**\n",
    "```python\n",
    "# Sort providers by success rate\n",
    "providers_sorted = sorted(providers, key=lambda p: success_rate[p], reverse=True)\n",
    "```\n",
    "**Use when**: Reliability is critical\n",
    "\n",
    "**Strategy 3: Cost-Optimized**\n",
    "```python\n",
    "# Sort providers by cost per request\n",
    "providers_sorted = sorted(providers, key=lambda p: cost_per_request[p])\n",
    "```\n",
    "**Use when**: Cost is critical\n",
    "\n",
    "**Strategy 4: Balanced**\n",
    "```python\n",
    "# Weighted score: speed + reliability + cost\n",
    "score = (latency_weight * latency) + (reliability_weight * success_rate) + (cost_weight * cost)\n",
    "```\n",
    "**Use when**: Need balance\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Failover Scenarios\n",
    "\n",
    "**Scenario 1: Rate Limit Hit**\n",
    "```\n",
    "Request \u2192 Provider A (rate limited)\n",
    "    \u2193\n",
    "Automatic failover \u2192 Provider B\n",
    "    \u2193\n",
    "Success!\n",
    "```\n",
    "\n",
    "**Scenario 2: Service Outage**\n",
    "```\n",
    "Request \u2192 Provider A (timeout)\n",
    "    \u2193\n",
    "Retry with exponential backoff\n",
    "    \u2193\n",
    "Still failing \u2192 Switch to Provider B\n",
    "    \u2193\n",
    "Success!\n",
    "```\n",
    "\n",
    "**Scenario 3: Geographic Routing**\n",
    "```\n",
    "EU User \u2192 EU Provider (faster, GDPR compliant)\n",
    "US User \u2192 US Provider (faster)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Monitor Continuously**\n",
    "- Track provider performance in real-time\n",
    "- Alert on degradation\n",
    "- Update provider rankings\n",
    "\n",
    "**2. Test Failover Regularly**\n",
    "- Simulate failures\n",
    "- Verify failover works\n",
    "- Measure failover time\n",
    "\n",
    "**3. Document Provider Differences**\n",
    "- Different capabilities\n",
    "- Different costs\n",
    "- Different SLAs\n",
    "\n",
    "**4. Set Timeouts**\n",
    "- Don't wait forever\n",
    "- Fail fast, failover quickly\n",
    "- Good user experience\n",
    "\n",
    "Now let's implement provider management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceProviderManager:\n",
    "    \"\"\"Manages HuggingFace inference providers with failover support.\"\"\"\n",
    "    \n",
    "    def __init__(self, token: str, preferred_providers: List[str] = None):\n",
    "        self.token = token\n",
    "        self.preferred_providers = preferred_providers or [\"auto\"]\n",
    "        self.client = self._initialize_client()\n",
    "        self.provider_performance = {}\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize HuggingFace Inference Client.\"\"\"\n",
    "        try:\n",
    "            from huggingface_hub import InferenceClient\n",
    "            return InferenceClient(token=self.token)\n",
    "        except ImportError:\n",
    "            print(\"HuggingFace Hub not installed. Install with: pip install huggingface_hub\")\n",
    "            return None\n",
    "    \n",
    "    def chat_completion_with_failover(self, messages: List[Dict[str, str]], \n",
    "                                    model: str, max_retries: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Attempt chat completion with provider failover.\"\"\"\n",
    "        if not self.client:\n",
    "            return {'success': False, 'error': 'HuggingFace client not available'}\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Try with specified providers\n",
    "                for provider in self.preferred_providers:\n",
    "                    try:\n",
    "                        start_time = time.time()\n",
    "                        response = self.client.chat_completion(\n",
    "                            messages=messages,\n",
    "                            model=model,\n",
    "                            provider=provider,\n",
    "                            max_tokens=150,\n",
    "                            temperature=0.7\n",
    "                        )\n",
    "                        \n",
    "                        latency = time.time() - start_time\n",
    "                        self._record_provider_performance(provider, latency, True)\n",
    "                        \n",
    "                        return {\n",
    "                            'success': True,\n",
    "                            'provider': provider,\n",
    "                            'response': response.choices[0].message.content,\n",
    "                            'latency': latency,\n",
    "                            'attempt': attempt + 1\n",
    "                        }\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Provider {provider} failed: {e}\")\n",
    "                        self._record_provider_performance(provider, 0, False)\n",
    "                        continue\n",
    "                \n",
    "                # Fallback to auto selection\n",
    "                start_time = time.time()\n",
    "                response = self.client.chat_completion(\n",
    "                    messages=messages,\n",
    "                    model=model,\n",
    "                    provider=\"auto\",\n",
    "                    max_tokens=150,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - start_time\n",
    "                self._record_provider_performance(\"auto\", latency, True)\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'provider': 'auto',\n",
    "                    'response': response.choices[0].message.content,\n",
    "                    'latency': latency,\n",
    "                    'attempt': attempt + 1\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'error': str(e),\n",
    "                        'attempts': max_retries\n",
    "                    }\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        \n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'All providers failed after maximum retries',\n",
    "            'attempts': max_retries\n",
    "        }\n",
    "    \n",
    "    def _record_provider_performance(self, provider: str, latency: float, success: bool):\n",
    "        \"\"\"Record provider performance metrics.\"\"\"\n",
    "        if provider not in self.provider_performance:\n",
    "            self.provider_performance[provider] = {\n",
    "                'total_requests': 0,\n",
    "                'successful_requests': 0,\n",
    "                'total_latency': 0,\n",
    "                'latencies': []\n",
    "            }\n",
    "        \n",
    "        self.provider_performance[provider]['total_requests'] += 1\n",
    "        if success:\n",
    "            self.provider_performance[provider]['successful_requests'] += 1\n",
    "            self.provider_performance[provider]['total_latency'] += latency\n",
    "            self.provider_performance[provider]['latencies'].append(latency)\n",
    "    \n",
    "    def get_provider_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get performance statistics for all providers.\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for provider, data in self.provider_performance.items():\n",
    "            if data['total_requests'] > 0:\n",
    "                success_rate = (data['successful_requests'] / data['total_requests']) * 100\n",
    "                avg_latency = data['total_latency'] / data['successful_requests'] if data['successful_requests'] > 0 else 0\n",
    "                \n",
    "                stats[provider] = {\n",
    "                    'total_requests': data['total_requests'],\n",
    "                    'successful_requests': data['successful_requests'],\n",
    "                    'success_rate': success_rate,\n",
    "                    'average_latency': avg_latency,\n",
    "                    'p50_latency': np.median(data['latencies']) if data['latencies'] else 0,\n",
    "                    'p95_latency': np.percentile(data['latencies'], 95) if len(data['latencies']) > 1 else 0\n",
    "                }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Test HuggingFace provider management\n",
    "if HF_TOKEN != \"your-huggingface-token-here\":\n",
    "    print(\"\\n--- Testing HuggingFace Provider Management ---\")\n",
    "    \n",
    "    hf_manager = HuggingFaceProviderManager(\n",
    "        token=HF_TOKEN,\n",
    "        preferred_providers=[\"hf-inference\", \"auto\"]\n",
    "    )\n",
    "    \n",
    "    test_messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What are the benefits of renewable energy?\"}\n",
    "    ]\n",
    "    \n",
    "    # Test multiple requests to gather statistics\n",
    "    for i in range(3):\n",
    "        result = hf_manager.chat_completion_with_failover(\n",
    "            test_messages,\n",
    "            model=\"microsoft/DialoGPT-medium\"\n",
    "        )\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"\u2705 Request {i+1}: Provider={result['provider']}, Latency={result['latency']:.2f}s\")\n",
    "        else:\n",
    "            print(f\"\u274c Request {i+1}: {result['error']}\")\n",
    "    \n",
    "    # Show provider statistics\n",
    "    stats = hf_manager.get_provider_statistics()\n",
    "    print(\"\\nProvider Statistics:\")\n",
    "    for provider, data in stats.items():\n",
    "        print(f\"{provider}: {data['success_rate']:.1f}% success, {data['average_latency']:.2f}s avg latency\")\n",
    "else:\n",
    "    print(\"HuggingFace token not configured. Skipping provider management test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Local Ollama Integration\n",
    "\n",
    "### Why Local Integration? Revisiting the Benefits\n",
    "\n",
    "**Local Models in Production:**\n",
    "\n",
    "While we covered Ollama basics earlier, here we focus on **production integration**:\n",
    "- **Health Checks**: Verify Ollama is running\n",
    "- **Model Management**: List and select available models\n",
    "- **Error Handling**: Graceful degradation if Ollama unavailable\n",
    "- **Performance Monitoring**: Track local model performance\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "**1. Availability**\n",
    "- Ollama must be running\n",
    "- Models must be downloaded\n",
    "- System resources must be sufficient\n",
    "\n",
    "**2. Reliability**\n",
    "- Local hardware can fail\n",
    "- Models can be slow\n",
    "- Need fallback options\n",
    "\n",
    "**3. Monitoring**\n",
    "- Track local model performance\n",
    "- Monitor resource usage\n",
    "- Alert on failures\n",
    "\n",
    "---\n",
    "\n",
    "### Health Check Pattern\n",
    "\n",
    "**Why Health Checks?**\n",
    "\n",
    "Before using Ollama, verify:\n",
    "- \u2705 Service is running\n",
    "- \u2705 Models are available\n",
    "- \u2705 System has resources\n",
    "- \u2705 API is responding\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "def check_ollama_health():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return True, \"Ollama is healthy\"\n",
    "        else:\n",
    "            return False, f\"Ollama returned {response.status_code}\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False, \"Ollama is not running\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return False, \"Ollama connection timed out\"\n",
    "```\n",
    "\n",
    "**When to Check:**\n",
    "- **Startup**: Verify before accepting requests\n",
    "- **Periodically**: Monitor ongoing health\n",
    "- **Before Requests**: Quick check before each request\n",
    "- **On Failure**: Verify after errors\n",
    "\n",
    "---\n",
    "\n",
    "### Model Discovery and Selection\n",
    "\n",
    "**Why Model Discovery?**\n",
    "\n",
    "Different Ollama installations have different models:\n",
    "- User may have downloaded different models\n",
    "- Models vary in size and capability\n",
    "- Need to select appropriate model\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "```python\n",
    "def get_available_models():\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    models = response.json().get('models', [])\n",
    "    return [model['name'] for model in models]\n",
    "\n",
    "def select_best_model(available_models, preferred_models):\n",
    "    # Try preferred models first\n",
    "    for preferred in preferred_models:\n",
    "        if preferred in available_models:\n",
    "            return preferred\n",
    "    # Fallback to first available\n",
    "    return available_models[0] if available_models else None\n",
    "```\n",
    "\n",
    "**Model Selection Strategy:**\n",
    "- **Preferred Models**: Try best models first\n",
    "- **Fallback**: Use any available model\n",
    "- **Capability Matching**: Match model to task\n",
    "\n",
    "---\n",
    "\n",
    "### Error Handling and Fallback\n",
    "\n",
    "**Error Scenarios:**\n",
    "\n",
    "**1. Ollama Not Running**\n",
    "```python\n",
    "except requests.exceptions.ConnectionError:\n",
    "    # Fallback to cloud provider\n",
    "    return use_cloud_provider()\n",
    "```\n",
    "\n",
    "**2. Model Not Available**\n",
    "```python\n",
    "if model not in available_models:\n",
    "    # Use alternative model\n",
    "    model = select_alternative_model()\n",
    "```\n",
    "\n",
    "**3. Timeout**\n",
    "```python\n",
    "except requests.exceptions.Timeout:\n",
    "    # Ollama too slow, use cloud\n",
    "    return use_cloud_provider()\n",
    "```\n",
    "\n",
    "**4. Resource Exhaustion**\n",
    "```python\n",
    "if system_memory < required_memory:\n",
    "    # Not enough resources, use cloud\n",
    "    return use_cloud_provider()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Hybrid Local/Cloud Strategy\n",
    "\n",
    "**When to Use Local:**\n",
    "- \u2705 Sensitive data (privacy)\n",
    "- \u2705 High volume (cost savings)\n",
    "- \u2705 Offline requirements\n",
    "- \u2705 Development/testing\n",
    "\n",
    "**When to Use Cloud:**\n",
    "- \u2705 Low latency requirements\n",
    "- \u2705 Limited local resources\n",
    "- \u2705 Need latest models\n",
    "- \u2705 High reliability needs\n",
    "\n",
    "**Hybrid Approach:**\n",
    "```python\n",
    "def route_request(request, data_sensitivity):\n",
    "    if data_sensitivity == \"high\":\n",
    "        return use_local_ollama(request)\n",
    "    elif ollama_available and ollama_fast_enough:\n",
    "        return use_local_ollama(request)\n",
    "    else:\n",
    "        return use_cloud_provider(request)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Monitoring\n",
    "\n",
    "**Metrics to Track:**\n",
    "\n",
    "**1. Response Time**\n",
    "- Local models can be slower\n",
    "- Track vs cloud providers\n",
    "- Optimize if needed\n",
    "\n",
    "**2. Resource Usage**\n",
    "- CPU usage\n",
    "- Memory usage\n",
    "- GPU usage (if available)\n",
    "\n",
    "**3. Availability**\n",
    "- Uptime percentage\n",
    "- Failure rate\n",
    "- Recovery time\n",
    "\n",
    "**4. Cost Comparison**\n",
    "- Local: Hardware + electricity\n",
    "- Cloud: Per-request pricing\n",
    "- Calculate break-even point\n",
    "\n",
    "Now let's test the integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_integration():\n",
    "    \"\"\"Test integration with local Ollama instance.\"\"\"\n",
    "    try:\n",
    "        # Check if Ollama is running\n",
    "        ollama_response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if ollama_response.status_code == 200:\n",
    "            print(\"Ollama is running!\")\n",
    "            models = ollama_response.json().get('models', [])\n",
    "            print(f\"Available models: {[model['name'] for model in models]}\")\n",
    "            \n",
    "            # Test with a simple prompt\n",
    "            test_payload = {\n",
    "                \"model\": \"llama2\" if any('llama2' in m['name'] for m in models) else models[0]['name'],\n",
    "                \"prompt\": \"Generate a simple JSON object with 'name' and 'age' fields.\",\n",
    "                \"format\": \"json\",\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json=test_payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                print(f\"Ollama response: {result.get('response', 'No response')}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Ollama API error: {response.status_code}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(\"Ollama is not responding properly\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Ollama is not running. Start it with: ollama serve\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Ollama connection timed out\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test Ollama integration\n",
    "print(\"Testing Ollama integration...\")\n",
    "ollama_available = test_ollama_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Performance Benchmarking Framework\n",
    "\n",
    "### Why Benchmark? Making Data-Driven Decisions\n",
    "\n",
    "**The Decision Problem:**\n",
    "\n",
    "When choosing AI providers, you need to answer:\n",
    "- **Which is fastest?** (User experience)\n",
    "- **Which is cheapest?** (Cost optimization)\n",
    "- **Which is most reliable?** (Uptime)\n",
    "- **Which has best quality?** (Output quality)\n",
    "\n",
    "**Without Benchmarking:**\n",
    "- \u274c Guess based on marketing\n",
    "- \u274c Anecdotal evidence\n",
    "- \u274c Inconsistent testing\n",
    "- \u274c Wrong decisions\n",
    "\n",
    "**With Benchmarking:**\n",
    "- \u2705 Data-driven decisions\n",
    "- \u2705 Objective comparisons\n",
    "- \u2705 Systematic testing\n",
    "- \u2705 Right choices\n",
    "\n",
    "---\n",
    "\n",
    "### What to Benchmark\n",
    "\n",
    "**1. Latency (Response Time)**\n",
    "- **What**: Time from request to response\n",
    "- **Why**: Users notice slow responses\n",
    "- **How**: Measure end-to-end time\n",
    "- **Target**: <2s for most use cases\n",
    "\n",
    "**2. Throughput (Tokens/Second)**\n",
    "- **What**: How fast tokens are generated\n",
    "- **Why**: Affects total response time\n",
    "- **How**: Tokens generated / time taken\n",
    "- **Target**: Higher is better\n",
    "\n",
    "**3. Cost per Request**\n",
    "- **What**: Total cost divided by requests\n",
    "- **Why**: Cost optimization critical\n",
    "- **How**: Track API costs + infrastructure\n",
    "- **Target**: Balance with quality\n",
    "\n",
    "**4. Reliability (Success Rate)**\n",
    "- **What**: Percentage of successful requests\n",
    "- **Why**: Failures = bad user experience\n",
    "- **How**: Successful requests / total requests\n",
    "- **Target**: >99% for production\n",
    "\n",
    "**5. Quality (Output Quality)**\n",
    "- **What**: How good are the outputs?\n",
    "- **Why**: Quality matters more than speed\n",
    "- **How**: Human evaluation or automated metrics\n",
    "- **Target**: Depends on use case\n",
    "\n",
    "---\n",
    "\n",
    "### Benchmarking Methodology\n",
    "\n",
    "**1. Test Design**\n",
    "\n",
    "**Diverse Test Prompts:**\n",
    "- Different lengths\n",
    "- Different complexities\n",
    "- Different domains\n",
    "- Real-world scenarios\n",
    "\n",
    "**Why Diversity Matters:**\n",
    "- Some providers better at certain tasks\n",
    "- Avoid bias toward one provider\n",
    "- Real-world performance\n",
    "\n",
    "**2. Statistical Rigor**\n",
    "\n",
    "**Multiple Runs:**\n",
    "- Run each test multiple times\n",
    "- Account for variability\n",
    "- Calculate confidence intervals\n",
    "\n",
    "**Why Multiple Runs:**\n",
    "- AI outputs are probabilistic\n",
    "- Network conditions vary\n",
    "- Need statistical significance\n",
    "\n",
    "**3. Controlled Environment**\n",
    "\n",
    "**Same Conditions:**\n",
    "- Same test prompts\n",
    "- Same time of day (if network matters)\n",
    "- Same hardware (for local)\n",
    "- Same parameters\n",
    "\n",
    "**Why Control Matters:**\n",
    "- Fair comparison\n",
    "- Reproducible results\n",
    "- Valid conclusions\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding Latency Metrics\n",
    "\n",
    "**Average Latency:**\n",
    "```python\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "```\n",
    "**Use for**: Overall performance indicator\n",
    "\n",
    "**Median Latency (P50):**\n",
    "```python\n",
    "median_latency = median(latencies)\n",
    "```\n",
    "**Use for**: Typical user experience\n",
    "\n",
    "**P95 Latency:**\n",
    "```python\n",
    "p95_latency = percentile(latencies, 95)\n",
    "```\n",
    "**Use for**: Worst-case for most users (5% are slower)\n",
    "\n",
    "**P99 Latency:**\n",
    "```python\n",
    "p99_latency = percentile(latencies, 99)\n",
    "```\n",
    "**Use for**: Extreme cases (1% are slower)\n",
    "\n",
    "**Why Percentiles Matter:**\n",
    "- Average can hide outliers\n",
    "- P95/P99 show worst-case scenarios\n",
    "- Users remember bad experiences\n",
    "\n",
    "---\n",
    "\n",
    "### Cost Analysis\n",
    "\n",
    "**Total Cost of Ownership (TCO):**\n",
    "\n",
    "**Cloud Providers:**\n",
    "```\n",
    "Cost = (requests \u00d7 cost_per_request) + infrastructure\n",
    "```\n",
    "\n",
    "**Local Models:**\n",
    "```\n",
    "Cost = hardware_cost + electricity + maintenance\n",
    "```\n",
    "\n",
    "**Break-Even Analysis:**\n",
    "```\n",
    "Break-even requests = hardware_cost / (cloud_cost_per_request - local_cost_per_request)\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Hardware: $5,000\n",
    "- Cloud: $0.01/request\n",
    "- Local: $0.001/request (electricity)\n",
    "- Break-even: 5,000 / (0.01 - 0.001) = ~555,556 requests\n",
    "\n",
    "---\n",
    "\n",
    "### Benchmarking Best Practices\n",
    "\n",
    "**1. Test Realistic Workloads**\n",
    "- Use actual production prompts\n",
    "- Test at production scale\n",
    "- Include edge cases\n",
    "\n",
    "**2. Measure What Matters**\n",
    "- Don't optimize wrong metrics\n",
    "- Focus on user experience\n",
    "- Balance multiple factors\n",
    "\n",
    "**3. Document Everything**\n",
    "- Test conditions\n",
    "- Provider versions\n",
    "- System configuration\n",
    "- Results and analysis\n",
    "\n",
    "**4. Regular Re-benchmarking**\n",
    "- Providers improve over time\n",
    "- New models released\n",
    "- Your needs change\n",
    "- Keep benchmarks current\n",
    "\n",
    "---\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "**Example Results:**\n",
    "\n",
    "```\n",
    "Provider A:\n",
    "  Avg Latency: 1.2s\n",
    "  P95 Latency: 2.5s\n",
    "  Cost: $0.01/request\n",
    "  Success Rate: 99.5%\n",
    "\n",
    "Provider B:\n",
    "  Avg Latency: 0.5s\n",
    "  P95 Latency: 1.0s\n",
    "  Cost: $0.05/request\n",
    "  Success Rate: 99.9%\n",
    "\n",
    "Provider C:\n",
    "  Avg Latency: 3.0s\n",
    "  P95 Latency: 5.0s\n",
    "  Cost: $0.001/request\n",
    "  Success Rate: 98.0%\n",
    "```\n",
    "\n",
    "**Decision Framework:**\n",
    "- **Speed critical**: Choose Provider B\n",
    "- **Cost critical**: Choose Provider C\n",
    "- **Balanced**: Choose Provider A\n",
    "\n",
    "**Trade-offs:**\n",
    "- Speed vs Cost\n",
    "- Quality vs Speed\n",
    "- Reliability vs Cost\n",
    "\n",
    "Now let's build the benchmarking framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceBenchmark:\n",
    "    \"\"\"Comprehensive benchmarking framework for AI inference engines.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.test_prompts = self.load_test_prompts()\n",
    "    \n",
    "    def load_test_prompts(self):\n",
    "        \"\"\"Load diverse test prompts for benchmarking.\"\"\"\n",
    "        return [\n",
    "            \"Write a short story about artificial intelligence.\",\n",
    "            \"Explain quantum computing in simple terms.\",\n",
    "            \"What are the benefits of renewable energy?\",\n",
    "            \"How does machine learning work?\",\n",
    "            \"Describe the process of photosynthesis.\",\n",
    "            \"What are the key principles of software engineering?\",\n",
    "            \"Explain the difference between SQL and NoSQL databases.\",\n",
    "            \"How do neural networks learn?\",\n",
    "            \"What is the importance of data structures?\",\n",
    "            \"Describe a sustainable city of the future.\"\n",
    "        ]\n",
    "    \n",
    "    def benchmark_openai(self):\n",
    "        \"\"\"Benchmark OpenAI API.\"\"\"\n",
    "        try:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            \n",
    "            results = {\n",
    "                'latencies': [],\n",
    "                'token_counts': [],\n",
    "                'throughput': [],\n",
    "                'cost_estimates': []\n",
    "            }\n",
    "            \n",
    "            for prompt in self.test_prompts:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - start_time\n",
    "                token_count = response.usage.total_tokens\n",
    "                \n",
    "                results['latencies'].append(latency)\n",
    "                results['token_counts'].append(token_count)\n",
    "                results['throughput'].append(token_count / latency)\n",
    "                results['cost_estimates'].append(token_count * 0.002 / 1000)  # Approximate cost\n",
    "            \n",
    "            return self.summarize_results(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def benchmark_huggingface(self):\n",
    "        \"\"\"Benchmark HuggingFace Inference API.\"\"\"\n",
    "        try:\n",
    "            from huggingface_hub import InferenceClient\n",
    "            client = InferenceClient()\n",
    "            \n",
    "            results = {\n",
    "                'latencies': [],\n",
    "                'token_counts': [],\n",
    "                'throughput': []\n",
    "            }\n",
    "            \n",
    "            for prompt in self.test_prompts:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                response = client.text_generation(\n",
    "                    prompt,\n",
    "                    model=\"microsoft/DialoGPT-medium\",\n",
    "                    max_new_tokens=150\n",
    "                )\n",
    "                \n",
    "                latency = time.time() - start_time\n",
    "                token_count = len(response.split())\n",
    "                \n",
    "                results['latencies'].append(latency)\n",
    "                results['token_counts'].append(token_count)\n",
    "                results['throughput'].append(token_count / latency)\n",
    "            \n",
    "            return self.summarize_results(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def summarize_results(self, results: Dict[str, List]) -> Dict[str, Any]:\n",
    "        \"\"\"Summarize benchmark results.\"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        summary = {}\n",
    "        for metric, values in results.items():\n",
    "            if values:\n",
    "                summary[metric] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'median': np.median(values),\n",
    "                    'std': np.std(values),\n",
    "                    'min': np.min(values),\n",
    "                    'max': np.max(values)\n",
    "                }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def run_comprehensive_benchmark(self):\n",
    "        \"\"\"Run comprehensive benchmark across all available providers.\"\"\"\n",
    "        print(\"Running comprehensive benchmark...\")\n",
    "        \n",
    "        # Test OpenAI if available\n",
    "        if OPENAI_API_KEY != \"your-openai-key-here\":\n",
    "            print(\"\\n--- Benchmarking OpenAI ---\")\n",
    "            openai_results = self.benchmark_openai()\n",
    "            if 'error' not in openai_results:\n",
    "                self.results['openai'] = openai_results\n",
    "                print(\"\u2705 OpenAI benchmark completed\")\n",
    "            else:\n",
    "                print(f\"\u274c OpenAI benchmark failed: {openai_results['error']}\")\n",
    "        \n",
    "        # Test HuggingFace if available\n",
    "        if HF_TOKEN != \"your-huggingface-token-here\":\n",
    "            print(\"\\n--- Benchmarking HuggingFace ---\")\n",
    "            hf_results = self.benchmark_huggingface()\n",
    "            if 'error' not in hf_results:\n",
    "                self.results['huggingface'] = hf_results\n",
    "                print(\"\u2705 HuggingFace benchmark completed\")\n",
    "            else:\n",
    "                print(f\"\u274c HuggingFace benchmark failed: {hf_results['error']}\")\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "benchmark = InferenceBenchmark()\n",
    "results = benchmark.run_comprehensive_benchmark()\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for provider, metrics in results.items():\n",
    "    print(f\"\\n{provider.upper()}:\")\n",
    "    for metric, stats in metrics.items():\n",
    "        print(f\"  {metric}:\")\n",
    "        print(f\"    Mean: {stats['mean']:.3f}\")\n",
    "        print(f\"    Median: {stats['median']:.3f}\")\n",
    "        print(f\"    Std Dev: {stats['std']:.3f}\")\n",
    "        print(f\"    Range: {stats['min']:.3f} - {stats['max']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Production Deployment Architecture\n",
    "\n",
    "### What Makes Deployment \"Production-Ready\"?\n",
    "\n",
    "**Development vs Production:**\n",
    "\n",
    "**Development:**\n",
    "- \u2705 Works on your machine\n",
    "- \u2705 Manual testing\n",
    "- \u2705 No monitoring\n",
    "- \u2705 Single user\n",
    "\n",
    "**Production:**\n",
    "- \u2705 Works reliably for thousands of users\n",
    "- \u2705 Automated testing and validation\n",
    "- \u2705 Comprehensive monitoring\n",
    "- \u2705 Handles failures gracefully\n",
    "\n",
    "**Production Requirements:**\n",
    "\n",
    "1. **Reliability**: 99.9%+ uptime\n",
    "2. **Performance**: Fast response times\n",
    "3. **Scalability**: Handle growing load\n",
    "4. **Monitoring**: Know what's happening\n",
    "5. **Security**: Protect data and systems\n",
    "6. **Maintainability**: Easy to update and fix\n",
    "\n",
    "---\n",
    "\n",
    "### The Production Architecture\n",
    "\n",
    "**Core Components:**\n",
    "\n",
    "**1. Application Layer**\n",
    "- Your AI application code\n",
    "- Request handling\n",
    "- Response formatting\n",
    "\n",
    "**2. Provider Management**\n",
    "- Multi-provider support\n",
    "- Failover logic\n",
    "- Load balancing\n",
    "\n",
    "**3. Monitoring Layer**\n",
    "- Health checks\n",
    "- Performance metrics\n",
    "- Error tracking\n",
    "- Alerting\n",
    "\n",
    "**4. Infrastructure Layer**\n",
    "- Servers/containers\n",
    "- Load balancers\n",
    "- Databases\n",
    "- Caching\n",
    "\n",
    "---\n",
    "\n",
    "### Health Check System\n",
    "\n",
    "**Why Health Checks?**\n",
    "\n",
    "Health checks verify:\n",
    "- \u2705 Services are running\n",
    "- \u2705 Providers are available\n",
    "- \u2705 System has resources\n",
    "- \u2705 Dependencies are working\n",
    "\n",
    "**Types of Health Checks:**\n",
    "\n",
    "**1. Liveness Check**\n",
    "```python\n",
    "def liveness_check():\n",
    "    # Is the service running?\n",
    "    return service_is_running()\n",
    "```\n",
    "**Purpose**: Detect if service crashed\n",
    "**Action**: Restart if failed\n",
    "\n",
    "**2. Readiness Check**\n",
    "```python\n",
    "def readiness_check():\n",
    "    # Can the service handle requests?\n",
    "    return providers_available() and resources_sufficient()\n",
    "```\n",
    "**Purpose**: Detect if service ready\n",
    "**Action**: Don't route traffic if not ready\n",
    "\n",
    "**3. Startup Probe**\n",
    "```python\n",
    "def startup_check():\n",
    "    # Has the service finished starting?\n",
    "    return initialization_complete()\n",
    "```\n",
    "**Purpose**: Detect slow startups\n",
    "**Action**: Give more time or restart\n",
    "\n",
    "---\n",
    "\n",
    "### Monitoring and Metrics\n",
    "\n",
    "**What to Monitor:**\n",
    "\n",
    "**1. Request Metrics**\n",
    "- Total requests\n",
    "- Successful requests\n",
    "- Failed requests\n",
    "- Request rate (requests/second)\n",
    "\n",
    "**2. Performance Metrics**\n",
    "- Average latency\n",
    "- P50/P95/P99 latencies\n",
    "- Throughput\n",
    "- Error rate\n",
    "\n",
    "**3. System Metrics**\n",
    "- CPU usage\n",
    "- Memory usage\n",
    "- Disk usage\n",
    "- Network usage\n",
    "\n",
    "**4. Business Metrics**\n",
    "- Cost per request\n",
    "- User satisfaction\n",
    "- Conversion rates\n",
    "- Revenue impact\n",
    "\n",
    "---\n",
    "\n",
    "### Alerting Strategy\n",
    "\n",
    "**When to Alert:**\n",
    "\n",
    "**Critical Alerts (Immediate Action):**\n",
    "- Service down\n",
    "- Error rate > 5%\n",
    "- Latency > 10s\n",
    "- Cost spike\n",
    "\n",
    "**Warning Alerts (Investigate):**\n",
    "- Error rate > 1%\n",
    "- Latency > 2s\n",
    "- Resource usage > 80%\n",
    "- Unusual patterns\n",
    "\n",
    "**Info Alerts (Monitor):**\n",
    "- New deployment\n",
    "- Configuration changes\n",
    "- Performance trends\n",
    "\n",
    "**Alert Best Practices:**\n",
    "- **Actionable**: Alert should trigger action\n",
    "- **Not Noisy**: Don't alert on everything\n",
    "- **Context**: Include relevant information\n",
    "- **Escalation**: Critical alerts need immediate attention\n",
    "\n",
    "---\n",
    "\n",
    "### Scaling Strategies\n",
    "\n",
    "**Horizontal Scaling (Scale Out):**\n",
    "- Add more servers/instances\n",
    "- Distribute load across instances\n",
    "- **Pros**: No downtime, handle more load\n",
    "- **Cons**: More complex, need load balancer\n",
    "\n",
    "**Vertical Scaling (Scale Up):**\n",
    "- Increase server resources (CPU, RAM)\n",
    "- **Pros**: Simple, no code changes\n",
    "- **Cons**: Limited by hardware, downtime\n",
    "\n",
    "**Auto-Scaling:**\n",
    "```python\n",
    "if cpu_usage > 80%:\n",
    "    add_instance()\n",
    "elif cpu_usage < 30%:\n",
    "    remove_instance()\n",
    "```\n",
    "**Benefits**: Automatic, cost-efficient\n",
    "**Challenges**: Configuration, scaling delays\n",
    "\n",
    "---\n",
    "\n",
    "### Error Handling and Recovery\n",
    "\n",
    "**Error Categories:**\n",
    "\n",
    "**1. Transient Errors**\n",
    "- Network timeouts\n",
    "- Rate limits\n",
    "- Temporary unavailability\n",
    "- **Action**: Retry with backoff\n",
    "\n",
    "**2. Permanent Errors**\n",
    "- Invalid API key\n",
    "- Model not found\n",
    "- Malformed request\n",
    "- **Action**: Return error, don't retry\n",
    "\n",
    "**3. Resource Exhaustion**\n",
    "- Out of memory\n",
    "- Too many connections\n",
    "- **Action**: Scale up or throttle\n",
    "\n",
    "**Recovery Strategies:**\n",
    "\n",
    "**1. Retry with Exponential Backoff**\n",
    "```python\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        return make_request()\n",
    "    except TransientError:\n",
    "        time.sleep(2 ** attempt)\n",
    "```\n",
    "\n",
    "**2. Circuit Breaker**\n",
    "```python\n",
    "if error_rate > threshold:\n",
    "    stop_sending_requests()\n",
    "    wait_for_recovery()\n",
    "    try_again()\n",
    "```\n",
    "\n",
    "**3. Graceful Degradation**\n",
    "```python\n",
    "if primary_provider_fails():\n",
    "    use_backup_provider()\n",
    "    notify_team()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Security Considerations\n",
    "\n",
    "**1. API Key Management**\n",
    "- Store in environment variables\n",
    "- Use secret management services\n",
    "- Rotate keys regularly\n",
    "- Never commit to version control\n",
    "\n",
    "**2. Data Privacy**\n",
    "- Encrypt sensitive data\n",
    "- Use local models for sensitive data\n",
    "- Comply with regulations (GDPR, HIPAA)\n",
    "- Audit data access\n",
    "\n",
    "**3. Rate Limiting**\n",
    "- Prevent abuse\n",
    "- Protect from DDoS\n",
    "- Fair resource usage\n",
    "- Cost control\n",
    "\n",
    "**4. Input Validation**\n",
    "- Validate all inputs\n",
    "- Sanitize user data\n",
    "- Prevent injection attacks\n",
    "- Size limits\n",
    "\n",
    "---\n",
    "\n",
    "### Deployment Checklist\n",
    "\n",
    "**Before Production:**\n",
    "\n",
    "- [ ] **Testing**: Comprehensive test suite\n",
    "- [ ] **Monitoring**: Metrics and alerting set up\n",
    "- [ ] **Documentation**: Runbooks and procedures\n",
    "- [ ] **Security**: Keys, encryption, access control\n",
    "- [ ] **Scaling**: Auto-scaling configured\n",
    "- [ ] **Backup**: Disaster recovery plan\n",
    "- [ ] **Rollback**: Can revert if needed\n",
    "- [ ] **Load Testing**: Test under production load\n",
    "\n",
    "**Ongoing:**\n",
    "\n",
    "- [ ] **Monitor**: Watch metrics continuously\n",
    "- [ ] **Update**: Keep dependencies current\n",
    "- [ ] **Optimize**: Improve based on data\n",
    "- [ ] **Document**: Update runbooks\n",
    "- [ ] **Review**: Regular architecture reviews\n",
    "\n",
    "Now let's build the production deployment system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionDeployment:\n",
    "    \"\"\"Production-ready deployment architecture with monitoring and scaling.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.health_status = {}\n",
    "        self.request_metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'average_latency': 0,\n",
    "            'p95_latency': 0,\n",
    "            'p99_latency': 0\n",
    "        }\n",
    "        self.latencies = []\n",
    "    \n",
    "    def health_check(self, provider: str, client) -> Dict[str, Any]:\n",
    "        \"\"\"Perform health check on a provider.\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Simple health check request\n",
    "            response = client.chat_completion([\n",
    "                {\"role\": \"user\", \"content\": \"Health check\"}\n",
    "            ])\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            health_status = {\n",
    "                'status': 'healthy' if response['success'] else 'unhealthy',\n",
    "                'latency': latency,\n",
    "                'timestamp': time.time(),\n",
    "                'error': response.get('error', None)\n",
    "            }\n",
    "            \n",
    "            self.health_status[provider] = health_status\n",
    "            return health_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            health_status = {\n",
    "                'status': 'unhealthy',\n",
    "                'latency': -1,\n",
    "                'timestamp': time.time(),\n",
    "                'error': str(e)\n",
    "            }\n",
    "            self.health_status[provider] = health_status\n",
    "            return health_status\n",
    "    \n",
    "    def record_request_metrics(self, success: bool, latency: float):\n",
    "        \"\"\"Record request metrics for monitoring.\"\"\"\n",
    "        self.request_metrics['total_requests'] += 1\n",
    "        \n",
    "        if success:\n",
    "            self.request_metrics['successful_requests'] += 1\n",
    "            self.latencies.append(latency)\n",
    "            \n",
    "            # Update latency metrics\n",
    "            if self.latencies:\n",
    "                import numpy as np\n",
    "                self.request_metrics['average_latency'] = np.mean(self.latencies)\n",
    "                self.request_metrics['p95_latency'] = np.percentile(self.latencies, 95)\n",
    "                self.request_metrics['p99_latency'] = np.percentile(self.latencies, 99)\n",
    "        else:\n",
    "            self.request_metrics['failed_requests'] += 1\n",
    "    \n",
    "    def get_system_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current system metrics.\"\"\"\n",
    "        import psutil\n",
    "        \n",
    "        return {\n",
    "            'cpu_percent': psutil.cpu_percent(interval=1),\n",
    "            'memory_percent': psutil.virtual_memory().percent,\n",
    "            'disk_usage': psutil.disk_usage('/').percent,\n",
    "            'network_connections': len(psutil.net_connections()),\n",
    "            'process_count': len(psutil.pids())\n",
    "        }\n",
    "    \n",
    "    def generate_deployment_report(self) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive deployment report.\"\"\"\n",
    "        success_rate = (self.request_metrics['successful_requests'] / \n",
    "                       max(self.request_metrics['total_requests'], 1)) * 100\n",
    "        \n",
    "        return {\n",
    "            'timestamp': time.time(),\n",
    "            'health_status': self.health_status,\n",
    "            'request_metrics': self.request_metrics,\n",
    "            'system_metrics': self.get_system_metrics(),\n",
    "            'success_rate': success_rate,\n",
    "            'recommendations': self.generate_recommendations()\n",
    "        }\n",
    "    \n",
    "    def generate_recommendations(self) -> List[str]:\n",
    "        \"\"\"Generate deployment recommendations based on metrics.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # Check success rate\n",
    "        success_rate = (self.request_metrics['successful_requests'] / \n",
    "                       max(self.request_metrics['total_requests'], 1)) * 100\n",
    "        \n",
    "        if success_rate < 95:\n",
    "            recommendations.append(\"Consider implementing circuit breaker pattern for failed requests\")\n",
    "        \n",
    "        # Check latency\n",
    "        if self.request_metrics['p95_latency'] > 2.0:\n",
    "            recommendations.append(\"P95 latency is high. Consider scaling up resources or optimizing model\")\n",
    "        \n",
    "        # Check system resources\n",
    "        system_metrics = self.get_system_metrics()\n",
    "        \n",
    "        if system_metrics['cpu_percent'] > 80:\n",
    "            recommendations.append(\"High CPU usage detected. Consider horizontal scaling\")\n",
    "        \n",
    "        if system_metrics['memory_percent'] > 85:\n",
    "            recommendations.append(\"High memory usage detected. Consider increasing memory allocation\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Test production deployment monitoring\n",
    "print(\"Testing Production Deployment Monitoring...\")\n",
    "\n",
    "deployment = ProductionDeployment()\n",
    "\n",
    "# Test with available providers\n",
    "if OPENAI_API_KEY != \"your-openai-key-here\":\n",
    "    print(\"\\n--- Testing OpenAI Health Check ---\")\n",
    "    openai_client = UniversalAIClient(\"openai\", OPENAI_API_KEY)\n",
    "    health_status = deployment.health_check(\"openai\", openai_client)\n",
    "    print(f\"OpenAI Health: {health_status['status']}, Latency: {health_status['latency']:.3f}s\")\n",
    "    \n",
    "    # Record some test metrics\n",
    "    deployment.record_request_metrics(True, 0.5)\n",
    "    deployment.record_request_metrics(True, 0.7)\n",
    "    deployment.record_request_metrics(False, 1.2)\n",
    "\n",
    "# Generate deployment report\n",
    "report = deployment.generate_deployment_report()\n",
    "print(\"\\n--- Deployment Report ---\")\n",
    "print(f\"Success Rate: {report['success_rate']:.1f}%\")\n",
    "print(f\"Total Requests: {report['request_metrics']['total_requests']}\")\n",
    "print(f\"Average Latency: {report['request_metrics']['average_latency']:.3f}s\")\n",
    "print(f\"P95 Latency: {report['request_metrics']['p95_latency']:.3f}s\")\n",
    "\n",
    "if report['recommendations']:\n",
    "    print(\"\\nRecommendations:\")\n",
    "    for rec in report['recommendations']:\n",
    "        print(f\"  - {rec}\")\n",
    "else:\n",
    "    print(\"\\nNo recommendations - system is performing well!\")\n",
    "\n",
    "print(\"\\nSystem Metrics:\")\n",
    "for metric, value in report['system_metrics'].items():\n",
    "    print(f\"  {metric}: {value}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Model Interfaces and Deployment lab. Here's what you've learned:\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- \u2705 Universal OpenAI-compatible client implementation\n",
    "- \u2705 HuggingFace provider management with failover\n",
    "- \u2705 Local Ollama integration and testing\n",
    "- \u2705 Comprehensive performance benchmarking\n",
    "- \u2705 Production deployment monitoring and health checks\n",
    "- \u2705 System metrics collection and analysis\n",
    "\n",
    "### Best Practices:\n",
    "- Always implement proper error handling and retry mechanisms\n",
    "- Use provider failover for high availability\n",
    "- Monitor system resources and performance metrics\n",
    "- Implement health checks for all services\n",
    "- Use environment variables for sensitive configuration\n",
    "- Design for scalability from the start\n",
    "\n",
    "### Production Considerations:\n",
    "- **Security**: Implement proper authentication and authorization\n",
    "- **Monitoring**: Set up comprehensive logging and alerting\n",
    "- **Scaling**: Design for horizontal scaling with load balancing\n",
    "- **Backup**: Implement backup and disaster recovery procedures\n",
    "- **Compliance**: Ensure compliance with data privacy regulations\n",
    "\n",
    "### Next Steps:\n",
    "1. Set up a local Ollama instance and test with different models\n",
    "2. Implement a load balancer for multiple inference engines\n",
    "3. Add comprehensive logging and monitoring dashboards\n",
    "4. Create automated deployment pipelines\n",
    "5. Implement A/B testing for model performance comparison\n",
    "6. Add support for streaming responses\n",
    "7. Implement rate limiting and quota management\n",
    "8. Set up automated scaling based on demand\n",
    "\n",
    "Remember: Production deployment requires careful planning, monitoring, and continuous optimization. Always test thoroughly in staging environments before deploying to production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}