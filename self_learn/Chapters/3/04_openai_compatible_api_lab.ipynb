{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Compatible API Lab\n",
    "\n",
    "**Estimated time:** 90â€“120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the OpenAI API compatibility standard\n",
    "- Configure clients for multiple providers\n",
    "- Implement streaming responses\n",
    "- Handle errors gracefully\n",
    "- Use structured outputs and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI, RateLimitError, APIError, APIConnectionError, AuthenticationError, BadRequestError\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Provider Configuration\n",
    "\n",
    "Configure multiple providers for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider configurations\n",
    "# FREE providers (no credit card required) - recommended for learning\n",
    "# Get your API keys:\n",
    "# - Groq: https://console.groq.com/keys\n",
    "# - OpenRouter: https://openrouter.ai/keys\n",
    "\n",
    "PROVIDERS = {\n",
    "    # Groq - Ultra-fast inference, generous free tier\n",
    "    # Docs: https://console.groq.com/docs/openai\n",
    "    \"groq\": {\n",
    "        \"base_url\": \"https://api.groq.com/openai/v1\",\n",
    "        \"api_key\": os.environ.get(\"GROQ_API_KEY\", \"your-groq-api-key\"),\n",
    "        \"default_model\": \"llama-3.3-70b-versatile\"\n",
    "    },\n",
    "    \n",
    "    # OpenRouter - 100+ models, free tier available\n",
    "    # Docs: https://openrouter.ai/docs/quickstart\n",
    "    # Free models have :free suffix\n",
    "    \"openrouter\": {\n",
    "        \"base_url\": \"https://openrouter.ai/api/v1\",\n",
    "        \"api_key\": os.environ.get(\"OPENROUTER_API_KEY\", \"your-openrouter-api-key\"),\n",
    "        \"default_model\": \"meta-llama/llama-3.3-70b-instruct:free\"\n",
    "    },\n",
    "    \n",
    "    # Ollama (local) - requires Ollama running on localhost\n",
    "    # See documentation for setup instructions\n",
    "    # \"ollama\": {\n",
    "    #     \"base_url\": \"http://localhost:11434/v1\",\n",
    "    #     \"api_key\": \"ollama\",\n",
    "    #     \"default_model\": \"llama3.2\"\n",
    "    # },\n",
    "    \n",
    "    # LiteLLM proxy - requires running LiteLLM proxy server\n",
    "    # \"litellm\": {\n",
    "    #     \"base_url\": \"http://localhost:4000/v1\",\n",
    "    #     \"api_key\": os.environ.get(\"LITELLM_KEY\", \"sk-xxxx\"),\n",
    "    #     \"default_model\": \"gpt-3.5-turbo\"\n",
    "    # },\n",
    "    \n",
    "    # Vercel AI Gateway\n",
    "    # \"vercel\": {\n",
    "    #     \"base_url\": \"https://ai-gateway.vercel.sh/v1\",\n",
    "    #     \"api_key\": os.environ.get(\"AI_GATEWAY_API_KEY\"),\n",
    "    #     \"default_model\": \"openai/gpt-4o-mini\"\n",
    "    # },\n",
    "}\n",
    "\n",
    "def get_client(provider_name: str) -> OpenAI:\n",
    "    \"\"\"Create an OpenAI client for a specific provider.\"\"\"\n",
    "    config = PROVIDERS.get(provider_name)\n",
    "    if not config:\n",
    "        raise ValueError(f\"Unknown provider: {provider_name}\")\n",
    "    return OpenAI(base_url=config[\"base_url\"], api_key=config[\"api_key\"])\n",
    "\n",
    "print(f\"Configured providers: {list(PROVIDERS.keys())}\")\n",
    "print(\"\\nTo get API keys:\")\n",
    "print(\"  Groq: https://console.groq.com/keys\")\n",
    "print(\"  OpenRouter: https://openrouter.ai/keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: List Models\n",
    "\n",
    "Explore available models from each provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(provider_name: str) -> list:\n",
    "    \"\"\"List available models from a provider.\"\"\"\n",
    "    client = get_client(provider_name)\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        return [m.id for m in models.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "for provider in PROVIDERS:\n",
    "    print(f\"\\n=== {provider.upper()} ===\")\n",
    "    models = list_models(provider)\n",
    "    if models:\n",
    "        print(f\"Found {len(models)} models:\")\n",
    "        for m in models[:10]:\n",
    "            print(f\"  - {m}\")\n",
    "        if len(models) > 10:\n",
    "            print(f\"  ... and {len(models) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Basic Chat Completions\n",
    "\n",
    "Make simple requests to different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(provider_name: str, prompt: str, **kwargs) -> dict:\n",
    "    \"\"\"Make a chat completion request.\"\"\"\n",
    "    config = PROVIDERS[provider_name]\n",
    "    client = get_client(provider_name)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.pop(\"model\", config[\"default_model\"]),\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"finish_reason\": response.choices[0].finish_reason,\n",
    "        \"usage\": response.usage.model_dump()\n",
    "    }\n",
    "\n",
    "# Test basic chat\n",
    "for provider in PROVIDERS:\n",
    "    print(f\"\\n=== {provider.upper()} ===\")\n",
    "    result = chat(provider, \"What is 2 + 2?\", max_tokens=50)\n",
    "    print(f\"Response: {result['content']}\")\n",
    "    print(f\"Tokens: {result['usage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Temperature and Parameters\n",
    "\n",
    "Experiment with temperature and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_temperature(provider_name: str, prompt: str, temperatures: list) -> dict:\n",
    "    \"\"\"Test same prompt at different temperatures.\"\"\"\n",
    "    results = {}\n",
    "    config = PROVIDERS[provider_name]\n",
    "    client = get_client(provider_name)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        response = client.chat.completions.create(\n",
    "            model=config[\"default_model\"],\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        results[temp] = response.choices[0].message.content\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test temperature variation\n",
    "provider = list(PROVIDERS.keys())[0]\n",
    "prompt = \"Write a one-sentence story about a robot.\"\n",
    "temperatures = [0.0, 0.5, 1.0, 1.5]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "results = test_temperature(provider, prompt, temperatures)\n",
    "\n",
    "for temp, output in results.items():\n",
    "    print(f\"Temperature {temp}:\\n  {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Streaming Responses\n",
    "\n",
    "Process responses in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_chat(provider_name: str, prompt: str, **kwargs):\n",
    "    \"\"\"Stream a chat completion response.\"\"\"\n",
    "    config = PROVIDERS[provider_name]\n",
    "    client = get_client(provider_name)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.pop(\"model\", config[\"default_model\"]),\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    full_content = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            full_content += content\n",
    "            print(content, end=\"\", flush=True)\n",
    "    \n",
    "    print()  # Newline at end\n",
    "    return full_content\n",
    "\n",
    "# Test streaming\n",
    "provider = list(PROVIDERS.keys())[0]\n",
    "print(f\"Streaming from {provider}:\\n\")\n",
    "content = stream_chat(provider, \"Tell me a short joke about programming.\", max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Error Handling\n",
    "\n",
    "Implement robust error handling with retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_chat(provider_name: str, messages: list, max_retries: int = 3, **kwargs) -> dict:\n",
    "    \"\"\"Make a chat request with error handling and retries.\"\"\"\n",
    "    config = PROVIDERS[provider_name]\n",
    "    client = get_client(provider_name)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=kwargs.pop(\"model\", config[\"default_model\"]),\n",
    "                messages=messages,\n",
    "                **kwargs\n",
    "            )\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": response.usage.model_dump(),\n",
    "                \"attempts\": attempt + 1\n",
    "            }\n",
    "            \n",
    "        except RateLimitError as e:\n",
    "            wait = 2 ** attempt\n",
    "            print(f\"Rate limited, waiting {wait}s (attempt {attempt + 1})\")\n",
    "            time.sleep(wait)\n",
    "            \n",
    "        except APIConnectionError as e:\n",
    "            print(f\"Connection error: {e}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except AuthenticationError as e:\n",
    "            return {\"success\": False, \"error\": f\"Auth error: {e}\", \"attempts\": attempt + 1}\n",
    "            \n",
    "        except BadRequestError as e:\n",
    "            return {\"success\": False, \"error\": f\"Bad request: {e}\", \"attempts\": attempt + 1}\n",
    "            \n",
    "        except APIError as e:\n",
    "            print(f\"API error: {e}\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return {\"success\": False, \"error\": \"Max retries exceeded\", \"attempts\": max_retries}\n",
    "\n",
    "# Test safe chat\n",
    "provider = list(PROVIDERS.keys())[0]\n",
    "result = safe_chat(provider, [{\"role\": \"user\", \"content\": \"Hello!\"}], max_tokens=20)\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Attempts: {result['attempts']}\")\n",
    "if result['success']:\n",
    "    print(f\"Response: {result['content']}\")\n",
    "else:\n",
    "    print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Structured Outputs\n",
    "\n",
    "Request JSON-formatted responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_response(provider_name: str, prompt: str, **kwargs) -> dict:\n",
    "    \"\"\"Request a JSON response.\"\"\"\n",
    "    config = PROVIDERS[provider_name]\n",
    "    client = get_client(provider_name)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=kwargs.pop(\"model\", config[\"default_model\"]),\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Respond with valid JSON only. No markdown, no explanation.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        return {\"success\": True, \"data\": json.loads(content), \"raw\": content}\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"success\": False, \"error\": str(e), \"raw\": content}\n",
    "\n",
    "# Test JSON output\n",
    "provider = list(PROVIDERS.keys())[0]\n",
    "result = get_json_response(\n",
    "    provider,\n",
    "    \"List 3 programming languages as JSON: {\\\"languages\\\": [{\\\"name\\\": string, \\\"year\\\": int}]}\"\n",
    ")\n",
    "\n",
    "if result['success']:\n",
    "    print(\"Parsed JSON:\")\n",
    "    print(json.dumps(result['data'], indent=2))\n",
    "else:\n",
    "    print(f\"Failed to parse: {result['error']}\")\n",
    "    print(f\"Raw: {result['raw']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Multi-Turn Conversation\n",
    "\n",
    "Maintain context across multiple messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"Manage a multi-turn conversation.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider_name: str, system: str = \"You are a helpful assistant.\"):\n",
    "        self.provider_name = provider_name\n",
    "        self.config = PROVIDERS[provider_name]\n",
    "        self.client = get_client(provider_name)\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system}]\n",
    "    \n",
    "    def send(self, user_input: str, **kwargs) -> str:\n",
    "        \"\"\"Send a message and get a response.\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=kwargs.pop(\"model\", self.config[\"default_model\"]),\n",
    "            messages=self.messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        assistant_msg = response.choices[0].message.content\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "        \n",
    "        return assistant_msg\n",
    "    \n",
    "    def history(self) -> list:\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def clear(self, system: str = None):\n",
    "        system = system or self.messages[0][\"content\"]\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system}]\n",
    "\n",
    "# Test conversation\n",
    "provider = list(PROVIDERS.keys())[0]\n",
    "conv = Conversation(provider, \"You are a helpful math tutor. Be concise.\")\n",
    "\n",
    "print(\"User: What is 5 * 7?\")\n",
    "print(f\"Assistant: {conv.send('What is 5 * 7?')}\\n\")\n",
    "\n",
    "print(\"User: What about 5 * 8?\")\n",
    "print(f\"Assistant: {conv.send('What about 5 * 8?')}\\n\")\n",
    "\n",
    "print(\"User: What was my first question?\")\n",
    "print(f\"Assistant: {conv.send('What was my first question?')}\\n\")\n",
    "\n",
    "print(\"=== Full History ===\")\n",
    "for msg in conv.history():\n",
    "    print(f\"{msg['role']}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Multi-Provider Client\n",
    "\n",
    "Build a client that can switch between providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiProviderClient:\n",
    "    \"\"\"A client that can switch between multiple providers.\"\"\"\n",
    "    \n",
    "    def __init__(self, providers: dict):\n",
    "        self.providers = providers\n",
    "        self._clients = {}\n",
    "        \n",
    "        for name, config in providers.items():\n",
    "            self._clients[name] = OpenAI(\n",
    "                base_url=config[\"base_url\"],\n",
    "                api_key=config[\"api_key\"]\n",
    "            )\n",
    "    \n",
    "    def chat(self, provider_name: str, messages: list, **kwargs) -> str:\n",
    "        \"\"\"Send a chat request to a specific provider.\"\"\"\n",
    "        if provider_name not in self._clients:\n",
    "            raise ValueError(f\"Unknown provider: {provider_name}\")\n",
    "        \n",
    "        client = self._clients[provider_name]\n",
    "        config = self.providers[provider_name]\n",
    "        model = kwargs.pop(\"model\", config[\"default_model\"])\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    def list_providers(self) -> list:\n",
    "        \"\"\"Return available provider names.\"\"\"\n",
    "        return list(self.providers.keys())\n",
    "\n",
    "# Test multi-provider client\n",
    "multi_client = MultiProviderClient(PROVIDERS)\n",
    "print(\"Available providers:\", multi_client.list_providers())\n",
    "\n",
    "for provider in multi_client.list_providers():\n",
    "    try:\n",
    "        response = multi_client.chat(\n",
    "            provider,\n",
    "            [{\"role\": \"user\", \"content\": \"Say 'hello' in one word.\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(f\"{provider}: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{provider}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Provider Comparison**: Make identical requests to multiple providers and compare responses.\n",
    "2. **Streaming with Error Handling**: Combine streaming with retry logic.\n",
    "3. **Cost Tracker**: Build a wrapper that tracks token usage and estimates costs.\n",
    "4. **Fallback Chain**: Implement automatic fallback from one provider to another on failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Cost Tracker\n",
    "\n",
    "class CostTracker:\n",
    "    \"\"\"Track API usage and estimate costs.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_cost_per_1k: float = 0.0005, output_cost_per_1k: float = 0.0015):\n",
    "        self.input_cost_per_1k = input_cost_per_1k\n",
    "        self.output_cost_per_1k = output_cost_per_1k\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "    \n",
    "    def track(self, response):\n",
    "        \"\"\"Track usage from a response.\"\"\"\n",
    "        self.total_input_tokens += response.usage.prompt_tokens\n",
    "        self.total_output_tokens += response.usage.completion_tokens\n",
    "    \n",
    "    def get_cost(self) -> float:\n",
    "        \"\"\"Calculate total cost.\"\"\"\n",
    "        input_cost = (self.total_input_tokens * self.input_cost_per_1k) / 1000\n",
    "        output_cost = (self.total_output_tokens * self.output_cost_per_1k) / 1000\n",
    "        return input_cost + output_cost\n",
    "    \n",
    "    def report(self) -> dict:\n",
    "        \"\"\"Get usage report.\"\"\"\n",
    "        return {\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "            \"total_tokens\": self.total_input_tokens + self.total_output_tokens,\n",
    "            \"estimated_cost\": self.get_cost()\n",
    "        }\n",
    "\n",
    "# Test cost tracker\n",
    "tracker = CostTracker()\n",
    "\n",
    "provider = list(PROVIDERS.keys())[0]\n",
    "client = get_client(provider)\n",
    "config = PROVIDERS[provider]\n",
    "\n",
    "for _ in range(3):\n",
    "    response = client.chat.completions.create(\n",
    "        model=config[\"default_model\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    tracker.track(response)\n",
    "\n",
    "print(\"Usage Report:\")\n",
    "print(json.dumps(tracker.report(), indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
