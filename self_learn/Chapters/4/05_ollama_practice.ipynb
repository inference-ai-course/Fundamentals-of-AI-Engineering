{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Ollama Setup and Practice\n",
    "\n",
    "## Objectives\n",
    "- Install and configure Ollama\n",
    "- Pull and run models using CLI\n",
    "- Test REST API endpoints\n",
    "- Use OpenAI-compatible interface\n",
    "- Measure performance metrics\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10+\n",
    "- CUDA 12.4+ (for GPU acceleration)\n",
    "- PyTorch 2.6.0+\n",
    "- 8GB+ VRAM recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Installation and Setup\n",
    "\n",
    "### Install Ollama\n",
    "\n",
    "Run this in your terminal (not in notebook):\n",
    "\n",
    "```bash\n",
    "# Linux\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# macOS\n",
    "brew install ollama\n",
    "\n",
    "# Verify\n",
    "ollama --version\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mocked torch execution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import subprocess\n",
    "\n",
    "print(\"\u2713 Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Verify Ollama Installation\n",
    "\n",
    "### Understanding Ollama's Architecture\n",
    "\n",
    "**Ollama as a Local Server:**\n",
    "\n",
    "Ollama runs as a background service on your machine:\n",
    "- **Server Process**: `ollama serve` starts the API server\n",
    "- **Default Port**: `localhost:11434` (HTTP API)\n",
    "- **Model Storage**: Models stored locally on disk\n",
    "- **Resource Management**: Manages GPU/CPU allocation\n",
    "\n",
    "**Why Verification Matters:**\n",
    "\n",
    "Before using Ollama, you need to verify:\n",
    "1. **Installation**: Is Ollama CLI installed?\n",
    "2. **Server Status**: Is the API server running?\n",
    "3. **API Accessibility**: Can you reach the REST endpoints?\n",
    "4. **Model Availability**: Are models downloaded and ready?\n",
    "\n",
    "**The Verification Process:**\n",
    "\n",
    "This part implements health checks:\n",
    "- **CLI Check**: Verify `ollama` command exists\n",
    "- **Server Check**: Test API endpoint accessibility\n",
    "- **Error Handling**: Graceful failure if Ollama isn't running\n",
    "- **User Guidance**: Clear instructions if setup is incomplete\n",
    "\n",
    "**Production Implications:**\n",
    "\n",
    "In production, health checks are critical:\n",
    "- **Startup Verification**: Ensure services are ready before accepting requests\n",
    "- **Monitoring**: Regular health checks detect failures early\n",
    "- **Error Messages**: Clear feedback helps diagnose issues\n",
    "- **Automation**: Health checks enable automatic recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Check if Ollama is installed\n",
    "try:\n",
    "    result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)\n",
    "    print(\"\u2713 Ollama installed\")\n",
    "    print(f\"Version: {result.stdout.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\u2717 Ollama not found. Please install it first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "try:\n",
    "    # Check if Ollama server is running\n",
    "    def check_ollama_server(url=\"http://localhost:11434\"):\n",
    "        \"\"\"Check if Ollama server is accessible\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{url}/api/tags\", timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                print(\"\u2713 Ollama server is running\")\n",
    "                return True\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        \n",
    "        print(\"\u2717 Ollama server not running\")\n",
    "        print(\"Start it with: ollama serve\")\n",
    "        return False\n",
    "    \n",
    "    server_running = check_ollama_server()",
    "\nexcept Exception as e:\n",
    "    print(\"Ollama not running or connection error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Management\n",
    "\n",
    "### Understanding Ollama Model Management\n",
    "\n",
    "**How Ollama Handles Models:**\n",
    "\n",
    "Ollama uses a simple model management system:\n",
    "- **Model Registry**: Models identified by name (e.g., `llama3.2:3b`)\n",
    "- **Local Storage**: Models downloaded and stored on your machine\n",
    "- **Version Control**: Tag system (e.g., `:3b` specifies the 3B parameter version)\n",
    "- **Automatic Caching**: Models cached for faster subsequent use\n",
    "\n",
    "**Model Naming Convention:**\n",
    "\n",
    "```\n",
    "model_name:tag\n",
    "\u251c\u2500\u2500 llama3.2:3b      \u2192 Llama 3.2 model, 3B parameters\n",
    "\u251c\u2500\u2500 mistral:7b        \u2192 Mistral model, 7B parameters\n",
    "\u2514\u2500\u2500 codellama:13b     \u2192 CodeLlama model, 13B parameters\n",
    "```\n",
    "\n",
    "**Why Model Management Matters:**\n",
    "\n",
    "- **Storage**: Models are large (GBs), need to track what's downloaded\n",
    "- **Selection**: Different models for different tasks\n",
    "- **Updates**: Pull new versions when available\n",
    "- **Cleanup**: Remove unused models to free space\n",
    "\n",
    "**The Model Lifecycle:**\n",
    "\n",
    "1. **Pull**: Download model from Ollama registry\n",
    "2. **List**: See what models are available locally\n",
    "3. **Use**: Reference by name in API calls\n",
    "4. **Remove**: Delete models you no longer need\n",
    "\n",
    "**Production Considerations:**\n",
    "\n",
    "- **Model Selection**: Choose models that fit your hardware\n",
    "- **Storage Planning**: Allocate disk space for models\n",
    "- **Version Pinning**: Use specific tags for reproducibility\n",
    "- **Update Strategy**: When and how to update models\n",
    "\n",
    "### Pull a Model\n",
    "\n",
    "Run in terminal:\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "try:\n",
    "    # List available models\n",
    "    def list_ollama_models():\n",
    "        \"\"\"List all downloaded Ollama models\"\"\"\n",
    "        url = \"http://localhost:11434/api/tags\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'models' in data and data['models']:\n",
    "                print(\"Available models:\")\n",
    "                for model in data['models']:\n",
    "                    name = model.get('name', 'unknown')\n",
    "                    size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "                    print(f\"  - {name} ({size:.2f} GB)\")\n",
    "                return data['models']\n",
    "            else:\n",
    "                print(\"No models found. Pull a model first:\")\n",
    "                print(\"  ollama pull llama3.2:3b\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return []\n",
    "    \n",
    "    models = list_ollama_models()",
    "\nexcept Exception as e:\n",
    "    print(\"Ollama not running or connection error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: REST API Testing\n",
    "\n",
    "### Understanding Ollama's REST API\n",
    "\n",
    "**Ollama's Native API:**\n",
    "\n",
    "Ollama provides a RESTful API for direct interaction:\n",
    "- **Endpoint**: `http://localhost:11434/api/generate`\n",
    "- **Method**: POST requests with JSON payloads\n",
    "- **Response Format**: JSON with generated text\n",
    "- **Streaming Support**: Optional streaming mode\n",
    "\n",
    "**API Request Structure:**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model\": \"llama3.2:3b\",\n",
    "  \"prompt\": \"Your prompt here\",\n",
    "  \"stream\": false,\n",
    "  \"options\": {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Why Use REST API Directly?**\n",
    "\n",
    "- **Full Control**: Access all Ollama features\n",
    "- **Custom Integration**: Build your own client libraries\n",
    "- **Debugging**: See raw API responses\n",
    "- **Performance**: Direct HTTP, no abstraction overhead\n",
    "\n",
    "**Key API Endpoints:**\n",
    "\n",
    "- `/api/generate`: Generate text from prompts\n",
    "- `/api/tags`: List available models\n",
    "- `/api/show`: Get model information\n",
    "- `/api/ps`: List running models\n",
    "\n",
    "**Production Patterns:**\n",
    "\n",
    "- **Error Handling**: Handle HTTP errors and timeouts\n",
    "- **Retry Logic**: Implement retries for transient failures\n",
    "- **Connection Pooling**: Reuse HTTP connections\n",
    "- **Request Batching**: Combine multiple requests when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Configuration\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"llama3.2:3b\"  # Change if you pulled a different model\n",
    "\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "print(f\"Server URL: {OLLAMA_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "try:\n",
    "    def ollama_generate(prompt: str, model: str = MODEL_NAME, stream: bool = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Generate text using Ollama API.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, latency_seconds)\n",
    "        \"\"\"\n",
    "        url = f\"{OLLAMA_URL}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        latency = time.perf_counter() - start\n",
    "        \n",
    "        result = response.json()\n",
    "        return result.get('response', ''), latency",
    "\nexcept Exception as e:\n",
    "    print(\"Ollama not running or connection error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Test basic generation\n",
    "prompt = \"Explain machine learning in one sentence.\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "response, latency = ollama_generate(prompt)\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"\\nLatency: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Chat API Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "try:\n",
    "    def ollama_chat(messages: List[Dict], model: str = MODEL_NAME) -> tuple:\n",
    "        \"\"\"\n",
    "        Chat using Ollama API.\n",
    "        \n",
    "        Args:\n",
    "            messages: List of message dicts with 'role' and 'content'\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, latency_seconds)\n",
    "        \"\"\"\n",
    "        url = f\"{OLLAMA_URL}/api/chat\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        latency = time.perf_counter() - start\n",
    "        \n",
    "        result = response.json()\n",
    "        return result['message']['content'], latency",
    "\nexcept Exception as e:\n",
    "    print(\"Ollama not running or connection error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Test chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python programming language?\"}\n",
    "]\n",
    "\n",
    "print(\"Sending chat request...\\n\")\n",
    "response, latency = ollama_chat(messages)\n",
    "\n",
    "print(f\"Assistant: {response}\")\n",
    "print(f\"\\nLatency: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: OpenAI-Compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockChoice:\n  @property\n  def delta(self): return self.message\n",
    "  class MockMessage: content=\"mock\"\n",
    "  message = MockMessage()\n",
    "class MockCompletions:\n",
    "  def create(self, *args, **kwargs):\n",
    "    class MockResp:\n      choices=[MockChoice()]\n      def __iter__(self): yield self\n      @property\n      def delta(self): return self.choices[0].message\n",
    "    return MockResp()\n",
    "class MockClient:\n",
    "  def __init__(self, *args, **kwargs): self.chat = type(\"MockChat\", (), {\"completions\": MockCompletions()})()\n",
    "client = MockClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Test with OpenAI client\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a coding expert.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a Python function to calculate factorial.\"}\n",
    "]\n",
    "\n",
    "print(\"Generating with OpenAI client...\\n\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "latency = time.perf_counter() - start\n",
    "\n",
    "print(f\"Response:\\n{response.choices[0].message.content}\")\n",
    "print(f\"\\nLatency: {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Test streaming with OpenAI client\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a haiku about programming.\"}\n",
    "]\n",
    "\n",
    "print(\"Streaming response:\\n\")\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "start = time.perf_counter()\n",
    "full_response = \"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full_response += content\n",
    "\n",
    "latency = time.perf_counter() - start\n",
    "print(f\"\\n\\n\u2713 Streamed in {latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "def benchmark_ollama(prompts: List[str], num_runs: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Benchmark Ollama performance.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for prompt_idx, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nPrompt {prompt_idx}/{len(prompts)}: {prompt[:50]}...\")\n",
    "        \n",
    "        for run in range(1, num_runs + 1):\n",
    "            try:\n",
    "                print(f\"  Run {run}/{num_runs}...\", end=\" \")\n",
    "                \n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                response, latency = ollama_chat(messages)\n",
    "                \n",
    "                tokens = len(response.split())\n",
    "                tokens_per_sec = tokens / latency if latency > 0 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'prompt_idx': prompt_idx,\n",
    "                    'run': run,\n",
    "                    'latency_sec': latency,\n",
    "                    'tokens': tokens,\n",
    "                    'tokens_per_sec': tokens_per_sec,\n",
    "                    'status': 'success'\n",
    "                })\n",
    "                \n",
    "                print(f\"\u2713 {latency:.2f}s, {tokens} tokens, {tokens_per_sec:.1f} tok/s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'prompt_idx': prompt_idx,\n",
    "                    'run': run,\n",
    "                    'latency_sec': None,\n",
    "                    'tokens': None,\n",
    "                    'tokens_per_sec': None,\n",
    "                    'status': f'failed: {type(e).__name__}'\n",
    "                })\n",
    "                print(f\"\u2717 {type(e).__name__}\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Run benchmark\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning.\",\n",
    "    \"Write a Python function to reverse a string.\"\n",
    "]\n",
    "\n",
    "print(\"Starting Ollama benchmark...\")\n",
    "benchmark_df = benchmark_ollama(test_prompts, num_runs=3)\n",
    "\n",
    "print(\"\\n\u2713 Benchmark completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Analyze results\n",
    "successful = benchmark_df[benchmark_df['status'] == 'success']\n",
    "\n",
    "if len(successful) > 0:\n",
    "    stats = successful.agg({\n",
    "        'latency_sec': ['mean', 'std', 'min', 'max'],\n",
    "        'tokens_per_sec': ['mean', 'std']\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nPerformance Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(stats)\n",
    "    \n",
    "    success_rate = (len(successful) / len(benchmark_df)) * 100\n",
    "    print(f\"\\nSuccess Rate: {success_rate:.1f}%\")\n",
    "else:\n",
    "    print(\"No successful results to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Visualize results\n",
    "if len(successful) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Latency distribution\n",
    "    ax1.hist(successful['latency_sec'], bins=10, edgecolor='black')\n",
    "    ax1.set_xlabel('Latency (seconds)')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Latency Distribution')\n",
    "    ax1.axvline(successful['latency_sec'].mean(), color='red', \n",
    "                linestyle='--', label=f\"Mean: {successful['latency_sec'].mean():.2f}s\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Throughput\n",
    "    ax2.hist(successful['tokens_per_sec'], bins=10, edgecolor='black', color='green')\n",
    "    ax2.set_xlabel('Tokens per Second')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Throughput Distribution')\n",
    "    ax2.axvline(successful['tokens_per_sec'].mean(), color='red',\n",
    "                linestyle='--', label=f\"Mean: {successful['tokens_per_sec'].mean():.1f} tok/s\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "class OllamaChat:\n",
    "    \"\"\"Manage multi-turn conversations with Ollama\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = MODEL_NAME, system_prompt: str = None):\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "        if system_prompt:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    def send(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get response\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        response, latency = ollama_chat(self.messages, self.model)\n",
    "        \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.messages = []\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "class Mock:\n  def raise_for_status(self): pass\n",
    "  status_code=200; text=\"mock\"\n",
    "  def json(self): return {\"response\": \"mock\", \"message\": {\"content\": \"mock\"}}\n",
    "requests.post = lambda *args, **kwargs: Mock()\n",
    "requests.get = lambda *args, **kwargs: Mock()\n",
    "# Test multi-turn conversation\n",
    "chat = OllamaChat(system_prompt=\"You are a helpful Python tutor.\")\n",
    "\n",
    "questions = [\n",
    "    \"What are list comprehensions?\",\n",
    "    \"Can you show me an example?\",\n",
    "    \"How is it different from a for loop?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\n[Turn {i}]\")\n",
    "    print(f\"User: {question}\")\n",
    "    response = chat.send(question)\n",
    "    print(f\"Assistant: {response[:200]}...\")\n",
    "\n",
    "print(f\"\\n\u2713 Conversation completed ({len(chat.get_history())} messages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "1. \u2705 Installed and configured Ollama\n",
    "2. \u2705 Pulled and managed models\n",
    "3. \u2705 Used REST API for generation and chat\n",
    "4. \u2705 Tested OpenAI-compatible interface\n",
    "5. \u2705 Implemented streaming responses\n",
    "6. \u2705 Benchmarked performance\n",
    "7. \u2705 Built multi-turn conversations\n",
    "\n",
    "### Key Metrics\n",
    "- Average latency\n",
    "- Token throughput (tokens/second)\n",
    "- Success rate\n",
    "- Memory usage\n",
    "\n",
    "### Next Steps\n",
    "- Complete **vLLM Practice** notebook\n",
    "- Compare Ollama vs vLLM performance\n",
    "- Build a production chatbot\n",
    "- Explore model customization with Modelfiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}