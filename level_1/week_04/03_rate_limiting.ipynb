{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Part 03: Rate limiting + graceful degradation\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You can explain what a 429 means and how clients should respond.\n",
    "- You can parse `Retry-After` and decide whether to wait or degrade.\n",
    "- You can implement a deterministic degradation strategy (shrink prompt, fallback model).\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should be able to:\n",
    "\n",
    "- show a `decide_on_429(...)` output for at least 2 cases\n",
    "- show a degraded prompt that respects `max_chars`\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand rate limiting as a capacity allocation policy\n",
    "- Handle HTTP 429 (Retry-After, backoff, retry)\n",
    "- Implement a small local token bucket limiter\n",
    "- Practice graceful degradation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5917ef",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Rate limits protect providers and enforce fair usage.\n",
    "\n",
    "Your client should behave gracefully:\n",
    "\n",
    "- pause and retry\n",
    "- or degrade (fallback model, smaller prompt, cached response)\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: rate limiting is a capacity allocation policy\n",
    "\n",
    "You can think of a provider as having finite capacity. Rate limiting enforces a maximum request rate per user.\n",
    "\n",
    "A common conceptual model is a token bucket:\n",
    "\n",
    "- bucket capacity $B$\n",
    "- tokens refill at rate $r$ tokens/second\n",
    "- each request spends tokens\n",
    "\n",
    "If there are not enough tokens, requests are rejected or delayed.\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- bursts may succeed, but sustained high QPS will hit limits\n",
    "- your client must treat 429s as normal and recover gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882e6b1",
   "metadata": {},
   "source": [
    "## HTTP 429\n",
    "\n",
    "429 means “Too Many Requests”.\n",
    "\n",
    "Client behavior:\n",
    "\n",
    "- respect the `Retry-After` header if present\n",
    "- otherwise backoff and retry\n",
    "\n",
    "Graceful degradation options (choose based on your product):\n",
    "\n",
    "- return a clear “busy, try later” message\n",
    "- fall back to a cheaper/faster model\n",
    "- reduce prompt size / requested output length\n",
    "- serve a cached result if correctness allows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bac34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def parse_retry_after_seconds(value: str) -> Optional[float]:\n",
    "    \"\"\"Parse Retry-After header.\n",
    "\n",
    "    For Level 1, we support the most common form: integer seconds.\n",
    "    (HTTP also allows an HTTP-date; production clients should support both.)\n",
    "    \"\"\"\n",
    "    v = value.strip()\n",
    "    if not v:\n",
    "        return None\n",
    "    try:\n",
    "        seconds = int(v)\n",
    "        return max(0.0, float(seconds))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(parse_retry_after_seconds(\"2\"))\n",
    "print(parse_retry_after_seconds(\"0\"))\n",
    "print(parse_retry_after_seconds(\"\"))\n",
    "print(parse_retry_after_seconds(\"Wed, 21 Oct 2015 07:28:00 GMT\"))  # unsupported here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenBucket:\n",
    "    capacity: float\n",
    "    refill_per_s: float\n",
    "    tokens: float\n",
    "    last_refill_s: float\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *, capacity: float, refill_per_s: float) -> \"TokenBucket\":\n",
    "        now = time.time()\n",
    "        return cls(capacity=capacity, refill_per_s=refill_per_s, tokens=capacity, last_refill_s=now)\n",
    "\n",
    "    def _refill(self) -> None:\n",
    "        now = time.time()\n",
    "        dt = max(0.0, now - self.last_refill_s)\n",
    "        self.tokens = min(self.capacity, self.tokens + dt * self.refill_per_s)\n",
    "        self.last_refill_s = now\n",
    "\n",
    "    def allow(self, cost: float = 1.0) -> bool:\n",
    "        self._refill()\n",
    "        if self.tokens >= cost:\n",
    "            self.tokens -= cost\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "bucket = TokenBucket.create(capacity=5, refill_per_s=1.0)\n",
    "\n",
    "for i in range(12):\n",
    "    ok = bucket.allow(cost=1.0)\n",
    "    print(f\"i={i:02d} allowed={ok} tokens_left={bucket.tokens:.2f}\")\n",
    "    time.sleep(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc18f1",
   "metadata": {},
   "source": [
    "## Graceful degradation (what to do when limited)\n",
    "\n",
    "When you hit rate limits, you typically have two broad options:\n",
    "\n",
    "- **Wait and retry** (respecting `Retry-After` if provided)\n",
    "- **Degrade** (return something cheaper/faster or less precise)\n",
    "\n",
    "Common degradation choices:\n",
    "\n",
    "- Return a clear “busy, try later” message\n",
    "- Fall back to a cheaper/faster model\n",
    "- Reduce prompt size (trim history, remove low-value context)\n",
    "- Reduce requested output length\n",
    "- Serve a cached result (only if acceptable for correctness)\n",
    "\n",
    "The correct choice depends on your product:\n",
    "\n",
    "- For interactive UX, a fast partial answer may be better than waiting.\n",
    "- For offline pipelines, retrying may be preferable to degrading quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RateLimitResponse:\n",
    "    status_code: int\n",
    "    retry_after: Optional[str] = None\n",
    "\n",
    "\n",
    "def decide_on_429(\n",
    "    resp: RateLimitResponse,\n",
    "    *,\n",
    "    max_wait_s: float,\n",
    ") -> Tuple[str, Optional[float]]:\n",
    "    \"\"\"Return (action, wait_seconds).\n",
    "\n",
    "    action is one of: \"wait\", \"degrade\".\n",
    "    \"\"\"\n",
    "    if resp.retry_after is not None:\n",
    "        s = parse_retry_after_seconds(resp.retry_after)\n",
    "        if s is not None and s <= max_wait_s:\n",
    "            return (\"wait\", s)\n",
    "    return (\"degrade\", None)\n",
    "\n",
    "\n",
    "print(decide_on_429(RateLimitResponse(429, retry_after=\"2\"), max_wait_s=5.0))\n",
    "print(decide_on_429(RateLimitResponse(429, retry_after=\"60\"), max_wait_s=5.0))\n",
    "print(decide_on_429(RateLimitResponse(429, retry_after=None), max_wait_s=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_request(prompt: str, *, max_chars: int) -> str:\n",
    "    # TODO: implement a deterministic prompt shrinking strategy.\n",
    "    # Requirements:\n",
    "    # - Never exceed max_chars.\n",
    "    # - Preserve the beginning of the prompt (often contains instructions).\n",
    "    # - Consider preserving the end too (often contains the latest user input).\n",
    "    if max_chars <= 0:\n",
    "        return \"\"\n",
    "    if len(prompt) <= max_chars:\n",
    "        return prompt\n",
    "\n",
    "    head = max(0, max_chars // 2)\n",
    "    tail = max_chars - head\n",
    "    if tail <= 0:\n",
    "        return prompt[:max_chars]\n",
    "    return prompt[:head] + prompt[-tail:]\n",
    "\n",
    "\n",
    "def choose_fallback_model(primary: str) -> str:\n",
    "    # TODO: implement a simple fallback mapping.\n",
    "    # Example behavior:\n",
    "    # - if primary is \"gpt-4\", fallback to \"gpt-4-mini\"\n",
    "    # - otherwise fallback to a safe default\n",
    "    mapping = {\n",
    "        \"gpt-4\": \"gpt-4-mini\",\n",
    "        \"gpt-4o\": \"gpt-4o-mini\",\n",
    "    }\n",
    "    return mapping.get(primary, \"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "print(\"degraded:\", degrade_request(\"INSTRUCTIONS...\" + (\"x\" * 200) + \"...LATEST\", max_chars=60))\n",
    "print(\"fallback:\", choose_fallback_model(\"gpt-4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466a40b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- HTTP 429: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff2954",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for `degrade_request` and `choose_fallback_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_request(prompt: str, *, max_chars: int) -> str:\n",
    "    if max_chars <= 0:\n",
    "        return \"\"\n",
    "    if len(prompt) <= max_chars:\n",
    "        return prompt\n",
    "\n",
    "    if max_chars <= 10:\n",
    "        return prompt[:max_chars]\n",
    "\n",
    "    keep_head = max(1, int(max_chars * 0.6))\n",
    "    keep_tail = max_chars - keep_head\n",
    "    return prompt[:keep_head] + prompt[-keep_tail:]\n",
    "\n",
    "\n",
    "def choose_fallback_model(primary: str) -> str:\n",
    "    mapping = {\n",
    "        \"gpt-4\": \"gpt-4-mini\",\n",
    "        \"gpt-4o\": \"gpt-4o-mini\",\n",
    "        \"claude-3-opus\": \"claude-3-sonnet\",\n",
    "    }\n",
    "    return mapping.get(primary, \"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "print(\"degraded:\", degrade_request(\"INSTRUCTIONS...\" + (\"x\" * 200) + \"...LATEST\", max_chars=60))\n",
    "print(\"fallback:\", choose_fallback_model(\"gpt-4\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
