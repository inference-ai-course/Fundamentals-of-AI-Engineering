{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 1 â€” Week 4 Practice (Starter Notebook)\n",
        "\n",
        "Starter LLM client skeleton: timeouts, retries, caching, and logs.\n",
        "\n",
        "## References (docs)\n",
        "- `requests` timeouts: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts\n",
        "- Tenacity: https://tenacity.readthedocs.io/\n",
        "- Python `logging`: https://docs.python.org/3/library/logging.html\n",
        "- HTTP 429 Too Many Requests (MDN): https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n",
        "- Twelve-Factor App: https://12factor.net/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "This notebook demonstrates patterns without requiring a real API key.\n",
        "Replace `fake_provider_call(...)` with a real provider call later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict\n",
        "\n",
        "import requests\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s %(message)s')\n",
        "logger = logging.getLogger('llm_client')\n",
        "\n",
        "CACHE: Dict[str, Any] = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stable cache keys\n",
        "\n",
        "Cache keys should include everything that changes output: model, prompt, temperature, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_cache_key(payload: dict) -> str:\n",
        "    raw = json.dumps(payload, sort_keys=True, ensure_ascii=False).encode('utf-8')\n",
        "    return hashlib.sha256(raw).hexdigest()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Provider call stub\n",
        "\n",
        "Simulate transient failures so you can test retries/timeouts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fake_provider_call(payload: dict, timeout_s: float) -> dict:\n",
        "    if payload.get('force_error'):\n",
        "        raise requests.Timeout('Simulated timeout')\n",
        "    time.sleep(0.05)\n",
        "    return {\n",
        "        'text': 'echo: ' + str(payload.get('prompt', '')) ,\n",
        "        'model': payload.get('model', 'fake'),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM client skeleton\n",
        "\n",
        "Implements timeout + retry/backoff + caching + logs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class LLMConfig:\n",
        "    model: str = 'fake-model'\n",
        "    timeout_s: float = 10.0\n",
        "    max_retries: int = 3\n",
        "\n",
        "cfg = LLMConfig()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def llm_call(prompt: str, *, config: LLMConfig, force_error: bool = False) -> dict:\n",
        "    payload = {\n",
        "        'model': config.model,\n",
        "        'prompt': prompt,\n",
        "        'force_error': force_error,\n",
        "    }\n",
        "    cache_key = make_cache_key(payload)\n",
        "    if cache_key in CACHE:\n",
        "        logger.info('cache_hit')\n",
        "        return CACHE[cache_key]\n",
        "\n",
        "    @retry(stop=stop_after_attempt(config.max_retries), wait=wait_exponential(multiplier=0.5, min=0.5, max=4.0))\n",
        "    def _call_once():\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            return fake_provider_call(payload, timeout_s=config.timeout_s)\n",
        "        finally:\n",
        "            logger.info('latency_ms=%s' % int((time.time()-t0)*1000))\n",
        "\n",
        "    resp = _call_once()\n",
        "    CACHE[cache_key] = resp\n",
        "    return resp\n",
        "\n",
        "llm_call('hello', config=cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "Replace stub with real provider call, then add parsing/validation (Week 3).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
