{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Part 04: Caching and observability (logging)\n",
    "\n",
    "**Estimated time:** 75–120 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Design safe cache keys for pure-ish LLM calls\n",
    "- Implement an in-memory cache and a simple file cache\n",
    "- Add minimum viable request logging (request id, latency, success/failure, attempt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061e502",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Two practical realities of LLM APIs:\n",
    "\n",
    "- calls can be expensive\n",
    "- failures are hard to debug without logs\n",
    "\n",
    "Caching reduces cost/latency.\n",
    "\n",
    "Logging makes failures diagnosable.\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: caching is memoization of a pure-ish function\n",
    "\n",
    "If your LLM call were a pure function:\n",
    "\n",
    "$$\n",
    " y = f(x)\n",
    "$$\n",
    "\n",
    "then caching would be memoization: store $f(x)$ so repeated calls return instantly.\n",
    "\n",
    "LLM calls are only “pure-ish” because settings affect output. So your cache key must include **every input that can change the result**.\n",
    "\n",
    "Practical implication: incorrect cache keys cause **silent wrong answers**, which are worse than visible failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e70112",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "Cache when:\n",
    "\n",
    "- the same request repeats\n",
    "- you are iterating on downstream code\n",
    "\n",
    "Cache key must include everything that changes output:\n",
    "\n",
    "- model name\n",
    "- system prompt\n",
    "- user prompt\n",
    "- temperature\n",
    "\n",
    "Common cache pitfalls:\n",
    "\n",
    "- forgetting system prompt / tool context in the key\n",
    "- caching when temperature is high (outputs are intentionally stochastic)\n",
    "- caching errors (you accidentally “remember” a failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LLMRequest:\n",
    "    model: str\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "    temperature: float = 0.0\n",
    "\n",
    "\n",
    "def make_cache_key(req: LLMRequest) -> str:\n",
    "    # Key must include every field that can change the output.\n",
    "    raw = json.dumps(asdict(req), sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "req = LLMRequest(model=\"demo\", system_prompt=\"You are helpful.\", user_prompt=\"Hello\", temperature=0.0)\n",
    "print(\"cache_key=\", make_cache_key(req)[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMemoryCache:\n",
    "    def __init__(self) -> None:\n",
    "        self._store: dict[str, str] = {}\n",
    "\n",
    "    def get(self, key: str) -> str | None:\n",
    "        return self._store.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._store[key] = value\n",
    "\n",
    "\n",
    "cache = SimpleMemoryCache()\n",
    "key = \"k1\"\n",
    "print(cache.get(key))\n",
    "cache.set(key, \"value\")\n",
    "print(cache.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFileCache:\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        self.path = path\n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.path.exists():\n",
    "            self.path.write_text(\"{}\", encoding=\"utf-8\")\n",
    "\n",
    "    def _read(self) -> dict[str, str]:\n",
    "        return json.loads(self.path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    def _write(self, data: dict[str, str]) -> None:\n",
    "        self.path.write_text(json.dumps(data, ensure_ascii=False, sort_keys=True, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    def get(self, key: str) -> str | None:\n",
    "        data = self._read()\n",
    "        return data.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        data = self._read()\n",
    "        data[key] = value\n",
    "        self._write(data)\n",
    "\n",
    "\n",
    "file_cache = SimpleFileCache(Path(\"output/cache/llm_cache.json\"))\n",
    "k = \"demo\"\n",
    "print(file_cache.get(k))\n",
    "file_cache.set(k, \"hello\")\n",
    "print(file_cache.get(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe37600",
   "metadata": {},
   "source": [
    "## Logging (minimum viable request log)\n",
    "\n",
    "A minimal request log should include:\n",
    "\n",
    "- request id\n",
    "- model\n",
    "- latency\n",
    "- success/failure\n",
    "- failure location (network vs parsing vs validation)\n",
    "\n",
    "Two extra fields that help later:\n",
    "\n",
    "- prompt length (or token estimate)\n",
    "- retry attempt count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9282397",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"demo\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def fake_llm_call(text: str) -> str:\n",
    "    time.sleep(0.05)\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "def logged_call(request_id: str, req: LLMRequest) -> str:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        out = fake_llm_call(req.user_prompt)\n",
    "        logger.info(\n",
    "            \"llm_call_ok\",\n",
    "            extra={\n",
    "                \"request_id\": request_id,\n",
    "                \"model\": req.model,\n",
    "                \"latency_s\": time.time() - t0,\n",
    "                \"prompt_len\": len(req.system_prompt) + len(req.user_prompt),\n",
    "                \"attempt\": 0,\n",
    "            },\n",
    "        )\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        logger.warning(\n",
    "            \"llm_call_failed\",\n",
    "            extra={\n",
    "                \"request_id\": request_id,\n",
    "                \"model\": req.model,\n",
    "                \"latency_s\": time.time() - t0,\n",
    "                \"attempt\": 0,\n",
    "                \"error_type\": type(e).__name__,\n",
    "            },\n",
    "        )\n",
    "        raise\n",
    "\n",
    "\n",
    "print(logged_call(\"req_001\", req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5cf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_call(cache_obj: SimpleMemoryCache, req: LLMRequest) -> str:\n",
    "    key = make_cache_key(req)\n",
    "    hit = cache_obj.get(key)\n",
    "    if hit is not None:\n",
    "        logger.info(\"llm_cache_hit\", extra={\"model\": req.model})\n",
    "        return hit\n",
    "\n",
    "    out = fake_llm_call(req.user_prompt)\n",
    "    cache_obj.set(key, out)\n",
    "    logger.info(\"llm_cache_set\", extra={\"model\": req.model})\n",
    "    return out\n",
    "\n",
    "\n",
    "cache2 = SimpleMemoryCache()\n",
    "print(cached_call(cache2, req))\n",
    "print(cached_call(cache2, req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cache_key_todo(req: LLMRequest) -> str:\n",
    "    # TODO: extend the key so it would remain correct if you add fields like:\n",
    "    # - top_p\n",
    "    # - max_tokens\n",
    "    # - tool schema / tool definitions\n",
    "    # - few-shot examples\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def should_cache(req: LLMRequest) -> bool:\n",
    "    # TODO: implement policy, e.g.\n",
    "    # - cache only if temperature == 0.0\n",
    "    # - avoid caching very large prompts\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "print(\"Implement make_cache_key_todo() and should_cache().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3488d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- `functools.lru_cache`: https://docs.python.org/3/library/functools.html#functools.lru_cache\n",
    "- Python logging: https://docs.python.org/3/library/logging.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
