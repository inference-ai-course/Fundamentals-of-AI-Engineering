{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Part 04: Caching and observability (logging)\n",
    "\n",
    "**Estimated time:** 75–120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Level 0)\n",
    "\n",
    "Level 1 assumes Level 0 is complete. If you need a refresher on production constraints, debugging/observability habits:\n",
    "\n",
    "- [Level 1 Pre-study index](../PRESTUDY.md)\n",
    "- [Level 0 — Chapter 5: Resource Monitoring and Containerization](../../level_0/Chapters/5/Chapter5.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 04)\n",
    "\n",
    "- You can construct a cache key that changes when any relevant request field changes.\n",
    "- You can demonstrate a cache hit vs cache miss.\n",
    "- You can emit a minimal request log line with request id, model, latency, and success/failure.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should be able to:\n",
    "\n",
    "- show one cache hit log event\n",
    "- show at least one request log line that includes request id + latency\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Design safe cache keys for pure-ish LLM calls\n",
    "- Implement an in-memory cache and a simple file cache\n",
    "- Add minimum viable request logging (request id, latency, success/failure, attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061e502",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Two practical realities of LLM APIs:\n",
    "\n",
    "- calls can be expensive\n",
    "- failures are hard to debug without logs\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- build a safe cache key (include every field that can change output)\n",
    "- demonstrate cache hit vs miss\n",
    "- emit a minimal request log line (request_id, latency, success/failure)\n",
    "\n",
    "If you want the deeper caching theory, use the Level 0 links at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e70112",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "Cache when:\n",
    "\n",
    "- the same request repeats\n",
    "- you are iterating on downstream code\n",
    "\n",
    "Cache key must include everything that changes output:\n",
    "\n",
    "- model name\n",
    "- system prompt\n",
    "- user prompt\n",
    "- temperature\n",
    "\n",
    "Common cache pitfalls:\n",
    "\n",
    "- forgetting system prompt / tool context in the key\n",
    "- caching when temperature is high (outputs are intentionally stochastic)\n",
    "- caching errors (you accidentally “remember” a failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadcc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from dataclasses import asdict, dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LLMRequest:\n",
    "    model: str\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "    temperature: float = 0.0\n",
    "\n",
    "\n",
    "def make_cache_key(req: LLMRequest) -> str:\n",
    "    # Key must include every field that can change the output.\n",
    "    raw = json.dumps(asdict(req), sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "req = LLMRequest(model=\"demo\", system_prompt=\"You are helpful.\", user_prompt=\"Hello\", temperature=0.0)\n",
    "print(\"cache_key=\", make_cache_key(req)[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class SimpleMemoryCache:\n",
    "    def __init__(self) -> None:\n",
    "        self._store = {}\n",
    "\n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        return self._store.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._store[key] = value\n",
    "\n",
    "\n",
    "cache = SimpleMemoryCache()\n",
    "key = \"k1\"\n",
    "print(cache.get(key))\n",
    "cache.set(key, \"value\")\n",
    "print(cache.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "class SimpleFileCache:\n",
    "    def __init__(self, path: Path) -> None:\n",
    "        self.path = path\n",
    "        self.path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not self.path.exists():\n",
    "            self.path.write_text(\"{}\", encoding=\"utf-8\")\n",
    "\n",
    "    def _read(self) -> Dict[str, str]:\n",
    "        return json.loads(self.path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    def _write(self, data: Dict[str, str]) -> None:\n",
    "        self.path.write_text(json.dumps(data, ensure_ascii=False, sort_keys=True, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        data = self._read()\n",
    "        return data.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        data = self._read()\n",
    "        data[key] = value\n",
    "        self._write(data)\n",
    "\n",
    "\n",
    "file_cache = SimpleFileCache(Path(\"output/cache/llm_cache.json\"))\n",
    "k = \"demo\"\n",
    "print(file_cache.get(k))\n",
    "file_cache.set(k, \"hello\")\n",
    "print(file_cache.get(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe37600",
   "metadata": {},
   "source": [
    "## Logging (minimum viable request log)\n",
    "\n",
    "A minimal request log should include:\n",
    "\n",
    "- request id\n",
    "- model\n",
    "- latency\n",
    "- success/failure\n",
    "- failure location (network vs parsing vs validation)\n",
    "\n",
    "Two extra fields that help later:\n",
    "\n",
    "- prompt length (or token estimate)\n",
    "- retry attempt count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9282397",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"demo\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def fake_llm_call(text: str) -> str:\n",
    "    time.sleep(0.05)\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "def logged_call(request_id: str, req: LLMRequest) -> str:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        out = fake_llm_call(req.user_prompt)\n",
    "        logger.info(\n",
    "            \"llm_call_ok\",\n",
    "            extra={\n",
    "                \"request_id\": request_id,\n",
    "                \"model\": req.model,\n",
    "                \"latency_s\": time.time() - t0,\n",
    "                \"prompt_len\": len(req.system_prompt) + len(req.user_prompt),\n",
    "                \"attempt\": 0,\n",
    "            },\n",
    "        )\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        logger.warning(\n",
    "            \"llm_call_failed\",\n",
    "            extra={\n",
    "                \"request_id\": request_id,\n",
    "                \"model\": req.model,\n",
    "                \"latency_s\": time.time() - t0,\n",
    "                \"attempt\": 0,\n",
    "                \"error_type\": type(e).__name__,\n",
    "            },\n",
    "        )\n",
    "        raise\n",
    "\n",
    "\n",
    "print(logged_call(\"req_001\", req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5cf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cached_call(cache_obj: SimpleMemoryCache, req: LLMRequest) -> str:\n",
    "    key = make_cache_key(req)\n",
    "    hit = cache_obj.get(key)\n",
    "    if hit is not None:\n",
    "        logger.info(\"llm_cache_hit\", extra={\"model\": req.model})\n",
    "        return hit\n",
    "\n",
    "    out = fake_llm_call(req.user_prompt)\n",
    "    cache_obj.set(key, out)\n",
    "    logger.info(\"llm_cache_set\", extra={\"model\": req.model})\n",
    "    return out\n",
    "\n",
    "\n",
    "cache2 = SimpleMemoryCache()\n",
    "print(cached_call(cache2, req))\n",
    "print(cached_call(cache2, req))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cache_key_todo(req: LLMRequest) -> str:\n",
    "    # TODO: extend the key so it would remain correct if you add fields like:\n",
    "    # - top_p\n",
    "    # - max_tokens\n",
    "    # - tool schema / tool definitions\n",
    "    # - few-shot examples\n",
    "    return make_cache_key(req)\n",
    "\n",
    "\n",
    "def should_cache(req: LLMRequest) -> bool:\n",
    "    # TODO: implement policy, e.g.\n",
    "    # - cache only if temperature == 0.0\n",
    "    # - avoid caching very large prompts\n",
    "    return req.temperature == 0.0 and (len(req.system_prompt) + len(req.user_prompt)) <= 10_000\n",
    "\n",
    "\n",
    "print(\"make_cache_key_todo:\", make_cache_key_todo(req)[:12])\n",
    "print(\"should_cache:\", should_cache(req))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3488d",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- `functools.lru_cache`: https://docs.python.org/3/library/functools.html#functools.lru_cache\n",
    "- Python logging: https://docs.python.org/3/library/logging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086885d",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for `make_cache_key_todo` and `should_cache`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b32c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cache_key_todo(req: LLMRequest) -> str:\n",
    "    # Safe approach: serialize all known fields deterministically.\n",
    "    raw = json.dumps(asdict(req), sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def should_cache(req: LLMRequest) -> bool:\n",
    "    # Conservative policy for reproducibility + correctness.\n",
    "    if req.temperature != 0.0:\n",
    "        return False\n",
    "    prompt_len = len(req.system_prompt) + len(req.user_prompt)\n",
    "    return prompt_len <= 10_000\n",
    "\n",
    "\n",
    "print(\"solution key:\", make_cache_key_todo(req)[:12])\n",
    "print(\"solution should_cache:\", should_cache(req))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
