{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 — Part 05: A reusable `llm_client.py` skeleton\n",
    "\n",
    "**Estimated time:** 90–150 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the client module as a reliability boundary\n",
    "- Define a provider-agnostic request payload + stable cache key\n",
    "- Implement a minimal `LLMClient` skeleton (timeouts, retries, caching, logging)\n",
    "- Leave clear extension points for provider-specific calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36e3aa",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Your goal is a single module you can reuse across projects that provides:\n",
    "\n",
    "- timeouts\n",
    "- retries + backoff\n",
    "- basic rate limit handling\n",
    "- basic caching\n",
    "- logging\n",
    "\n",
    "This is a Level 1 skeleton (provider-agnostic). You can adapt it to OpenAI/Anthropic/etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: the client is a reliability boundary\n",
    "\n",
    "Your application code wants a simple contract:\n",
    "\n",
    "- “given a request, return text or raise a clear error”\n",
    "\n",
    "The client module enforces reliability invariants:\n",
    "\n",
    "- bounded waiting (timeouts)\n",
    "- bounded retries (caps)\n",
    "- debuggability (logs with request id / attempt)\n",
    "- cost control (caching)\n",
    "\n",
    "Keeping these concerns centralized prevents every script from reinventing them inconsistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0ef1d",
   "metadata": {},
   "source": [
    "## Skeleton design\n",
    "\n",
    "We’ll define:\n",
    "\n",
    "- a request payload (model + prompt + settings)\n",
    "- a stable cache key\n",
    "- a `call()` method\n",
    "\n",
    "The cache key must represent the “effective input” to the model. If two requests differ in any setting that can change output, they must not share a key.\n",
    "\n",
    "Next you’ll implement a provider-agnostic skeleton you can adapt later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56bc70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import uuid\n",
    "from dataclasses import asdict, dataclass\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LLMRequest:\n",
    "    model: str\n",
    "    prompt: str\n",
    "    temperature: float = 0.0\n",
    "\n",
    "\n",
    "def make_cache_key(req: LLMRequest) -> str:\n",
    "    raw = json.dumps(asdict(req), sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "class SimpleMemoryCache:\n",
    "    def __init__(self) -> None:\n",
    "        self._store: dict[str, str] = {}\n",
    "\n",
    "    def get(self, key: str) -> str | None:\n",
    "        return self._store.get(key)\n",
    "\n",
    "    def set(self, key: str, value: str) -> None:\n",
    "        self._store[key] = value\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, cache: SimpleMemoryCache | None = None) -> None:\n",
    "        self._cache = cache or SimpleMemoryCache()\n",
    "\n",
    "    def _provider_call(self, req: LLMRequest, *, timeout_s: float) -> str:\n",
    "        # TODO: Implement provider-specific HTTP/API call.\n",
    "        # This method must:\n",
    "        # - respect timeout_s\n",
    "        # - raise clear exceptions for retry classification\n",
    "        raise NotImplementedError(\"Implement provider-specific call\")\n",
    "\n",
    "    def call(self, req: LLMRequest, *, timeout_s: float = 30.0, max_retries: int = 2) -> str:\n",
    "        request_id = str(uuid.uuid4())\n",
    "        cache_key = make_cache_key(req)\n",
    "\n",
    "        cached = self._cache.get(cache_key)\n",
    "        if cached is not None:\n",
    "            logger.info(\"llm_cache_hit\", extra={\"request_id\": request_id, \"model\": req.model})\n",
    "            return cached\n",
    "\n",
    "        last_err: Exception | None = None\n",
    "        for attempt in range(max_retries + 1):\n",
    "            t0 = time.time()\n",
    "            try:\n",
    "                text = self._provider_call(req, timeout_s=timeout_s)\n",
    "                logger.info(\n",
    "                    \"llm_call_ok\",\n",
    "                    extra={\"request_id\": request_id, \"model\": req.model, \"latency_s\": time.time() - t0, \"attempt\": attempt},\n",
    "                )\n",
    "                self._cache.set(cache_key, text)\n",
    "                return text\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                logger.warning(\n",
    "                    \"llm_call_failed\",\n",
    "                    extra={\"request_id\": request_id, \"model\": req.model, \"latency_s\": time.time() - t0, \"attempt\": attempt, \"error_type\": type(e).__name__},\n",
    "                )\n",
    "                if attempt < max_retries:\n",
    "                    time.sleep(min(2 ** attempt, 4))\n",
    "\n",
    "        raise RuntimeError(f\"LLM call failed after retries: {last_err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8b83b",
   "metadata": {},
   "source": [
    "## Practice exercises\n",
    "\n",
    "1) Extend `LLMRequest` to include `system_prompt` and update `make_cache_key()` accordingly.\n",
    "\n",
    "2) Add jitter to the backoff so many clients do not retry at the same times.\n",
    "\n",
    "3) Add a simple 429 handler:\n",
    "\n",
    "- if `Retry-After` is present and small enough, sleep that long\n",
    "- otherwise backoff\n",
    "\n",
    "4) Add structured output validation (from Week 3) as an optional mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_jitter(delay_s: float) -> float:\n",
    "    # TODO: implement jitter.\n",
    "    # Example: \"full jitter\" uniform(0, delay_s).\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def backoff_delay(attempt: int, *, base: float = 0.5, cap: float = 8.0) -> float:\n",
    "    # TODO: implement exponential backoff with cap.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "print(\"Implement add_jitter() and backoff_delay().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fd77e",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Implement `_provider_call()` using your chosen provider SDK.\n",
    "- Add structured output validation from Week 3.\n",
    "- Use this client in later pipeline/capstone work.\n",
    "\n",
    "## References\n",
    "\n",
    "- Python logging: https://docs.python.org/3/library/logging.html\n",
    "- Tenacity (for more robust retries): https://tenacity.readthedocs.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
