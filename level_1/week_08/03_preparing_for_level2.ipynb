{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 — Part 03: Preparing for Level 2 (what changes)\n",
    "\n",
    "**Estimated time:** 30–45 minutes\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You can describe at least 3 ways Level 2 differs from Level 1.\n",
    "- You can name at least 2 new failure surfaces (e.g., retrieval quality, prompt injection).\n",
    "- You write a short readiness checklist artifact under `output/`.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should have:\n",
    "\n",
    "- `output/LEVEL2_SELF_CHECK.md`\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Identify the shift from scripts to systems in Level 2\n",
    "- Understand new failure surfaces (retrieval, evaluation, agents)\n",
    "- Capture practical mindset shifts for Level 2 work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c55b96",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Level 1 is mostly:\n",
    "\n",
    "- a single-project pipeline\n",
    "- mostly offline, script-based\n",
    "- focusing on reproducibility and reliability basics\n",
    "\n",
    "Level 2 shifts toward **systems thinking**:\n",
    "\n",
    "- retrieval (RAG)\n",
    "- evaluation loops\n",
    "- multi-step agent workflows\n",
    "- knowledge bases\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: Level 2 adds feedback loops and new failure surfaces\n",
    "\n",
    "In Level 1, many workflows are “run once and inspect outputs”.\n",
    "\n",
    "In Level 2, you build systems with feedback loops:\n",
    "\n",
    "- retrieval quality affects generation quality\n",
    "- evaluation metrics guide iteration\n",
    "- multi-step workflows introduce compounding failure probability\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- you need observability (traces/logs) to debug why a system answered\n",
    "- you need eval sets to prevent “prompt overfitting”\n",
    "- you need trust boundaries to resist prompt injection when external data is involved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e890c",
   "metadata": {},
   "source": [
    "## Practical mindset shifts\n",
    "\n",
    "- From “one script works” → “the system is observable and testable”.\n",
    "- From “prompting” → “prompt + retrieval + evaluation”.\n",
    "- From “manual checking” → “repeatable eval sets”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def level2_self_check_todo() -> List[str]:\n",
    "    # TODO: add your own readiness checklist.\n",
    "    return [\n",
    "        \"<todo: I can explain what RAG is and why it helps>\",\n",
    "        \"<todo: I know how to build a small evaluation set>\",\n",
    "        \"<todo: I understand prompt injection risk in retrieval systems>\",\n",
    "    ]\n",
    "\n",
    "\n",
    "items = level2_self_check_todo()\n",
    "\n",
    "out_path = OUTPUT_DIR / \"LEVEL2_SELF_CHECK.md\"\n",
    "out_path.write_text(\"\\n\".join([\"# Level 2 Self-Check\", \"\"] + [\"- \" + x for x in items]) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"Level 2 self-check:\")\n",
    "for item in items:\n",
    "    print(\"-\", item)\n",
    "print(\"wrote:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1eb00e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- RAG overview: https://www.pinecone.io/learn/retrieval-augmented-generation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc2408",
   "metadata": {},
   "source": [
    "## Self-check\n",
    "\n",
    "- Can you explain how retrieval quality affects generation quality?\n",
    "- Do you have a plan for eval sets and metrics?\n",
    "- Do you know how to handle prompt injection risks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14abc72",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `level2_self_check_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a70456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level2_self_check_todo() -> List[str]:\n",
    "    return [\n",
    "        \"I can explain what RAG is and why it helps.\",\n",
    "        \"I can describe a minimal chunking + embedding + retrieval pipeline.\",\n",
    "        \"I know how to build a small evaluation set and choose at least one metric.\",\n",
    "        \"I understand prompt injection risk and can name at least one mitigation.\",\n",
    "        \"I know why observability (logs/traces) matters for multi-step systems.\",\n",
    "    ]\n",
    "\n",
    "\n",
    "solution_path = OUTPUT_DIR / \"LEVEL2_SELF_CHECK_solution.md\"\n",
    "items = level2_self_check_todo()\n",
    "solution_path.write_text(\"\\n\".join([\"# Level 2 Self-Check\", \"\"] + [\"- \" + x for x in items]) + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"wrote:\", solution_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
