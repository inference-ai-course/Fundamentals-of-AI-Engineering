{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Part 03: Compare Runs and Report Lab\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Pre-study (Level 0)\n",
    "\n",
    "Level 1 assumes Level 0 is complete. If you need a refresher on evaluation metrics (accuracy/precision/recall/F1):\n",
    "\n",
    "- [Level 1 Pre-study index](../PRESTUDY.md)\n",
    "- [Level 0 — Evaluation metrics](../../level_0/Chapters/4/02_core_concepts.md)\n",
    "\n",
    "---\n",
    "\n",
    "## What success looks like (end of Part 03)\n",
    "\n",
    "- You can load or create a list of runs with consistent fields.\n",
    "- You can compute a summary (best run + average metric).\n",
    "- You write a report artifact under `output/compare_runs/`:\n",
    "  - `runs.json`\n",
    "  - `report.md`\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should be able to open:\n",
    "\n",
    "- `output/compare_runs/runs.json`\n",
    "- `output/compare_runs/report.md`\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Compare experiment runs using consistent metrics\n",
    "- Summarize best/worst runs with clear reasoning\n",
    "- Build a simple report artifact (JSON/Markdown)\n",
    "- Practice root-cause analysis across runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b45f3b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Comparing runs requires consistent fields and consistent artifacts.\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- load or create a small list of runs\n",
    "- select the best run using a clear rule\n",
    "- compute a summary\n",
    "- write report artifacts under `output/compare_runs/`\n",
    "\n",
    "If you need a refresher on evaluation metrics, use the Level 0 links at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "runs = [\n",
    "    {\"run_id\": \"run_001\", \"model\": \"logreg\", \"accuracy\": 0.84, \"f1\": 0.82, \"notes\": \"baseline\"},\n",
    "    {\"run_id\": \"run_002\", \"model\": \"logreg\", \"accuracy\": 0.87, \"f1\": 0.86, \"notes\": \"more iterations\"},\n",
    "    {\"run_id\": \"run_003\", \"model\": \"rf\", \"accuracy\": 0.89, \"f1\": 0.88, \"notes\": \"higher depth\"},\n",
    "]\n",
    "\n",
    "out_dir = Path(\"output/compare_runs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "(out_dir / \"runs.json\").write_text(json.dumps(runs, indent=2), encoding=\"utf-8\")\n",
    "print(\"wrote\", out_dir / \"runs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_run_todo(runs):\n",
    "    \"\"\"TODO: return the best run.\n",
    "\n",
    "    Criteria (suggested):\n",
    "\n",
    "    - highest accuracy\n",
    "    - tie-break: highest f1\n",
    "    \"\"\"\n",
    "    return runs[0]\n",
    "\n",
    "\n",
    "def summarize_runs_todo(runs):\n",
    "    \"\"\"TODO: return a small summary dict used for reporting.\"\"\"\n",
    "    best = select_best_run_todo(runs)\n",
    "    avg_acc = sum(r[\"accuracy\"] for r in runs) / len(runs)\n",
    "    return {\"best\": best, \"avg_accuracy\": round(avg_acc, 3), \"n\": len(runs)}\n",
    "\n",
    "\n",
    "summary = summarize_runs_todo(runs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report_todo(path: Path, summary: dict) -> None:\n",
    "    \"\"\"TODO: write a markdown report.\n",
    "\n",
    "    Suggested sections:\n",
    "\n",
    "    - Total runs\n",
    "    - Average accuracy\n",
    "    - Best run (with run_id, model, metrics, notes)\n",
    "    \"\"\"\n",
    "    lines = [\"# Run Comparison Report\", \"\", f\"Total runs: {summary['n']}\"]\n",
    "    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "write_report_todo(out_dir / \"report.md\", summary)\n",
    "print(\"wrote\", out_dir / \"report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0f6eb",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementations for the TODO functions in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7c728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_run_todo(runs):\n",
    "    return max(runs, key=lambda r: (r[\"accuracy\"], r[\"f1\"]))\n",
    "\n",
    "\n",
    "def summarize_runs_todo(runs):\n",
    "    best = select_best_run_todo(runs)\n",
    "    avg_acc = sum(r[\"accuracy\"] for r in runs) / len(runs)\n",
    "    avg_f1 = sum(r[\"f1\"] for r in runs) / len(runs)\n",
    "    return {\n",
    "        \"best\": best,\n",
    "        \"avg_accuracy\": round(avg_acc, 3),\n",
    "        \"avg_f1\": round(avg_f1, 3),\n",
    "        \"n\": len(runs),\n",
    "    }\n",
    "\n",
    "\n",
    "def write_report_todo(path: Path, summary: dict) -> None:\n",
    "    lines = [\"# Run Comparison Report\", \"\"]\n",
    "    lines.append(f\"Total runs: {summary['n']}\")\n",
    "    lines.append(f\"Average accuracy: {summary['avg_accuracy']}\")\n",
    "    if \"avg_f1\" in summary:\n",
    "        lines.append(f\"Average f1: {summary['avg_f1']}\")\n",
    "    lines.append(\"\")\n",
    "\n",
    "    best = summary[\"best\"]\n",
    "    lines.append(\"## Best run\")\n",
    "    lines.append(f\"- run_id: {best['run_id']}\")\n",
    "    lines.append(f\"- model: {best['model']}\")\n",
    "    lines.append(f\"- accuracy: {best['accuracy']}\")\n",
    "    lines.append(f\"- f1: {best['f1']}\")\n",
    "    lines.append(f\"- notes: {best['notes']}\")\n",
    "\n",
    "    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "summary_solution = summarize_runs_todo(runs)\n",
    "write_report_todo(out_dir / \"report_solution.md\", summary_solution)\n",
    "print(\"wrote\", out_dir / \"report_solution.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
