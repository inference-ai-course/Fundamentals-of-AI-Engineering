{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Part 03: Compare Runs and Report Lab\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Compare experiment runs using consistent metrics\n",
    "- Summarize best/worst runs with clear reasoning\n",
    "- Build a simple report artifact (JSON/Markdown)\n",
    "- Practice root-cause analysis across runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b45f3b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Comparing runs requires consistent metrics and consistent artifacts. In this lab you'll load sample run data, compare metrics, and build a short report artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "runs = [\n",
    "    {\"run_id\": \"run_001\", \"model\": \"logreg\", \"accuracy\": 0.84, \"f1\": 0.82, \"notes\": \"baseline\"},\n",
    "    {\"run_id\": \"run_002\", \"model\": \"logreg\", \"accuracy\": 0.87, \"f1\": 0.86, \"notes\": \"more iterations\"},\n",
    "    {\"run_id\": \"run_003\", \"model\": \"rf\", \"accuracy\": 0.89, \"f1\": 0.88, \"notes\": \"higher depth\"},\n",
    "]\n",
    "\n",
    "out_dir = Path(\"output/compare_runs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "(out_dir / \"runs.json\").write_text(json.dumps(runs, indent=2), encoding=\"utf-8\")\n",
    "print(\"wrote\", out_dir / \"runs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_run(runs: list[dict]) -> dict:\n",
    "    return max(runs, key=lambda r: r[\"accuracy\"])\n",
    "\n",
    "\n",
    "def summarize_runs(runs: list[dict]) -> dict:\n",
    "    best = best_run(runs)\n",
    "    avg_acc = sum(r[\"accuracy\"] for r in runs) / len(runs)\n",
    "    return {\"best\": best, \"avg_accuracy\": round(avg_acc, 3), \"n\": len(runs)}\n",
    "\n",
    "\n",
    "summary = summarize_runs(runs)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report(path: Path, summary: dict) -> None:\n",
    "    lines = [\"# Run Comparison Report\", \"\"]\n",
    "    lines.append(f\"Total runs: {summary['n']}\")\n",
    "    lines.append(f\"Average accuracy: {summary['avg_accuracy']}\")\n",
    "    lines.append(\"\")\n",
    "    best = summary[\"best\"]\n",
    "    lines.append(\"## Best run\")\n",
    "    lines.append(f\"- run_id: {best['run_id']}\")\n",
    "    lines.append(f\"- model: {best['model']}\")\n",
    "    lines.append(f\"- accuracy: {best['accuracy']}\")\n",
    "    lines.append(f\"- f1: {best['f1']}\")\n",
    "    lines.append(f\"- notes: {best['notes']}\")\n",
    "    path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "write_report(out_dir / \"report.md\", summary)\n",
    "print(\"wrote\", out_dir / \"report.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
