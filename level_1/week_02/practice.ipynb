{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4fae7d5d",
      "metadata": {},
      "source": [
        "# Level 1 — Week 2 Practice (Starter Notebook)\n",
        "\n",
        "This notebook gives you starter code for the **ML training loop** using scikit-learn.\n",
        "\n",
        "## References (docs)\n",
        "- scikit-learn getting started: https://scikit-learn.org/stable/getting_started.html\n",
        "- scikit-learn train/test split: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "- scikit-learn model evaluation: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "- scikit-learn cross-validation concepts: https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "- F1 score (Wikipedia): https://en.wikipedia.org/wiki/F1_score\n",
        "- scikit-learn model persistence: https://scikit-learn.org/stable/model_persistence.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af577044",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "You should run this in an environment with `scikit-learn` installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9def234e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdd5c93",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = Path('output')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a8c6ea",
      "metadata": {},
      "source": [
        "## Load data\n",
        "\n",
        "We use Iris as a starter dataset. Replace it later with your own dataset as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e50279",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = load_iris(as_frame=True)\n",
        "X = data.data\n",
        "y = data.target\n",
        "X.head(), y.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798916ba",
      "metadata": {},
      "source": [
        "## Parameterize experiment config\n",
        "\n",
        "In your assignment, this becomes CLI args (e.g., `--seed`, `--model_type`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef03264",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    seed: int = 42\n",
        "    test_size: float = 0.2\n",
        "    max_iter: int = 200\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8d1296",
      "metadata": {},
      "source": [
        "## Split -> train -> evaluate\n",
        "\n",
        "Notes:\n",
        "- Use a fixed `random_state` for reproducibility.\n",
        "- Evaluate on the hold-out set (not training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e19c360",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=cfg.test_size, random_state=cfg.seed, stratify=y\n",
        ")\n",
        "\n",
        "model = LogisticRegression(max_iter=cfg.max_iter)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_val)\n",
        "acc = accuracy_score(y_val, pred)\n",
        "f1 = f1_score(y_val, pred, average='macro')\n",
        "\n",
        "acc, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1502b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_val, pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c91ba58f",
      "metadata": {},
      "source": [
        "## Save artifacts\n",
        "\n",
        "In a real project you should save:\n",
        "- model file\n",
        "- config used\n",
        "- metrics\n",
        "\n",
        "This is the minimum evidence that supports your report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71a096e",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'accuracy': float(acc),\n",
        "    'f1_macro': float(f1),\n",
        "}\n",
        "\n",
        "(OUTPUT_DIR / 'metrics.json').write_text(json.dumps(metrics, indent=2), encoding='utf-8')\n",
        "(OUTPUT_DIR / 'config.json').write_text(json.dumps(cfg.__dict__, indent=2), encoding='utf-8')\n",
        "\n",
        "# Optional: save model (requires joblib)\n",
        "try:\n",
        "    import joblib\n",
        "    joblib.dump(model, OUTPUT_DIR / 'model.joblib')\n",
        "    saved_model = True\n",
        "except ModuleNotFoundError:\n",
        "    saved_model = False\n",
        "\n",
        "metrics, cfg.__dict__, saved_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3c254b",
      "metadata": {},
      "source": [
        "## TODO: Compare two experiments\n",
        "\n",
        "- Change **one thing** (e.g., `max_iter`, solver, or model type).\n",
        "- Re-run and compare metrics.\n",
        "- Write a short `report.md`: what changed, what happened, why you think it happened, and what you’ll try next.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
