{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4fae7d5d",
      "metadata": {},
      "source": [
        "# Level 1 — Week 2 Practice (Starter Notebook)\n",
        "\n",
        "This notebook gives you starter code for the **ML training loop** using scikit-learn.\n",
        "\n",
        "## What success looks like (end of practice)\n",
        "\n",
        "- You ran at least 2 experiments (changing one variable).\n",
        "- You saved artifacts under `output/`:\n",
        "  - `metrics.json`\n",
        "  - `config.json`\n",
        "  - `report.md` (your comparison write-up)\n",
        "\n",
        "### Checkpoint\n",
        "\n",
        "After running the notebook, you should be able to open:\n",
        "\n",
        "- `output/metrics.json`\n",
        "- `output/config.json`\n",
        "- `output/report.md`\n",
        "\n",
        "## References (docs)\n",
        "- scikit-learn getting started: https://scikit-learn.org/stable/getting_started.html\n",
        "- scikit-learn train/test split: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "- scikit-learn model evaluation: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "- scikit-learn cross-validation concepts: https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "- F1 score (Wikipedia): https://en.wikipedia.org/wiki/F1_score\n",
        "- scikit-learn model persistence: https://scikit-learn.org/stable/model_persistence.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af577044",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "You should run this in an environment with `scikit-learn` installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9def234e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdd5c93",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = Path('output')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "OUTPUT_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a8c6ea",
      "metadata": {},
      "source": [
        "## Load data\n",
        "\n",
        "We use Iris as a starter dataset. Replace it later with your own dataset as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e50279",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = load_iris(as_frame=True)\n",
        "X = data.data\n",
        "y = data.target\n",
        "X.head(), y.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798916ba",
      "metadata": {},
      "source": [
        "## Parameterize experiment config\n",
        "\n",
        "In your assignment, this becomes CLI args (e.g., `--seed`, `--model_type`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef03264",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    seed: int = 42\n",
        "    test_size: float = 0.2\n",
        "    max_iter: int = 200\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8d1296",
      "metadata": {},
      "source": [
        "## Split -> train -> evaluate\n",
        "\n",
        "Notes:\n",
        "- Use a fixed `random_state` for reproducibility.\n",
        "- Evaluate on the hold-out set (not training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e19c360",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=cfg.test_size, random_state=cfg.seed, stratify=y\n",
        ")\n",
        "\n",
        "model = LogisticRegression(max_iter=cfg.max_iter)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_val)\n",
        "acc = accuracy_score(y_val, pred)\n",
        "f1 = f1_score(y_val, pred, average='macro')\n",
        "\n",
        "acc, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1502b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(y_val, pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c91ba58f",
      "metadata": {},
      "source": [
        "## Save artifacts\n",
        "\n",
        "In a real project you should save:\n",
        "- model file\n",
        "- config used\n",
        "- metrics\n",
        "\n",
        "This is the minimum evidence that supports your report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71a096e",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    'accuracy': float(acc),\n",
        "    'f1_macro': float(f1),\n",
        "}\n",
        "\n",
        "(OUTPUT_DIR / 'metrics.json').write_text(json.dumps(metrics, indent=2), encoding='utf-8')\n",
        "(OUTPUT_DIR / 'config.json').write_text(json.dumps(cfg.__dict__, indent=2), encoding='utf-8')\n",
        "\n",
        "# Optional: save model (requires joblib)\n",
        "try:\n",
        "    import joblib\n",
        "    joblib.dump(model, OUTPUT_DIR / 'model.joblib')\n",
        "    saved_model = True\n",
        "except ModuleNotFoundError:\n",
        "    saved_model = False\n",
        "\n",
        "metrics, cfg.__dict__, saved_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3c254b",
      "metadata": {},
      "source": [
        "## Exercise: Compare two experiments (TODO)\n",
        "\n",
        "Goal:\n",
        "\n",
        "- Run **two** experiments that differ by exactly one change.\n",
        "- Write a short `output/report.md` explaining:\n",
        "  - what changed\n",
        "  - what happened (metrics)\n",
        "  - what you think caused it\n",
        "  - what you’d try next\n",
        "\n",
        "Checkpoint:\n",
        "\n",
        "- `output/report.md` exists and mentions both experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28610f26",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(cfg: Config):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=cfg.test_size, random_state=cfg.seed, stratify=y\n",
        "    )\n",
        "    m = LogisticRegression(max_iter=cfg.max_iter)\n",
        "    m.fit(X_train, y_train)\n",
        "    pred = m.predict(X_val)\n",
        "\n",
        "    return {\n",
        "        \"config\": cfg.__dict__.copy(),\n",
        "        \"metrics\": {\n",
        "            \"accuracy\": float(accuracy_score(y_val, pred)),\n",
        "            \"f1_macro\": float(f1_score(y_val, pred, average=\"macro\")),\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "cfg_a = cfg\n",
        "cfg_b = Config(seed=cfg.seed, test_size=cfg.test_size, max_iter=cfg.max_iter * 2)\n",
        "\n",
        "run_a = run_experiment(cfg_a)\n",
        "run_b = run_experiment(cfg_b)\n",
        "\n",
        "report_md = \"\\n\".join(\n",
        "    [\n",
        "        \"# Experiment Comparison Report\",\n",
        "        \"\",\n",
        "        \"## What changed\",\n",
        "        \"TODO: Describe the one change you made (max_iter / solver / model type).\",\n",
        "        \"\",\n",
        "        \"## Results\",\n",
        "        f\"- Experiment A config: {run_a['config']}\",\n",
        "        f\"- Experiment A metrics: {run_a['metrics']}\",\n",
        "        f\"- Experiment B config: {run_b['config']}\",\n",
        "        f\"- Experiment B metrics: {run_b['metrics']}\",\n",
        "        \"\",\n",
        "        \"## Why you think it happened\",\n",
        "        \"TODO: Write 2-5 sentences.\",\n",
        "        \"\",\n",
        "        \"## Next experiment\",\n",
        "        \"TODO: What will you try next?\",\n",
        "        \"\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "(OUTPUT_DIR / \"report.md\").write_text(report_md, encoding=\"utf-8\")\n",
        "print(\"wrote\", OUTPUT_DIR / \"report.md\")\n",
        "run_a, run_b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba2a3f5",
      "metadata": {},
      "source": [
        "## Appendix: Solutions (peek only after trying)\n",
        "\n",
        "Reference approach for comparing experiments and writing a short report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a680c4b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_run(run: dict) -> str:\n",
        "    cfg = run[\"config\"]\n",
        "    metrics = run[\"metrics\"]\n",
        "    return \"\\n\".join(\n",
        "        [\n",
        "            f\"- config: {cfg}\",\n",
        "            f\"- accuracy: {metrics['accuracy']}\",\n",
        "            f\"- f1_macro: {metrics['f1_macro']}\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "report_solution = \"\\n\".join(\n",
        "    [\n",
        "        \"# Experiment Comparison Report\",\n",
        "        \"\",\n",
        "        \"## What changed\",\n",
        "        \"In Experiment B, I increased `max_iter` while holding `seed` and `test_size` constant.\",\n",
        "        \"\",\n",
        "        \"## Results\",\n",
        "        \"### Experiment A\",\n",
        "        format_run(run_a),\n",
        "        \"\",\n",
        "        \"### Experiment B\",\n",
        "        format_run(run_b),\n",
        "        \"\",\n",
        "        \"## Why you think it happened\",\n",
        "        \"Logistic regression sometimes needs more optimization steps to converge; increasing `max_iter` can improve metrics if the model was under-trained.\",\n",
        "        \"\",\n",
        "        \"## Next experiment\",\n",
        "        \"Try a different solver (e.g. `lbfgs` vs `liblinear`) or add feature scaling and compare again.\",\n",
        "        \"\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "(OUTPUT_DIR / \"report_solution.md\").write_text(report_solution, encoding=\"utf-8\")\n",
        "print(\"wrote\", OUTPUT_DIR / \"report_solution.md\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
