{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 — Part 01: ML Training Loop Lab\n",
    "\n",
    "**Estimated time:** 120–150 minutes\n",
    "\n",
    "## What success looks like (end of Part 01)\n",
    "\n",
    "- You can run a full loop:\n",
    "  - split → train → evaluate\n",
    "- You save artifacts under `output/`:\n",
    "  - one run file under a timestamped folder (e.g. `output/run_.../result.json`)\n",
    "  - one summary file (e.g. `output/training_loop_summary.json`)\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "After running this notebook, you should be able to point to:\n",
    "\n",
    "- the exact `result.json` that produced one metric\n",
    "- the `training_loop_summary.json` that ranks multiple configs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement a complete ML training loop (split → train → evaluate → save)\n",
    "- Understand train/validation splits\n",
    "- Practice model evaluation metrics\n",
    "- Save and reload artifacts for reproducibility\n",
    "- Compare model configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b1951",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The ML training loop is the smallest workflow that is still “real engineering”:\n",
    "\n",
    "1. Load data\n",
    "2. Split train/validation\n",
    "3. Train a baseline model\n",
    "4. Evaluate on validation\n",
    "5. Save artifacts\n",
    "\n",
    "Even if you later focus on LLMs, this disciplined loop is the basis for comparing changes reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dca3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    seed: int = 7\n",
    "    test_size: float = 0.25\n",
    "    max_iter: int = 250\n",
    "\n",
    "\n",
    "cfg = TrainConfig()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f19e4",
   "metadata": {},
   "source": [
    "### Task 1.1: Load Data\n",
    "\n",
    "Load a dataset and inspect basic shapes/labels. We'll use Iris for a small reproducible example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826f246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_once(seed: int, test_size: float, max_iter: int) -> dict:\n",
    "    data = load_iris(as_frame=True)\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "\n",
    "    model = LogisticRegression(max_iter=max_iter, random_state=seed)\n",
    "    model.fit(X_train_s, y_train)\n",
    "\n",
    "    pred = model.predict(X_val_s)\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_val, pred)),\n",
    "        \"f1_macro\": float(f1_score(y_val, pred, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"config\": {\"seed\": seed, \"test_size\": test_size, \"max_iter\": max_iter},\n",
    "        \"metrics\": metrics,\n",
    "        \"class_counts_train\": [int(x) for x in np.bincount(y_train)],\n",
    "        \"class_counts_val\": [int(x) for x in np.bincount(y_val)],\n",
    "    }\n",
    "\n",
    "\n",
    "single = run_once(seed=cfg.seed, test_size=cfg.test_size, max_iter=cfg.max_iter)\n",
    "print(\"single run:\", single[\"config\"], single[\"metrics\"])\n",
    "\n",
    "run_id = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "run_dir = OUTPUT_DIR / run_id\n",
    "run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "(run_dir / \"result.json\").write_text(json.dumps(single, indent=2), encoding=\"utf-8\")\n",
    "print(\"wrote:\", run_dir / \"result.json\")\n",
    "\n",
    "candidates = [\n",
    "    {\"seed\": 7, \"test_size\": 0.25, \"max_iter\": 100},\n",
    "    {\"seed\": 7, \"test_size\": 0.25, \"max_iter\": 400},\n",
    "    {\"seed\": 13, \"test_size\": 0.20, \"max_iter\": 250},\n",
    "]\n",
    "\n",
    "results = [run_once(**c) for c in candidates]\n",
    "results_sorted = sorted(results, key=lambda r: r[\"metrics\"][\"accuracy\"], reverse=True)\n",
    "\n",
    "summary = {\n",
    "    \"best\": results_sorted[0],\n",
    "    \"all\": results_sorted,\n",
    "}\n",
    "\n",
    "(OUTPUT_DIR / \"training_loop_summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"wrote:\", OUTPUT_DIR / \"training_loop_summary.json\")\n",
    "print(\"best config:\", summary[\"best\"][\"config\"], \"metrics:\", summary[\"best\"][\"metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
