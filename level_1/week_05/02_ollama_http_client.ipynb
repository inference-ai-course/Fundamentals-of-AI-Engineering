{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Part 02: Calling Ollama via HTTP (minimal client)\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Treat local inference as a service call with the same reliability concerns\n",
    "- Implement a minimal HTTP client for Ollama `/api/generate`\n",
    "- Apply timeouts and basic parsing/validation\n",
    "- Save outputs + latency as artifacts for later comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e5088",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Ollama exposes a local HTTP API. This lets you treat local inference like a normal service call.\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: local inference is still a distributed system (just smaller)\n",
    "\n",
    "Even though the model is on your machine, your Python script is still making a network-style call:\n",
    "\n",
    "- your client process sends a request\n",
    "- the Ollama server process does work\n",
    "- you receive a response (or a failure)\n",
    "\n",
    "So the same engineering principles apply:\n",
    "\n",
    "- always set timeouts\n",
    "- log failures with enough context to debug\n",
    "- treat response parsing as untrusted input (validate what you need)\n",
    "\n",
    "We’ll implement a minimal client that:\n",
    "\n",
    "- sends a prompt\n",
    "- receives text output\n",
    "- prints it\n",
    "- records latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884aa85",
   "metadata": {},
   "source": [
    "## Minimal client (Python)\n",
    "\n",
    "Dependency:\n",
    "\n",
    "- `requests` (commonly used for HTTP)\n",
    "\n",
    "This notebook shows a minimal call to Ollama’s local endpoint:\n",
    "\n",
    "- `POST http://localhost:11434/api/generate`\n",
    "\n",
    "Two practical notes:\n",
    "\n",
    "- The timeout is a policy choice. Slower hardware or larger models may need longer.\n",
    "- When you later build a benchmark, consider warmup: the first call can be slower due to model loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception as e:  # pragma: no cover\n",
    "    requests = None\n",
    "    _requests_import_error = e\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OllamaGenerateRequest:\n",
    "    model: str\n",
    "    prompt: str\n",
    "    host: str = \"http://localhost:11434\"\n",
    "    timeout_s: float = 60.0\n",
    "\n",
    "\n",
    "def call_ollama(req: OllamaGenerateRequest) -> dict:\n",
    "    if requests is None:\n",
    "        raise RuntimeError(f\"requests is required: {_requests_import_error}\")\n",
    "\n",
    "    url = f\"{req.host}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": req.model,\n",
    "        \"prompt\": req.prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.0},\n",
    "    }\n",
    "\n",
    "    t0 = time.time()\n",
    "    resp = requests.post(url, json=payload, timeout=req.timeout_s, headers={\"X-Client\": \"level-1\"})\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    response_text = data.get(\"response\", \"\")\n",
    "    return {\n",
    "        \"model\": req.model,\n",
    "        \"response\": response_text,\n",
    "        \"latency_s\": time.time() - t0,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"call_ollama() ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(out: dict, *, out_dir: Path) -> Path:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"ollama_{int(time.time())}.json\"\n",
    "    out_path.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# Example usage (requires Ollama running locally):\n",
    "# req = OllamaGenerateRequest(model=\"llama3.1\", prompt=\"Say hello in one sentence\")\n",
    "# out = call_ollama(req)\n",
    "# print(json.dumps(out, indent=2))\n",
    "# print(\"saved to\", save_result(out, out_dir=Path(\"output/ollama_runs\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c452d",
   "metadata": {},
   "source": [
    "## How to run (CLI)\n",
    "\n",
    "```bash\n",
    "python call_ollama.py --model llama3.1 --prompt \"Say hello in one sentence\"\n",
    "```\n",
    "\n",
    "If this works, you’ve proven local inference end-to-end.\n",
    "\n",
    "---\n",
    "\n",
    "## Common pitfalls\n",
    "\n",
    "- Ollama service not running\n",
    "- wrong model name\n",
    "- timeouts for slow hardware\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Ollama docs/issues: https://github.com/ollama/ollama\n",
    "- Requests timeouts: https://requests.readthedocs.io/en/latest/user/quickstart/#timeouts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
