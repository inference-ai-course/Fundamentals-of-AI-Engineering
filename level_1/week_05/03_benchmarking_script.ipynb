{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Part 03: Benchmarking script (latency + quality artifacts)\n",
    "\n",
    "**Estimated time:** 90–150 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Explain why benchmarks must control variables\n",
    "- Measure latency as a distribution (not a single number)\n",
    "- Save benchmark artifacts to disk for manual quality review\n",
    "- Build a small benchmark harness for local models (Ollama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d6df6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A benchmark must be consistent:\n",
    "\n",
    "- same prompt set\n",
    "- same measurement method\n",
    "- same saved outputs\n",
    "\n",
    "We will write a small benchmark harness that:\n",
    "\n",
    "- loops over prompts\n",
    "- loops over models\n",
    "- records latency\n",
    "- saves outputs to disk for later quality review\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: benchmarking is measurement under controlled conditions\n",
    "\n",
    "You are trying to estimate:\n",
    "\n",
    "- **speed** (latency / throughput)\n",
    "- **quality** (correctness, format adherence, completeness)\n",
    "\n",
    "The key rule is controlling variables:\n",
    "\n",
    "- same prompts\n",
    "- same settings\n",
    "- same machine state as much as possible\n",
    "\n",
    "Latency is a distribution, not a single number. Two useful summaries:\n",
    "\n",
    "- average latency (typical case)\n",
    "- slowest case / tail latency (worst case)\n",
    "\n",
    "Practical implication: a model that is “fast on average” but has very slow worst cases may still feel bad in a demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607048ca",
   "metadata": {},
   "source": [
    "## Benchmark harness (example)\n",
    "\n",
    "Benchmark hygiene notes:\n",
    "\n",
    "- consider a warmup run per model (do not record it) to avoid counting model load time\n",
    "- keep prompts short enough that you are comparing models, not just tokenization overhead\n",
    "- avoid changing the prompt set while comparing models (version your prompt list)\n",
    "\n",
    "Next you’ll implement a simple benchmark harness that saves JSON artifacts to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except Exception as e:  # pragma: no cover\n",
    "    requests = None\n",
    "    _requests_import_error = e\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BenchmarkItem:\n",
    "    model: str\n",
    "    prompt: str\n",
    "    prompt_id: str\n",
    "\n",
    "\n",
    "def call_ollama_local(host: str, item: BenchmarkItem, *, timeout_s: float = 120.0) -> dict[str, Any]:\n",
    "    if requests is None:\n",
    "        raise RuntimeError(f\"requests is required: {_requests_import_error}\")\n",
    "\n",
    "    url = f\"{host}/api/generate\"\n",
    "    payload = {\"model\": item.model, \"prompt\": item.prompt, \"stream\": False}\n",
    "    t0 = time.time()\n",
    "    resp = requests.post(url, json=payload, timeout=timeout_s)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return {\n",
    "        \"model\": item.model,\n",
    "        \"prompt_id\": item.prompt_id,\n",
    "        \"response\": data.get(\"response\", \"\"),\n",
    "        \"latency_s\": time.time() - t0,\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_latencies(results: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    lats = [float(r[\"latency_s\"]) for r in results]\n",
    "    if not lats:\n",
    "        return {\"n\": 0}\n",
    "    return {\n",
    "        \"n\": len(lats),\n",
    "        \"avg_latency_s\": sum(lats) / len(lats),\n",
    "        \"p95_latency_s\": sorted(lats)[max(0, int(len(lats) * 0.95) - 1)],\n",
    "        \"max_latency_s\": max(lats),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"benchmark helpers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(items: list[BenchmarkItem], *, host: str, out_dir: Path) -> list[dict[str, Any]]:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results: list[dict[str, Any]] = []\n",
    "\n",
    "    seen_models: set[str] = set()\n",
    "    for item in items:\n",
    "        if item.model not in seen_models:\n",
    "            seen_models.add(item.model)\n",
    "            _ = call_ollama_local(host, BenchmarkItem(item.model, \"Warmup\", \"warmup\"), timeout_s=120.0)\n",
    "\n",
    "        r = call_ollama_local(host, item, timeout_s=120.0)\n",
    "        results.append(r)\n",
    "        out_path = out_dir / f\"{item.model}_{item.prompt_id}.json\"\n",
    "        out_path.write_text(json.dumps(r, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    summary = summarize_latencies(results)\n",
    "    (out_dir / \"summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (requires Ollama running locally):\n",
    "# host = \"http://localhost:11434\"\n",
    "# prompts = [\n",
    "#     (\"p01\", \"Summarize: Large language models are useful but require careful evaluation.\"),\n",
    "#     (\"p02\", \"Extract JSON with keys {name, email} from: 'Name: Sam, Email: sam@example.com'\"),\n",
    "#     (\"p03\", \"Write 3 bullet points about caching.\"),\n",
    "# ]\n",
    "# items = [BenchmarkItem(model=m, prompt=p, prompt_id=pid) for m in [\"llama3.1\", \"qwen2.5\"] for pid, p in prompts]\n",
    "# results = run_benchmark(items, host=host, out_dir=Path(\"output/benchmarks\"))\n",
    "# print(\"wrote\", len(results), \"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2845208",
   "metadata": {},
   "source": [
    "## How to compare models\n",
    "\n",
    "Compare:\n",
    "\n",
    "- **Speed**: average latency + slowest case\n",
    "- **Quality**: read saved outputs for:\n",
    "  - correctness\n",
    "  - adherence to format\n",
    "  - completeness\n",
    "\n",
    "Simple “quality heuristics” without heavy math:\n",
    "\n",
    "- for JSON prompts: count parse failures\n",
    "- for extraction: check if required keys exist\n",
    "- for summaries: check length caps and whether key facts are present\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Python `time`: https://docs.python.org/3/library/time.html\n",
    "- Python `timeit`: https://docs.python.org/3/library/timeit.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
