{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 — Part 01: Local inference concepts + setup checklist\n",
    "\n",
    "**Estimated time:** 45–75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Define inference and local inference\n",
    "- Explain how moving from hosted APIs to local inference changes constraints\n",
    "- Understand how model size, context length, and quantization affect latency and memory\n",
    "- Follow a practical setup checklist for Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357111d",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Inference** = using a trained model to generate outputs.\n",
    "\n",
    "**Local inference** = you run the model on your own machine.\n",
    "\n",
    "Local inference is useful for:\n",
    "\n",
    "- privacy (data stays local)\n",
    "- cost control (no per-request billing)\n",
    "- offline capability\n",
    "\n",
    "Trade-offs:\n",
    "\n",
    "- quality may be lower than top hosted models\n",
    "- performance depends on your CPU/GPU/RAM/VRAM\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying theory: moving the boundary changes your constraints\n",
    "\n",
    "When you use a hosted API, the provider owns the compute and you mostly worry about:\n",
    "\n",
    "- request formatting\n",
    "- rate limits\n",
    "- latency and cost\n",
    "\n",
    "When you run locally, you become the provider. That means **hardware is now part of your system design**.\n",
    "\n",
    "You can think of local inference performance as a function:\n",
    "\n",
    "$$\n",
    "\\text{latency} = f(\\text{model size},\\ \\text{context length},\\ \\text{hardware},\\ \\text{quantization})\n",
    "$$\n",
    "\n",
    "Practical implication:\n",
    "\n",
    "- if a model does not fit in RAM/VRAM, it won’t run (or will thrash)\n",
    "- even if it fits, throughput/latency can vary dramatically across machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503b937e",
   "metadata": {},
   "source": [
    "## Setup checklist (practical)\n",
    "\n",
    "1. Install Ollama\n",
    "2. Start the Ollama service\n",
    "3. Pull a model\n",
    "4. Run a test prompt\n",
    "\n",
    "What to do and what “success” looks like:\n",
    "\n",
    "1. **Install Ollama**\n",
    "    - Goal: have the `ollama` CLI available.\n",
    "    - Verify: `ollama --version` prints a version.\n",
    "\n",
    "2. **Start the Ollama service**\n",
    "    - Goal: local server process ready to accept requests.\n",
    "    - Verify: `ollama serve` starts without immediately exiting.\n",
    "    - Common failure: port conflicts or permission issues.\n",
    "\n",
    "3. **Pull a model**\n",
    "    - Goal: download at least one model.\n",
    "    - Verify: `ollama list` shows the model.\n",
    "    - Practical note: start small to avoid memory failures.\n",
    "\n",
    "4. **Run a test prompt**\n",
    "    - Goal: confirm request → generation works locally.\n",
    "    - Verify: `ollama run <model_name>` produces output and doesn’t crash.\n",
    "    - Note: first run can be slow due to model loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632a9d7",
   "metadata": {},
   "source": [
    "import platform\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def try_run(cmd: list[str]) -> None:\n",
    "    print(\"$\", \" \".join(cmd))\n",
    "    try:\n",
    "        out = subprocess.run(cmd, capture_output=True, text=True, check=False)\n",
    "        print(\"returncode=\", out.returncode)\n",
    "        if out.stdout:\n",
    "            print(out.stdout.strip())\n",
    "        if out.stderr:\n",
    "            print(out.stderr.strip())\n",
    "    except FileNotFoundError:\n",
    "        print(\"command not found\")\n",
    "\n",
    "\n",
    "print(\"python=\", platform.python_version())\n",
    "print(\"platform=\", platform.platform())\n",
    "print(\"ollama_in_path=\", shutil.which(\"ollama\") is not None)\n",
    "\n",
    "try_run([\"ollama\", \"--version\"])\n",
    "try_run([\"ollama\", \"list\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06690bb",
   "metadata": {},
   "source": [
    "## What “model size / context window / quantization” mean\n",
    "\n",
    "- **Size (e.g. 7B, 13B)**: larger often means better quality but slower and more memory.\n",
    "- **Context window**: how much text you can include per request.\n",
    "- **Quantization**: smaller memory footprint (quality may change slightly).\n",
    "\n",
    "More concrete intuition:\n",
    "\n",
    "- model size is roughly the number of parameters\n",
    "- more parameters usually means more compute per generated token\n",
    "- quantization stores weights with fewer bits, reducing memory and often increasing speed on constrained hardware\n",
    "\n",
    "Practical rule of thumb: local inference is often bottlenecked by memory bandwidth and/or VRAM capacity, not just CPU speed.\n",
    "\n",
    "For Level 1, focus on the practical effect:\n",
    "\n",
    "- if it doesn’t fit, you can’t run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_gb(params_billion: float, bits_per_weight: int) -> float:\n",
    "    # Rough estimate: params * bits per weight -> bytes -> GB\n",
    "    params = params_billion * 1_000_000_000\n",
    "    bytes_used = params * (bits_per_weight / 8)\n",
    "    return bytes_used / (1024 ** 3)\n",
    "\n",
    "\n",
    "sizes = [7, 13, 70]\n",
    "for s in sizes:\n",
    "    for bits in [4, 8, 16]:\n",
    "        print(f\"{s}B @ {bits}-bit: {estimate_memory_gb(s, bits):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model_for_hardware(vram_gb: float) -> str:\n",
    "    # TODO: implement a rule-of-thumb mapping.\n",
    "    # Example:\n",
    "    # - vram < 8 -> prefer 3B or smaller\n",
    "    # - vram < 16 -> prefer 7B\n",
    "    # - otherwise -> 13B+ (if latency acceptable)\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "print(\"Implement choose_model_for_hardware().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d0df2",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Ollama: https://ollama.com/\n",
    "- Ollama GitHub: https://github.com/ollama/ollama\n",
    "- Hugging Face model cards: https://huggingface.co/docs/hub/model-cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2372d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ollama_status() -> bool:\n",
    "    # TODO: implement a quick local health check.\n",
    "    # Option 1: run `ollama list` and check return code.\n",
    "    # Option 2: attempt a small HTTP request to localhost:11434.\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "print(\"Implement check_ollama_status().\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
