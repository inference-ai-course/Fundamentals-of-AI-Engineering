# Level 1 — Week 5 Tutorials

## Overview

This week you run LLMs locally (Ollama) and compare models with a consistent benchmark.

You will practice:

- installing + running a local model
- sending prompts via HTTP
- measuring latency
- saving outputs for comparison

## Navigation

- [01 — Local inference: concepts + setup checklist](01_local_inference_setup.md)
- [02 — Calling Ollama via HTTP (minimal client)](02_ollama_http_client.md)
- [03 — Benchmarking script: latency + quality artifacts](03_benchmarking_script.md)

## Recommended order

1. Read 01 and confirm Ollama runs.
2. Read 02 and run a single prompt end-to-end.
3. Read 03 and build a benchmark harness.

Use [practice.ipynb](practice.ipynb) for extra work.
