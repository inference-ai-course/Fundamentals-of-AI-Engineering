{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Level 1 â€” Week 5 Practice (Starter Notebook)\n",
        "\n",
        "This notebook provides starter code to benchmark **local inference with Ollama**.\n",
        "\n",
        "## References (docs)\n",
        "- Ollama (official): https://ollama.com/\n",
        "- Ollama GitHub (docs): https://github.com/ollama/ollama\n",
        "- Python `time` (official): https://docs.python.org/3/library/time.html\n",
        "- `requests` (official docs): https://requests.readthedocs.io/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "This assumes you have Ollama running locally. Typical default endpoint:\n",
        "- `http://localhost:11434`\n",
        "\n",
        "If the endpoint is different, update `OLLAMA_BASE_URL`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_BASE_URL = 'http://localhost:11434'\n",
        "GENERATE_URL = f'{OLLAMA_BASE_URL}/api/generate'\n",
        "GENERATE_URL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a small benchmark set\n",
        "\n",
        "Keep it small and consistent so you can compare models fairly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROMPTS = [\n",
        "    'Summarize: LLMs can be used for extraction, summarization, and classification.',\n",
        "    'Extract as JSON: name, email from: John Doe (john@example.com).',\n",
        "    'Write 3 bullets: pros/cons of local inference.',\n",
        "]\n",
        "MODELS = [\n",
        "    # TODO: replace with models you have pulled, e.g. 'llama3.1:8b'\n",
        "    'llama3',\n",
        "]\n",
        "PROMPTS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ollama generate call\n",
        "\n",
        "We call the HTTP API and measure latency.\n",
        "If you get connection errors, confirm Ollama is running and your model name is correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ollama_generate(model: str, prompt: str, *, timeout_s: float = 60.0) -> Dict[str, Any]:\n",
        "    payload = {\n",
        "        'model': model,\n",
        "        'prompt': prompt,\n",
        "        'stream': False,\n",
        "    }\n",
        "    resp = requests.post(GENERATE_URL, json=payload, timeout=timeout_s)\n",
        "    resp.raise_for_status()\n",
        "    return resp.json()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark loop (starter)\n",
        "\n",
        "Stores latency and a small output sample for later inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "for model in MODELS:\n",
        "    for prompt in PROMPTS:\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            out = ollama_generate(model, prompt)\n",
        "            ok = True\n",
        "            text = out.get('response', '')\n",
        "        except Exception as e:\n",
        "            ok = False\n",
        "            text = f'ERROR: {type(e).__name__}: {e}'\n",
        "        latency_ms = int((time.time() - t0) * 1000)\n",
        "        results.append({\n",
        "            'model': model,\n",
        "            'prompt': prompt,\n",
        "            'ok': ok,\n",
        "            'latency_ms': latency_ms,\n",
        "            'output_preview': text[:400],\n",
        "        })\n",
        "\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO: Write conclusions\n",
        "\n",
        "- Which model is best for speed?\n",
        "- Which model is best for quality?\n",
        "- What are the failure modes (timeouts, low quality, context issues)?\n",
        "\n",
        "Optional next step: export `results` to JSON and include in your report.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
