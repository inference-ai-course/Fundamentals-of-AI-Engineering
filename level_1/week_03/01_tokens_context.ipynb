{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 — Part 01: Tokens and Context Lab\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Explain tokens and context windows\n",
    "- Count tokens using a tokenizer\n",
    "- Compare token usage across inputs\n",
    "- Practice truncation strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b994fc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Tokens are the units models operate on. Context windows limit how many tokens you can send at once, which affects prompt design and truncation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception as e:  # pragma: no cover\n",
    "    tiktoken = None\n",
    "    _tiktoken_error = e\n",
    "\n",
    "\n",
    "def simple_tokenize(text: str) -> list[str]:\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "\n",
    "sample = \"Hello, world! Token test.\"\n",
    "print(\"simple tokens:\", simple_tokenize(sample))\n",
    "\n",
    "if tiktoken is None:\n",
    "    print(\"tiktoken not available:\", _tiktoken_error)\n",
    "else:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(\"Token counting is useful for budgeting.\")\n",
    "    print(\"tiktoken count:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1919",
   "metadata": {},
   "source": [
    "## Truncation exercise\n",
    "\n",
    "Use a simple truncation strategy to fit text into a max token budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ae50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text_tokens(text: str, max_tokens: int, *, enc) -> str:\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    return enc.decode(tokens[:max_tokens])\n",
    "\n",
    "\n",
    "if tiktoken is not None:\n",
    "    long_text = \"data \" * 400\n",
    "    truncated = truncate_text_tokens(long_text, 200, enc=enc)\n",
    "    print(\"orig tokens:\", len(enc.encode(long_text)))\n",
    "    print(\"trunc tokens:\", len(enc.encode(truncated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec933c",
   "metadata": {},
   "source": [
    "## Practice exercises\n",
    "\n",
    "- Compare token counts for URLs vs plain text.\n",
    "- Experiment with different max token budgets.\n",
    "- Add a function that estimates cost given tokens.\n",
    "\n",
    "## References\n",
    "\n",
    "- tiktoken: https://github.com/openai/tiktoken"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
