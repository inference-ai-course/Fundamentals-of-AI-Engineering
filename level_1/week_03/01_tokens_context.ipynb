{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 — Part 01: Tokens and Context Lab\n",
    "\n",
    "**Estimated time:** 60–90 minutes\n",
    "\n",
    "## What success looks like (end of Part 01)\n",
    "\n",
    "- You can estimate whether a text is “too long” for a given context window.\n",
    "- You can compare token counts across different inputs.\n",
    "- You can apply a truncation strategy to fit a token budget.\n",
    "\n",
    "### Checkpoint\n",
    "\n",
    "- You can produce a token count for at least 2 different strings.\n",
    "- If `tiktoken` is available, you can truncate a long string down to a target token budget.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Explain tokens and context windows\n",
    "- Count tokens using a tokenizer\n",
    "- Compare token usage across inputs\n",
    "- Practice truncation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b994fc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Tokens are the units models operate on. Context windows limit how many tokens you can send at once, which affects prompt design and truncation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception as e:  # pragma: no cover\n",
    "    tiktoken = None\n",
    "    _tiktoken_error = e\n",
    "\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "\n",
    "sample = \"Hello, world! Token test.\"\n",
    "print(\"simple tokens:\", simple_tokenize(sample))\n",
    "\n",
    "if tiktoken is None:\n",
    "    print(\"tiktoken not available:\", _tiktoken_error)\n",
    "else:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(\"Token counting is useful for budgeting.\")\n",
    "    print(\"tiktoken count:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1919",
   "metadata": {},
   "source": [
    "## Truncation exercise\n",
    "\n",
    "Use a simple truncation strategy to fit text into a max token budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ae50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text_tokens(text: str, max_tokens: int, *, enc) -> str:\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    return enc.decode(tokens[:max_tokens])\n",
    "\n",
    "\n",
    "if tiktoken is not None:\n",
    "    long_text = \"data \" * 400\n",
    "    truncated = truncate_text_tokens(long_text, 200, enc=enc)\n",
    "    print(\"orig tokens:\", len(enc.encode(long_text)))\n",
    "    print(\"trunc tokens:\", len(enc.encode(truncated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ec933c",
   "metadata": {},
   "source": [
    "## Exercise: token budgeting helper (TODO)\n",
    "\n",
    "Implement the TODO function below.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Given a piece of text, return a dict containing:\n",
    "  - `n_simple_tokens` using `simple_tokenize`\n",
    "  - `n_tiktoken_tokens` if `tiktoken` is available, otherwise `None`\n",
    "\n",
    "Checkpoint:\n",
    "\n",
    "- Running the cell prints token counts for at least 2 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe04a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_budget_summary_todo(text: str) -> dict:\n",
    "    # TODO: implement.\n",
    "    # Keep it runnable even if tiktoken is not installed.\n",
    "    return {\"n_simple_tokens\": len(simple_tokenize(text)), \"n_tiktoken_tokens\": None}\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"Hello world!\",\n",
    "    \"https://example.com/some/path?with=query&and=more\",\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(s, \"->\", token_budget_summary_todo(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e17957",
   "metadata": {},
   "source": [
    "## Appendix: Solutions (peek only after trying)\n",
    "\n",
    "Reference implementation for `token_budget_summary_todo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_budget_summary_todo(text: str) -> dict:\n",
    "    n_simple = len(simple_tokenize(text))\n",
    "\n",
    "    if tiktoken is None:\n",
    "        return {\"n_simple_tokens\": n_simple, \"n_tiktoken_tokens\": None}\n",
    "\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    n_tiktoken = len(enc.encode(text))\n",
    "    return {\"n_simple_tokens\": n_simple, \"n_tiktoken_tokens\": int(n_tiktoken)}\n",
    "\n",
    "\n",
    "for s in examples:\n",
    "    print(s, \"->\", token_budget_summary_todo(s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
