{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Vibe Coding Workshop — Build a Learning Assistant CLI\n",
    "\n",
    "## Welcome to the Vibe Coding Workshop!\n",
    "\n",
    "In this lab, you'll learn AI-assisted development by **building**, not by studying theory.\n",
    "\n",
    "### What You'll Build\n",
    "\n",
    "A **Learning Assistant CLI** with:\n",
    "- 3-5 commands (track, status, recommend, quiz, export)\n",
    "- JSON persistence\n",
    "- Comprehensive test coverage\n",
    "- Clean project structure\n",
    "\n",
    "### The 5-Step Vibe Coding Loop\n",
    "\n",
    "1. **Spec** → Write requirements, constraints, acceptance tests\n",
    "2. **Scaffold** → Request minimal project structure\n",
    "3. **Test** → Generate pytest tests from spec\n",
    "4. **Patch** → Iterate: test failure → minimal fix → verify\n",
    "5. **Review** → AI-assisted code review + refactors\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- Your favorite AI coding assistant (Cursor, Windsurf, GitHub Copilot)\n",
    "- A text editor or IDE\n",
    "\n",
    "### Time Estimate\n",
    "\n",
    "3-4 hours (with breaks)\n",
    "\n",
    "---\n",
    "\n",
    "**Let's begin!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup: Create Your Project Directory\n",
    "\n",
    "Before starting, create a directory for your project:\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/learning_assistant\n",
    "cd ~/learning_assistant\n",
    "```\n",
    "\n",
    "**Checkpoint:**\n",
    "- [ ] Created project directory\n",
    "- [ ] Changed into the directory\n",
    "- [ ] Ready to start Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Write Your Spec\n",
    "\n",
    "**Time: 10-15 minutes**\n",
    "\n",
    "In this exercise, you'll write a specification for your Learning Assistant CLI. Your spec should define:\n",
    "- 3-5 commands with arguments\n",
    "- Requirements (data storage, validation, etc.)\n",
    "- Non-goals (what you're NOT building)\n",
    "- 8-10 acceptance tests\n",
    "\n",
    "**Use the template below or create your own spec in a separate file.**\n",
    "\n",
    "### Example Spec Template\n",
    "\n",
    "```markdown\n",
    "## Feature: Learning Assistant CLI\n",
    "\n",
    "### Commands\n",
    "1. `track <topic> <hours> [date]` - Log study session\n",
    "2. `status` - Show progress as JSON\n",
    "3. `recommend` - Suggest next topic\n",
    "4. `quiz <topic>` - Generate practice questions\n",
    "5. `export <filename>` - Export data to JSON\n",
    "\n",
    "### Requirements\n",
    "- Data stored in data.json\n",
    "- status and export output valid JSON\n",
    "- Input validation with helpful errors\n",
    "- Each command has --help text\n",
    "\n",
    "### Non-goals\n",
    "- No web UI (CLI only)\n",
    "- No external APIs (local data only)\n",
    "- No fancy TUI (plain text output)\n",
    "\n",
    "### Acceptance Tests\n",
    "1. `status` on empty data → {\"total_hours\": 0, \"topics\": []}\n",
    "2. `track python 2.5` → data.json updated\n",
    "3. `track python -1` → Error: \"Hours must be positive\"\n",
    "4. `recommend` after tracking → relevant suggestion\n",
    "5. `export out.json` → valid JSON file created\n",
    "...\n",
    "```\n",
    "\n",
    "**Your task:** Create a spec file or write it in your AI assistant, then move to Exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Checkpoint 1: Spec Complete**\n",
    "\n",
    "Before moving forward, verify your spec:\n",
    "- [ ] Defines 3-5 commands with clear arguments\n",
    "- [ ] Lists specific requirements (storage, validation, output format)\n",
    "- [ ] States non-goals (what you're NOT building)\n",
    "- [ ] Includes 8-10 acceptance tests\n",
    "- [ ] Is specific enough that someone could implement it without asking questions\n",
    "\n",
    "**If yes to all → proceed to Exercise 2 (Scaffold)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Request Project Scaffold (10 min)\n",
    "\n",
    "**Goal:** Get AI to create project structure with minimal stubs.\n",
    "\n",
    "Use your AI assistant with this prompt template:\n",
    "\n",
    "```\n",
    "Using my spec, create a Python CLI project with this structure:\n",
    "\n",
    "learning_assistant/\n",
    "├── README.md (setup + usage)\n",
    "├── requirements.txt (pytest, jsonschema)\n",
    "├── data.json (empty template)\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── cli.py (argparse setup)\n",
    "│   ├── commands.py (command stubs)\n",
    "│   ├── storage.py (load/save stubs)\n",
    "└── tests/\n",
    "    ├── __init__.py\n",
    "    └── test_commands.py (empty test stubs)\n",
    "\n",
    "Requirements:\n",
    "- Generate ONLY file structure with minimal stubs\n",
    "- Do NOT implement full logic yet\n",
    "- Add TODO comments where logic goes\n",
    "- Keep functions under 10 lines (stubs only)\n",
    "```\n",
    "\n",
    "**Action:** Use AI to generate scaffold, verify files exist, check structure makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemPromptDesigner:\n",
    "    \"\"\"Design effective system prompts for different use cases.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prompt_templates = {\n",
    "            'technical_writer': self.create_technical_writer_prompt(),\n",
    "            'customer_support': self.create_customer_support_prompt(),\n",
    "            'code_reviewer': self.create_code_reviewer_prompt(),\n",
    "            'data_analyst': self.create_data_analyst_prompt()\n",
    "        }\n",
    "    \n",
    "    def create_technical_writer_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for technical documentation.\"\"\"\n",
    "        return \"\"\"\n",
    "You are a senior technical writer with 10 years of experience in software documentation.\n",
    "You specialize in creating clear, concise, and accurate technical documentation for developers.\n",
    "\n",
    "Your characteristics:\n",
    "- Write in a professional but approachable tone\n",
    "- Use active voice and present tense\n",
    "- Include practical examples and code snippets\n",
    "- Structure information logically with clear headings\n",
    "\n",
    "Your constraints:\n",
    "- Avoid jargon unless necessary (define when used)\n",
    "- Keep sentences under 25 words when possible\n",
    "- Provide step-by-step instructions for complex tasks\n",
    "- Include troubleshooting sections for common issues\n",
    "\n",
    "Your output format:\n",
    "- Use Markdown formatting\n",
    "- Include a brief overview section\n",
    "- Structure content with hierarchical headings\n",
    "- End with a summary or next steps section\n",
    "\"\"\"\n",
    "    \n",
    "    def create_customer_support_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for customer support.\"\"\"\n",
    "        return \"\"\"\n",
    "You are a helpful customer support assistant for TechCorp.\n",
    "You have access to order information, product details, and troubleshooting guides.\n",
    "\n",
    "Guidelines:\n",
    "- Be polite and professional\n",
    "- Ask clarifying questions when needed\n",
    "- Provide step-by-step solutions\n",
    "- Escalate to human agent for complex issues\n",
    "- Never make promises about refunds or compensation\n",
    "\"\"\"\n",
    "    \n",
    "    def create_code_reviewer_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for code review.\"\"\"\n",
    "        return \"\"\"\n",
    "You are an experienced software engineer specializing in code review and best practices.\n",
    "You have expertise in Python, JavaScript, and software architecture.\n",
    "\n",
    "Review criteria:\n",
    "- Check for code readability and maintainability\n",
    "- Identify potential bugs or security issues\n",
    "- Suggest performance improvements\n",
    "- Ensure proper error handling\n",
    "- Verify adherence to coding standards\n",
    "\n",
    "Provide constructive feedback with specific examples and suggestions.\n",
    "\"\"\"\n",
    "    \n",
    "    def create_data_analyst_prompt(self) -> str:\n",
    "        \"\"\"Create prompt for data analysis.\"\"\"\n",
    "        return \"\"\"\n",
    "You are a senior data analyst with expertise in statistical analysis and data visualization.\n",
    "You specialize in extracting insights from complex datasets.\n",
    "\n",
    "Analysis approach:\n",
    "- Start with data quality assessment\n",
    "- Identify key patterns and trends\n",
    "- Provide statistical significance where applicable\n",
    "- Suggest actionable recommendations\n",
    "- Include appropriate visualizations\n",
    "\n",
    "Always explain your methodology and assumptions clearly.\n",
    "\"\"\"\n",
    "\n",
    "# Test system prompt effectiveness\n",
    "print(\"Testing System Prompt Designer...\")\n",
    "designer = SystemPromptDesigner()\n",
    "\n",
    "for role, prompt in designer.prompt_templates.items():\n",
    "    print(f\"\\n--- {role.replace('_', ' ').title()} Prompt ---\")\n",
    "    print(f\"Length: {len(prompt)} characters\")\n",
    "    print(f\"First 200 chars: {prompt[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: The CLEAR Framework Implementation\n",
    "\n",
    "### What is CLEAR? A Systematic Approach to Prompt Engineering\n",
    "\n",
    "**The CLEAR Framework:**\n",
    "\n",
    "CLEAR is a **structured methodology** for creating effective prompts:\n",
    "\n",
    "- **C**ontext: Set the scene and background\n",
    "- **L**ength: Specify desired output length\n",
    "- **E**xamples: Provide few-shot examples\n",
    "- **A**udience: Define who will read this\n",
    "- **R**equirements: List specific constraints and needs\n",
    "\n",
    "**Why Use a Framework?**\n",
    "\n",
    "Without a framework:\n",
    "- Prompts are inconsistent\n",
    "- Important details get forgotten\n",
    "- Results vary wildly\n",
    "- Hard to improve systematically\n",
    "\n",
    "With CLEAR:\n",
    "- ✅ **Consistent**: Same structure every time\n",
    "- ✅ **Complete**: All important aspects covered\n",
    "- ✅ **Comparable**: Easy to A/B test variations\n",
    "- ✅ **Improvable**: Clear what to change\n",
    "\n",
    "---\n",
    "\n",
    "### Breaking Down Each Component\n",
    "\n",
    "**1. Context (C)**\n",
    "```python\n",
    "context = \"You are writing product descriptions for an e-commerce website.\"\n",
    "```\n",
    "**Purpose**: \n",
    "- Establishes the situation\n",
    "- Provides background information\n",
    "- Sets expectations\n",
    "\n",
    "**Why It Matters**: \n",
    "- AI needs context to make good decisions\n",
    "- Without context, responses are generic\n",
    "- Context helps AI understand constraints\n",
    "\n",
    "**2. Length (L)**\n",
    "```python\n",
    "length = \"100-150 words\"\n",
    "```\n",
    "**Purpose**:\n",
    "- Specifies output size\n",
    "- Prevents too-short or too-long responses\n",
    "- Helps with consistency\n",
    "\n",
    "**Why It Matters**:\n",
    "- Users expect consistent length\n",
    "- Too short = missing information\n",
    "- Too long = wasted tokens, user fatigue\n",
    "\n",
    "**3. Examples (E)**\n",
    "```python\n",
    "examples = \"\"\"\n",
    "Input: Wireless Bluetooth Headphones\n",
    "Output: Experience premium sound quality...\n",
    "\"\"\"\n",
    "```\n",
    "**Purpose**:\n",
    "- Shows AI exactly what you want\n",
    "- Demonstrates format and style\n",
    "- Provides reference patterns\n",
    "\n",
    "**Why It Matters**:\n",
    "- **Few-shot learning**: AI learns from examples\n",
    "- **Format consistency**: Examples show structure\n",
    "- **Quality bar**: Examples set expectations\n",
    "\n",
    "**4. Audience (A)**\n",
    "```python\n",
    "audience = \"environmentally conscious consumers aged 25-45\"\n",
    "```\n",
    "**Purpose**:\n",
    "- Defines who will read the output\n",
    "- Influences tone and complexity\n",
    "- Guides content selection\n",
    "\n",
    "**Why It Matters**:\n",
    "- Technical audience → use jargon\n",
    "- General audience → simple language\n",
    "- Age group → appropriate references\n",
    "\n",
    "**5. Requirements (R)**\n",
    "```python\n",
    "requirements = \"Highlight these features: organic cotton, breathable fabric...\"\n",
    "```\n",
    "**Purpose**:\n",
    "- Lists must-have elements\n",
    "- Specifies constraints\n",
    "- Defines success criteria\n",
    "\n",
    "**Why It Matters**:\n",
    "- Ensures nothing is forgotten\n",
    "- Provides clear checklist\n",
    "- Makes evaluation easier\n",
    "\n",
    "---\n",
    "\n",
    "### The CLEAR Template Structure\n",
    "\n",
    "**Standard Template:**\n",
    "```\n",
    "Context: {context}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Requirements:\n",
    "- Length: {length}\n",
    "- Audience: {audience}\n",
    "- {requirements}\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Remember to follow all requirements and match the example format.\n",
    "```\n",
    "\n",
    "**Why This Order?**\n",
    "1. **Context first**: Sets the stage\n",
    "2. **Task second**: What to do\n",
    "3. **Requirements third**: How to do it\n",
    "4. **Examples last**: Reference for style\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**E-commerce Product Descriptions:**\n",
    "- **Context**: E-commerce website\n",
    "- **Length**: 100-150 words\n",
    "- **Examples**: Show successful descriptions\n",
    "- **Audience**: Target customer segment\n",
    "- **Requirements**: Highlight key features\n",
    "\n",
    "**Technical Documentation:**\n",
    "- **Context**: Developer documentation\n",
    "- **Length**: 300-500 words\n",
    "- **Examples**: Code examples and explanations\n",
    "- **Audience**: Software developers\n",
    "- **Requirements**: Include code snippets, troubleshooting\n",
    "\n",
    "**Marketing Copy:**\n",
    "- **Context**: Marketing campaign\n",
    "- **Length**: 50-100 words\n",
    "- **Examples**: Compelling copy samples\n",
    "- **Audience**: Target market\n",
    "- **Requirements**: Include call-to-action, benefits\n",
    "\n",
    "---\n",
    "\n",
    "### Advanced CLEAR Techniques\n",
    "\n",
    "**1. Multiple Examples**\n",
    "- Show variety in examples\n",
    "- Cover edge cases\n",
    "- Demonstrate range of acceptable outputs\n",
    "\n",
    "**2. Negative Examples**\n",
    "- Show what NOT to do\n",
    "- Highlight common mistakes\n",
    "- Set boundaries\n",
    "\n",
    "**3. Progressive Complexity**\n",
    "- Start with simple examples\n",
    "- Build to complex scenarios\n",
    "- Guide AI learning curve\n",
    "\n",
    "**4. Dynamic Requirements**\n",
    "- Adjust based on input\n",
    "- Context-aware constraints\n",
    "- Adaptive specifications\n",
    "\n",
    "---\n",
    "\n",
    "### Measuring CLEAR Effectiveness\n",
    "\n",
    "**Metrics to Track:**\n",
    "- **Completeness**: All requirements met?\n",
    "- **Consistency**: Similar inputs → similar outputs?\n",
    "- **Quality**: Does output match examples?\n",
    "- **Efficiency**: Right length, no waste?\n",
    "\n",
    "**A/B Testing:**\n",
    "- Test different CLEAR variations\n",
    "- Measure user satisfaction\n",
    "- Optimize based on results\n",
    "\n",
    "Now let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearFramework:\n",
    "    \"\"\"Implement the CLEAR framework for prompt engineering.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_prompt(task: str, context: str, length: str, examples: str, \n",
    "                     audience: str, requirements: str) -> str:\n",
    "        \"\"\"Create a prompt using the CLEAR framework.\"\"\"\n",
    "        return f\"\"\"\n",
    "Context: {context}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Requirements:\n",
    "- Length: {length}\n",
    "- Audience: {audience}\n",
    "- {requirements}\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Remember to follow all requirements and match the example format.\n",
    "\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_product_description_prompt(product_name: str, features: List[str], \n",
    "                                        target_audience: str) -> str:\n",
    "        \"\"\"Create a product description prompt using CLEAR framework.\"\"\"\n",
    "        context = f\"You are writing product descriptions for an e-commerce website.\"\n",
    "        task = f\"Write a compelling product description for {product_name}\"\n",
    "        length = \"100-150 words\"\n",
    "        audience = target_audience\n",
    "        requirements = f\"Highlight these features: {', '.join(features)}. Use persuasive language.\"\n",
    "        \n",
    "        examples = \"\"\"\n",
    "Input: Wireless Bluetooth Headphones\n",
    "Output: Experience premium sound quality with our wireless Bluetooth headphones. \n",
    "Featuring active noise cancellation, 30-hour battery life, and comfortable over-ear design. \n",
    "Perfect for music lovers and professionals who demand exceptional audio performance.\n",
    "\n",
    "Input: Stainless Steel Water Bottle\n",
    "Output: Stay hydrated with our durable stainless steel water bottle. Double-wall vacuum \n",
    "insulation keeps drinks cold for 24 hours or hot for 12 hours. Leak-proof lid and sweat-free \n",
    "design make it perfect for gym, office, or outdoor adventures.\n",
    "\"\"\"\n",
    "        \n",
    "        return ClearFramework.create_prompt(task, context, length, examples, audience, requirements)\n",
    "\n",
    "# Test CLEAR framework\n",
    "print(\"Testing CLEAR Framework...\")\n",
    "\n",
    "# Create product description prompt\n",
    "product_prompt = ClearFramework.create_product_description_prompt(\n",
    "    product_name=\"Organic Cotton T-Shirt\",\n",
    "    features=[\"100% organic cotton\", \"breathable fabric\", \"sustainable production\", \"comfortable fit\"],\n",
    "    target_audience=\"environmentally conscious consumers aged 25-45\"\n",
    ")\n",
    "\n",
    "print(\"Generated Product Description Prompt:\")\n",
    "print(product_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Parameter Tuning and Evaluation\n",
    "\n",
    "### Why Parameters Matter: Controlling AI Behavior\n",
    "\n",
    "**The Hidden Levers:**\n",
    "\n",
    "AI models have **parameters** that control their behavior:\n",
    "- **Temperature**: Controls randomness/creativity\n",
    "- **Top-p**: Controls diversity of token selection\n",
    "- **Max tokens**: Limits response length\n",
    "- **Frequency penalty**: Reduces repetition\n",
    "- **Presence penalty**: Encourages new topics\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "Same prompt + different parameters = **completely different outputs**:\n",
    "- Low temperature: Consistent, predictable\n",
    "- High temperature: Creative, varied\n",
    "- Wrong parameters: Useless or harmful outputs\n",
    "\n",
    "**Real-World Impact:**\n",
    "- **10-50% improvement** in output quality with right parameters\n",
    "- **Cost savings**: Shorter responses = lower costs\n",
    "- **User satisfaction**: Better outputs = happier users\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding Temperature: The Creativity Dial\n",
    "\n",
    "**What Temperature Does:**\n",
    "\n",
    "Temperature controls **how random** the model's token selection is:\n",
    "\n",
    "- **Temperature = 0.0**: Deterministic (always same output)\n",
    "- **Temperature = 0.1-0.3**: Low creativity, high consistency\n",
    "- **Temperature = 0.5-0.7**: Balanced (default)\n",
    "- **Temperature = 0.8-1.0**: High creativity, high variation\n",
    "- **Temperature > 1.0**: Very random, often incoherent\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "```\n",
    "Model calculates probability for each possible next token:\n",
    "- \"the\" (40%)\n",
    "- \"a\" (30%)\n",
    "- \"an\" (20%)\n",
    "- \"some\" (10%)\n",
    "\n",
    "Low temperature: Always picks \"the\" (highest probability)\n",
    "High temperature: Randomly picks based on probabilities\n",
    "```\n",
    "\n",
    "**When to Use Each:**\n",
    "\n",
    "| Temperature | Use Case | Example |\n",
    "|------------|----------|---------|\n",
    "| 0.0-0.2 | Code generation, factual answers | \"What is 2+2?\" |\n",
    "| 0.3-0.5 | Technical writing, analysis | \"Explain how X works\" |\n",
    "| 0.6-0.8 | Creative writing, brainstorming | \"Write a story about...\" |\n",
    "| 0.9-1.2 | Experimental, artistic | \"Generate unique ideas\" |\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding Top-p (Nucleus Sampling)\n",
    "\n",
    "**What Top-p Does:**\n",
    "\n",
    "Top-p selects from the **smallest set of tokens** whose cumulative probability exceeds p:\n",
    "\n",
    "- **Top-p = 0.1**: Only considers top 10% most likely tokens\n",
    "- **Top-p = 0.5**: Considers tokens up to 50% cumulative probability\n",
    "- **Top-p = 0.9**: Considers most tokens (default)\n",
    "- **Top-p = 1.0**: Considers all tokens\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Token probabilities:\n",
    "- \"the\" (40%)\n",
    "- \"a\" (30%)  ← Cumulative: 70%\n",
    "- \"an\" (20%) ← Cumulative: 90%\n",
    "- \"some\" (10%)\n",
    "\n",
    "Top-p = 0.7: Only considers \"the\" and \"a\"\n",
    "Top-p = 0.9: Considers \"the\", \"a\", and \"an\"\n",
    "```\n",
    "\n",
    "**Why Use Top-p?**\n",
    "\n",
    "- **More control** than temperature alone\n",
    "- **Adaptive**: Adjusts based on probability distribution\n",
    "- **Quality**: Filters out low-probability nonsense tokens\n",
    "\n",
    "---\n",
    "\n",
    "### Parameter Profiles for Different Use Cases\n",
    "\n",
    "**1. Accuracy-Focused (Temperature: 0.1, Top-p: 0.3)**\n",
    "```python\n",
    "{\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.3,\n",
    "    'description': 'High accuracy, low creativity'\n",
    "}\n",
    "```\n",
    "**Use for:**\n",
    "- Factual questions\n",
    "- Code generation\n",
    "- Data extraction\n",
    "- Classification tasks\n",
    "\n",
    "**2. Balanced (Temperature: 0.5, Top-p: 0.7)**\n",
    "```python\n",
    "{\n",
    "    'temperature': 0.5,\n",
    "    'top_p': 0.7,\n",
    "    'description': 'Balanced creativity and accuracy'\n",
    "}\n",
    "```\n",
    "**Use for:**\n",
    "- General conversation\n",
    "- Content writing\n",
    "- Analysis tasks\n",
    "- Most applications\n",
    "\n",
    "**3. Creative (Temperature: 0.8, Top-p: 0.9)**\n",
    "```python\n",
    "{\n",
    "    'temperature': 0.8,\n",
    "    'top_p': 0.9,\n",
    "    'description': 'High creativity, varied outputs'\n",
    "}\n",
    "```\n",
    "**Use for:**\n",
    "- Creative writing\n",
    "- Brainstorming\n",
    "- Idea generation\n",
    "- Artistic content\n",
    "\n",
    "**4. Deterministic (Temperature: 0.0, Top-p: 0.1)**\n",
    "```python\n",
    "{\n",
    "    'temperature': 0.0,\n",
    "    'top_p': 0.1,\n",
    "    'description': 'Maximum consistency'\n",
    "}\n",
    "```\n",
    "**Use for:**\n",
    "- Testing and debugging\n",
    "- Reproducible outputs\n",
    "- When consistency is critical\n",
    "\n",
    "---\n",
    "\n",
    "### Measuring Parameter Impact\n",
    "\n",
    "**Key Metrics:**\n",
    "\n",
    "**1. Consistency Score**\n",
    "- **What**: How similar are outputs for same input?\n",
    "- **How**: Compare multiple runs with same parameters\n",
    "- **Target**: High for accuracy, low for creativity\n",
    "\n",
    "**2. Lexical Diversity**\n",
    "- **What**: Ratio of unique words to total words\n",
    "- **How**: Count unique words / total words\n",
    "- **Target**: Higher = more varied vocabulary\n",
    "\n",
    "**3. Length Variance**\n",
    "- **What**: How much does response length vary?\n",
    "- **How**: Standard deviation of response lengths\n",
    "- **Target**: Low for consistent tasks, high for creative tasks\n",
    "\n",
    "**4. Quality Metrics**\n",
    "- **What**: Does output meet requirements?\n",
    "- **How**: Human evaluation or automated checks\n",
    "- **Target**: Depends on use case\n",
    "\n",
    "---\n",
    "\n",
    "### The Parameter Optimization Process\n",
    "\n",
    "**Step 1: Define Success Metrics**\n",
    "- What makes a \"good\" output?\n",
    "- How will you measure it?\n",
    "- What's acceptable vs unacceptable?\n",
    "\n",
    "**Step 2: Test Parameter Grid**\n",
    "```python\n",
    "param_grid = [\n",
    "    (0.1, 0.3),  # Accuracy-focused\n",
    "    (0.5, 0.7),  # Balanced\n",
    "    (0.8, 0.9),  # Creative\n",
    "]\n",
    "```\n",
    "\n",
    "**Step 3: Run Tests**\n",
    "- Same prompts across all parameter combinations\n",
    "- Multiple runs per combination (for consistency)\n",
    "- Record all metrics\n",
    "\n",
    "**Step 4: Analyze Results**\n",
    "- Which parameters give best results?\n",
    "- Are there trade-offs?\n",
    "- What's the optimal balance?\n",
    "\n",
    "**Step 5: Validate**\n",
    "- Test on new prompts\n",
    "- Verify in production\n",
    "- Monitor ongoing performance\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Optimization Example\n",
    "\n",
    "**Scenario: Customer Support Chatbot**\n",
    "\n",
    "**Initial Parameters:**\n",
    "- Temperature: 0.7 (default)\n",
    "- Top-p: 0.9 (default)\n",
    "- **Result**: Inconsistent, sometimes too creative\n",
    "\n",
    "**Optimized Parameters:**\n",
    "- Temperature: 0.2\n",
    "- Top-p: 0.5\n",
    "- **Result**: Consistent, accurate, professional\n",
    "\n",
    "**Impact:**\n",
    "- **30% improvement** in user satisfaction\n",
    "- **50% reduction** in escalations to human agents\n",
    "- **20% faster** response times (more concise)\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Start with Defaults**\n",
    "- Don't optimize prematurely\n",
    "- Understand baseline first\n",
    "- Then optimize based on data\n",
    "\n",
    "**2. Test Systematically**\n",
    "- Use parameter grids\n",
    "- Test one variable at a time\n",
    "- Document all results\n",
    "\n",
    "**3. Consider Trade-offs**\n",
    "- Accuracy vs creativity\n",
    "- Consistency vs variety\n",
    "- Speed vs quality\n",
    "\n",
    "**4. Monitor Continuously**\n",
    "- Parameters may need adjustment over time\n",
    "- Use cases evolve\n",
    "- Models get updated\n",
    "\n",
    "Now let's implement parameter optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterOptimizer:\n",
    "    \"\"\"Optimize model parameters for different use cases.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameter_profiles = {\n",
    "            'accuracy_focused': {'temperature': 0.1, 'top_p': 0.3, 'description': 'High accuracy, low creativity'},\n",
    "            'balanced': {'temperature': 0.5, 'top_p': 0.7, 'description': 'Balanced creativity and accuracy'},\n",
    "            'creative': {'temperature': 0.8, 'top_p': 0.9, 'description': 'High creativity, varied outputs'},\n",
    "            'deterministic': {'temperature': 0.0, 'top_p': 0.1, 'description': 'Maximum consistency'}\n",
    "        }\n",
    "    \n",
    "    def get_optimal_parameters(self, use_case: str) -> Dict[str, float]:\n",
    "        \"\"\"Get optimal parameters for specific use case.\"\"\"\n",
    "        return self.parameter_profiles.get(use_case, self.parameter_profiles['balanced'])\n",
    "    \n",
    "    def test_parameter_sensitivity(self, client, test_prompts: List[str], \n",
    "                                 param_grid: List[Tuple[float, float]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Test multiple parameter combinations.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for temp, top_p in param_grid:\n",
    "            combination_results = {\n",
    "                'temperature': temp,\n",
    "                'top_p': top_p,\n",
    "                'outputs': [],\n",
    "                'metrics': {}\n",
    "            }\n",
    "            \n",
    "            for prompt in test_prompts:\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                        temperature=temp,\n",
    "                        top_p=top_p,\n",
    "                        max_tokens=150\n",
    "                    )\n",
    "                    \n",
    "                    combination_results['outputs'].append({\n",
    "                        'prompt': prompt,\n",
    "                        'response': response.choices[0].message.content\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with temp={temp}, top_p={top_p}: {e}\")\n",
    "            \n",
    "            # Calculate metrics for this combination\n",
    "            combination_results['metrics'] = self.calculate_metrics(combination_results['outputs'])\n",
    "            results.append(combination_results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_metrics(self, outputs: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate stability and quality metrics.\"\"\"\n",
    "        responses = [output['response'] for output in outputs]\n",
    "        \n",
    "        return {\n",
    "            'avg_length': np.mean([len(r) for r in responses]),\n",
    "            'length_variance': np.var([len(r) for r in responses]),\n",
    "            'lexical_diversity': self.calculate_lexical_diversity(responses),\n",
    "            'consistency_score': self.calculate_consistency(responses)\n",
    "        }\n",
    "    \n",
    "    def calculate_lexical_diversity(self, responses: List[str]) -> float:\n",
    "        \"\"\"Calculate lexical diversity (unique words / total words).\"\"\"\n",
    "        all_words = []\n",
    "        for response in responses:\n",
    "            words = re.findall(r'\\b\\w+\\b', response.lower())\n",
    "            all_words.extend(words)\n",
    "        \n",
    "        if not all_words:\n",
    "            return 0.0\n",
    "        \n",
    "        unique_words = len(set(all_words))\n",
    "        total_words = len(all_words)\n",
    "        return unique_words / total_words\n",
    "    \n",
    "    def calculate_consistency(self, responses: List[str]) -> float:\n",
    "        \"\"\"Calculate consistency across responses.\"\"\"\n",
    "        if len(responses) < 2:\n",
    "            return 1.0\n",
    "        \n",
    "        # Use TF-IDF to measure similarity\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(responses)\n",
    "            similarities = cosine_similarity(tfidf_matrix)\n",
    "            \n",
    "            # Calculate average similarity (excluding diagonal)\n",
    "            n = len(responses)\n",
    "            total_similarity = 0\n",
    "            count = 0\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    total_similarity += similarities[i][j]\n",
    "                    count += 1\n",
    "            \n",
    "            return total_similarity / count if count > 0 else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "# Test parameter optimization\n",
    "print(\"Testing Parameter Optimization...\")\n",
    "\n",
    "optimizer = ParameterOptimizer()\n",
    "\n",
    "# Test different parameter profiles\n",
    "for profile_name, params in optimizer.parameter_profiles.items():\n",
    "    print(f\"\\n--- {profile_name.replace('_', ' ').title()} ---\")\n",
    "    print(f\"Temperature: {params['temperature']}\")\n",
    "    print(f\"Top_p: {params['top_p']}\")\n",
    "    print(f\"Description: {params['description']}\")\n",
    "\n",
    "# Test parameter sensitivity with sample prompts\n",
    "test_prompts = [\n",
    "    \"Write a creative story about artificial intelligence.\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"What are the benefits of renewable energy?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: LLM-as-Judge Evaluation Framework\n",
    "\n",
    "### Why Automated Evaluation? The Scale Problem\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "Evaluating AI outputs manually is:\n",
    "- **Slow**: Humans take minutes per evaluation\n",
    "- **Expensive**: Requires human evaluators\n",
    "- **Inconsistent**: Different evaluators, different standards\n",
    "- **Not Scalable**: Can't evaluate thousands of outputs\n",
    "\n",
    "**The Solution: LLM-as-Judge**\n",
    "\n",
    "Use **another AI model** to evaluate AI outputs:\n",
    "- **Fast**: Seconds per evaluation\n",
    "- **Cheap**: Fraction of human cost\n",
    "- **Consistent**: Same criteria every time\n",
    "- **Scalable**: Evaluate millions of outputs\n",
    "\n",
    "**When It Works:**\n",
    "- ✅ Objective criteria (accuracy, completeness)\n",
    "- ✅ Structured outputs (JSON, code)\n",
    "- ✅ Well-defined quality standards\n",
    "- ✅ High-volume evaluation needs\n",
    "\n",
    "**When It Doesn't:**\n",
    "- ❌ Subjective quality (artistic merit)\n",
    "- ❌ Domain expertise required\n",
    "- ❌ Nuanced human judgment\n",
    "- ❌ Critical decisions (use human review)\n",
    "\n",
    "---\n",
    "\n",
    "### Understanding LLM-as-Judge Architecture\n",
    "\n",
    "**The Two-Model System:**\n",
    "\n",
    "```\n",
    "Model 1 (Generator): Creates outputs\n",
    "    ↓\n",
    "Output: \"The capital of France is Paris...\"\n",
    "    ↓\n",
    "Model 2 (Judge): Evaluates outputs\n",
    "    ↓\n",
    "Evaluation: \"Relevance: 9/10, Accuracy: 10/10...\"\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "\n",
    "- **Separation**: Judge doesn't know it's evaluating AI output\n",
    "- **Objectivity**: Judge applies same criteria to all outputs\n",
    "- **Scalability**: Can evaluate at same speed as generation\n",
    "- **Cost**: Judge model can be smaller/cheaper than generator\n",
    "\n",
    "---\n",
    "\n",
    "### Designing Effective Evaluation Prompts\n",
    "\n",
    "**The Evaluation Prompt Structure:**\n",
    "\n",
    "```\n",
    "You are an expert evaluator. Assess the following response:\n",
    "\n",
    "Criteria: {criteria_list}\n",
    "\n",
    "{f'Reference answer: {reference}' if reference else ''}\n",
    "\n",
    "Response to evaluate: {response}\n",
    "\n",
    "Provide scores (1-10) for each criterion:\n",
    "- Relevance: [score] - [justification]\n",
    "- Accuracy: [score] - [justification]\n",
    "- Clarity: [score] - [justification]\n",
    "- Completeness: [score] - [justification]\n",
    "- Style: [score] - [justification]\n",
    "\n",
    "Overall score: [average]/10\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "**1. Role Definition**\n",
    "```python\n",
    "\"You are an expert evaluator...\"\n",
    "```\n",
    "**Why**: Gives judge authority and context\n",
    "\n",
    "**2. Criteria Specification**\n",
    "```python\n",
    "\"Criteria: Relevance, Accuracy, Clarity, Completeness, Style\"\n",
    "```\n",
    "**Why**: Defines what to evaluate\n",
    "\n",
    "**3. Reference Answer (Optional)**\n",
    "```python\n",
    "\"Reference answer: The capital of France is Paris.\"\n",
    "```\n",
    "**Why**: Provides ground truth for comparison\n",
    "\n",
    "**4. Structured Output Format**\n",
    "```python\n",
    "\"Provide scores (1-10) for each criterion...\"\n",
    "```\n",
    "**Why**: Makes parsing results easier\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Criteria Design\n",
    "\n",
    "**Common Criteria:**\n",
    "\n",
    "**1. Relevance**\n",
    "- **What**: Does response address the question?\n",
    "- **Scale**: 1 (completely off-topic) to 10 (perfectly relevant)\n",
    "- **Example**: Question about Python, response about Java → Low relevance\n",
    "\n",
    "**2. Accuracy**\n",
    "- **What**: Is the information correct?\n",
    "- **Scale**: 1 (completely wrong) to 10 (completely accurate)\n",
    "- **Example**: \"2+2=5\" → Low accuracy\n",
    "\n",
    "**3. Clarity**\n",
    "- **What**: Is the response easy to understand?\n",
    "- **Scale**: 1 (confusing) to 10 (crystal clear)\n",
    "- **Example**: Jargon-filled response → Low clarity\n",
    "\n",
    "**4. Completeness**\n",
    "- **What**: Does response cover all aspects?\n",
    "- **Scale**: 1 (missing key points) to 10 (comprehensive)\n",
    "- **Example**: Partial answer → Lower completeness\n",
    "\n",
    "**5. Style**\n",
    "- **What**: Does response match required style?\n",
    "- **Scale**: 1 (wrong style) to 10 (perfect style)\n",
    "- **Example**: Formal question, casual response → Lower style score\n",
    "\n",
    "---\n",
    "\n",
    "### Parsing Evaluation Results\n",
    "\n",
    "**The Challenge:**\n",
    "\n",
    "Judge outputs natural language:\n",
    "```\n",
    "\"Relevance: 9 - The response directly addresses the question about France's capital.\n",
    "Accuracy: 10 - The information is correct.\n",
    "Overall score: 9.5/10\"\n",
    "```\n",
    "\n",
    "**The Solution: Pattern Matching**\n",
    "\n",
    "```python\n",
    "score_patterns = {\n",
    "    'relevance': r'Relevance:\\s*(\\d+)',\n",
    "    'accuracy': r'Accuracy:\\s*(\\d+)',\n",
    "    'clarity': r'Clarity:\\s*(\\d+)',\n",
    "    'completeness': r'Completeness:\\s*(\\d+)',\n",
    "    'style': r'Style:\\s*(\\d+)'\n",
    "}\n",
    "\n",
    "for criterion, pattern in score_patterns.items():\n",
    "    match = re.search(pattern, evaluation_text)\n",
    "    if match:\n",
    "        scores[criterion] = int(match.group(1))\n",
    "```\n",
    "\n",
    "**Why This Works:**\n",
    "- **Reliable**: Consistent format = easy parsing\n",
    "- **Robust**: Handles minor variations\n",
    "- **Fast**: Regex is efficient\n",
    "\n",
    "---\n",
    "\n",
    "### Production Template Management\n",
    "\n",
    "**Why Templates Matter:**\n",
    "\n",
    "In production, you need:\n",
    "- **Version Control**: Track template changes\n",
    "- **A/B Testing**: Compare template variations\n",
    "- **Reusability**: Use templates across projects\n",
    "- **Maintainability**: Update templates easily\n",
    "\n",
    "**Template Structure:**\n",
    "\n",
    "```python\n",
    "{\n",
    "    'name': 'customer_support',\n",
    "    'version': '1.0.0',\n",
    "    'template': 'You are a customer support agent...',\n",
    "    'parameters': [\n",
    "        {'name': 'company_name', 'required': True},\n",
    "        {'name': 'product_type', 'required': True},\n",
    "        {'name': 'customer_issue', 'required': True}\n",
    "    ],\n",
    "    'metadata': {\n",
    "        'description': 'Customer support response template',\n",
    "        'category': 'support',\n",
    "        'created_at': timestamp,\n",
    "        'usage_count': 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- **Validation**: Check required parameters before rendering\n",
    "- **Tracking**: Monitor template usage\n",
    "- **Versioning**: Roll back if needed\n",
    "- **Documentation**: Self-documenting structure\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**1. Content Quality Control**\n",
    "- Evaluate generated articles before publishing\n",
    "- Ensure brand voice consistency\n",
    "- Check factual accuracy\n",
    "\n",
    "**2. Customer Support**\n",
    "- Evaluate chatbot responses\n",
    "- Ensure helpfulness and accuracy\n",
    "- Monitor quality over time\n",
    "\n",
    "**3. Code Generation**\n",
    "- Evaluate generated code quality\n",
    "- Check correctness and style\n",
    "- Ensure best practices\n",
    "\n",
    "**4. Data Extraction**\n",
    "- Evaluate extraction accuracy\n",
    "- Check completeness\n",
    "- Validate structure\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Use Reference Answers When Available**\n",
    "- Provides ground truth\n",
    "- Improves evaluation accuracy\n",
    "- Enables comparison\n",
    "\n",
    "**2. Define Clear Criteria**\n",
    "- Vague criteria = inconsistent evaluations\n",
    "- Specific criteria = reliable results\n",
    "- Examples help clarify\n",
    "\n",
    "**3. Calibrate with Human Evaluators**\n",
    "- Compare LLM judge to human judges\n",
    "- Adjust criteria based on differences\n",
    "- Validate approach\n",
    "\n",
    "**4. Monitor Judge Performance**\n",
    "- Track evaluation consistency\n",
    "- Check for bias\n",
    "- Update judge prompts as needed\n",
    "\n",
    "**5. Combine with Other Metrics**\n",
    "- Don't rely solely on LLM-as-judge\n",
    "- Use automated metrics (length, format)\n",
    "- Include human spot-checks\n",
    "\n",
    "Now let's implement the evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudge:\n",
    "    \"\"\"Implement LLM-as-a-judge evaluation system.\"\"\"\n",
    "    \n",
    "    def __init__(self, judge_model_client):\n",
    "        self.judge_model = judge_model_client\n",
    "    \n",
    "    def evaluate_response(self, response: str, criteria: List[str], reference: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate a response using LLM-as-judge.\"\"\"\n",
    "        evaluation_prompt = f\"\"\"\n",
    "You are an expert evaluator. Assess the following response based on these criteria:\n",
    "\n",
    "Criteria: {', '.join(criteria)}\n",
    "\n",
    "{f'Reference answer: {reference}' if reference else ''}\n",
    "\n",
    "Response to evaluate: {response}\n",
    "\n",
    "Provide scores (1-10) for each criterion and brief justifications:\n",
    "- Relevance: [score] - [justification]\n",
    "- Accuracy: [score] - [justification] \n",
    "- Clarity: [score] - [justification]\n",
    "- Completeness: [score] - [justification]\n",
    "- Style: [score] - [justification]\n",
    "\n",
    "Overall score: [average]/10\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            evaluation = self.judge_model.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": evaluation_prompt}]\n",
    "            )\n",
    "            \n",
    "            return self.parse_evaluation(evaluation.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            return {'error': str(e), 'overall_score': 0}\n",
    "    \n",
    "    def parse_evaluation(self, evaluation_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse evaluation results from judge response.\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Extract individual scores\n",
    "        score_patterns = {\n",
    "            'relevance': r'Relevance:\\s*(\\d+)',\n",
    "            'accuracy': r'Accuracy:\\s*(\\d+)',\n",
    "            'clarity': r'Clarity:\\s*(\\d+)',\n",
    "            'completeness': r'Completeness:\\s*(\\d+)',\n",
    "            'style': r'Style:\\s*(\\d+)'\n",
    "        }\n",
    "        \n",
    "        for criterion, pattern in score_patterns.items():\n",
    "            match = re.search(pattern, evaluation_text)\n",
    "            if match:\n",
    "                scores[criterion] = int(match.group(1))\n",
    "        \n",
    "        # Extract overall score\n",
    "        overall_match = re.search(r'Overall score:\\s*(\\d+(?:\\.\\d+)?)', evaluation_text)\n",
    "        overall_score = float(overall_match.group(1)) if overall_match else 0\n",
    "        \n",
    "        return {\n",
    "            'individual_scores': scores,\n",
    "            'overall_score': overall_score,\n",
    "            'full_evaluation': evaluation_text\n",
    "        }\n",
    "\n",
    "class ProductionPromptTemplate:\n",
    "    \"\"\"Production-ready prompt template management.\"\"\"\n",
    "    \n",
    "    def __init__(self, template_dir: str = \"templates\"):\n",
    "        self.template_dir = template_dir\n",
    "        self.templates = {}\n",
    "        self.version_history = {}\n",
    "    \n",
    "    def create_template(self, name: str, template: str, parameters: List[Dict[str, Any]], \n",
    "                       metadata: Dict[str, Any] = None) -> str:\n",
    "        \"\"\"Create a new prompt template.\"\"\"\n",
    "        template_data = {\n",
    "            'name': name,\n",
    "            'version': '1.0.0',\n",
    "            'template': template,\n",
    "            'parameters': parameters,\n",
    "            'metadata': metadata or {},\n",
    "            'created_at': time.time(),\n",
    "            'usage_count': 0\n",
    "        }\n",
    "        \n",
    "        self.templates[name] = template_data\n",
    "        self.version_history[f\"{name}_v1.0.0\"] = template_data.copy()\n",
    "        \n",
    "        return f\"{name}_v1.0.0\"\n",
    "    \n",
    "    def render_template(self, name: str, **kwargs) -> str:\n",
    "        \"\"\"Render template with provided parameters.\"\"\"\n",
    "        if name not in self.templates:\n",
    "            raise ValueError(f\"Template '{name}' not found\")\n",
    "        \n",
    "        template_data = self.templates[name]\n",
    "        template_str = template_data['template']\n",
    "        \n",
    "        # Validate required parameters\n",
    "        required_params = [p['name'] for p in template_data['parameters'] if p.get('required', True)]\n",
    "        missing_params = [p for p in required_params if p not in kwargs]\n",
    "        \n",
    "        if missing_params:\n",
    "            raise ValueError(f\"Missing required parameters: {missing_params}\")\n",
    "        \n",
    "        # Render template\n",
    "        try:\n",
    "            rendered = template_str.format(**kwargs)\n",
    "            self.templates[name]['usage_count'] += 1\n",
    "            return rendered\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Missing template parameter: {e}\")\n",
    "\n",
    "# Test the evaluation framework\n",
    "print(\"Testing LLM-as-Judge Evaluation Framework...\")\n",
    "\n",
    "# Create sample responses for evaluation\n",
    "sample_responses = [\n",
    "    \"The capital of France is Paris. It is known for the Eiffel Tower and Louvre Museum.\",\n",
    "    \"Paris is the capital city of France, located in Western Europe. It has a population of over 2 million people.\",\n",
    "    \"France's capital is Paris, which is famous for landmarks like the Eiffel Tower.\"\n",
    "]\n",
    "\n",
    "print(\"Sample responses created for evaluation\")\n",
    "\n",
    "# Test production template system\n",
    "print(\"\\nTesting Production Template System...\")\n",
    "\n",
    "template_manager = ProductionPromptTemplate()\n",
    "\n",
    "# Create a customer support template\n",
    "support_template = \"\"\"\n",
    "You are a customer support agent for {company_name}.\n",
    "You specialize in helping customers with {product_type} products.\n",
    "\n",
    "Guidelines:\n",
    "- Be polite and professional\n",
    "- Ask clarifying questions when needed\n",
    "- Provide step-by-step solutions\n",
    "- Escalate complex issues when necessary\n",
    "\n",
    "Customer Issue: {customer_issue}\n",
    "\"\"\"\n",
    "\n",
    "template_params = [\n",
    "    {'name': 'company_name', 'type': 'string', 'required': True, 'description': 'Company name'},\n",
    "    {'name': 'product_type', 'type': 'string', 'required': True, 'description': 'Type of product'},\n",
    "    {'name': 'customer_issue', 'type': 'string', 'required': True, 'description': 'Customer issue description'}\n",
    "]\n",
    "\n",
    "template_version = template_manager.create_template(\n",
    "    name=\"customer_support\",\n",
    "    template=support_template,\n",
    "    parameters=template_params,\n",
    "    metadata={'description': 'Customer support response template', 'category': 'support'}\n",
    ")\n",
    "\n",
    "print(f\"Created template: {template_version}\")\n",
    "\n",
    "# Test rendering the template\n",
    "rendered_prompt = template_manager.render_template(\n",
    "    name=\"customer_support\",\n",
    "    company_name=\"TechCorp\",\n",
    "    product_type=\"software\",\n",
    "    customer_issue=\"I can't log into my account\"\n",
    ")\n",
    "\n",
    "print(\"\\nRendered Template:\")\n",
    "print(rendered_prompt)\n",
    "\n",
    "# Test with missing parameter (should raise error)\n",
    "try:\n",
    "    template_manager.render_template(\n",
    "        name=\"customer_support\",\n",
    "        company_name=\"TechCorp\",\n",
    "        product_type=\"software\"\n",
    "        # Missing customer_issue parameter\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"\\nExpected error for missing parameter: {e}\")\n",
    "\n",
    "print(\"\\n✅ Prompt Engineering Lab Completed!\")\n",
    "print(\"\\nKey Skills Learned:\")\n",
    "print(\"- System prompt design for different roles\")\n",
    "print(\"- CLEAR framework implementation\")\n",
    "print(\"- Parameter optimization and tuning\")\n",
    "print(\"- LLM-as-judge evaluation framework\")\n",
    "print(\"- Production-ready template management\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Experiment with different parameter combinations\")\n",
    "print(\"2. Create custom evaluation criteria for your use case\")\n",
    "print(\"3. Build a comprehensive prompt template library\")\n",
    "print(\"4. Implement A/B testing for prompt variations\")\n",
    "print(\"5. Add monitoring and analytics for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Prompt Engineering and Evaluation lab. Here's what you've learned:\n",
    "\n",
    "### Key Skills Acquired:\n",
    "- ✅ System prompt design for different professional roles\n",
    "- ✅ CLEAR framework implementation for structured prompts\n",
    "- ✅ Parameter optimization (temperature, top_p) for different use cases\n",
    "- ✅ LLM-as-judge evaluation framework for automated assessment\n",
    "- ✅ Production-ready prompt template management with versioning\n",
    "\n",
    "### Best Practices:\n",
    "- Always define clear roles and constraints in system prompts\n",
    "- Use the CLEAR framework for complex prompt requirements\n",
    "- Test parameter sensitivity for your specific use case\n",
    "- Implement automated evaluation for quality assurance\n",
    "- Version control your prompt templates for production use\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different parameter combinations for your specific use cases\n",
    "2. Create custom evaluation criteria tailored to your domain\n",
    "3. Build a comprehensive library of prompt templates\n",
    "4. Implement A/B testing frameworks for prompt optimization\n",
    "5. Add monitoring and analytics for production prompt performance\n",
    "\n",
    "Remember: Effective prompt engineering is an iterative process. Continuously test, evaluate, and refine your prompts based on real-world performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
